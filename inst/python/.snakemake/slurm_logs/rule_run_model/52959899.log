Building DAG of jobs...
Using shell: /usr/bin/bash
Provided cores: 2
Rules claiming more threads will be scaled down.
Provided resources: mem_mb=10000, mem_mib=9537, disk_mb=1000, disk_mib=954
Select jobs to execute...
Building DAG of jobs...
Using shell: /usr/bin/bash
Provided cores: 2
Rules claiming more threads will be scaled down.
Provided resources: mem_mb=10000, mem_mib=9537, disk_mb=1000, disk_mib=954
Select jobs to execute...

[Sat Jun 10 01:33:30 2023]
rule run_model:
    output: Res/PSD95-PDZ3/mochi__fit_tmodel_3state_sparse_dimsum128_subsample100p/tri_state_equilibrium/model_performance_perepoch.pdf, Res/PSD95-PDZ3/mochi__fit_tmodel_3state_sparse_dimsum128_subsample100p/tri_state_equilibrium/predicted_fitness.txt, Res/PSD95-PDZ3/mochi__fit_tmodel_3state_sparse_dimsum128_subsample100p/tri_state_equilibrium/model_weights.txt, Res/PSD95-PDZ3/mochi__fit_tmodel_3state_sparse_dimsum128_subsample100p/tri_state_equilibrium/model_parameters.txt
    jobid: 0
    reason: Missing output files: Res/PSD95-PDZ3/mochi__fit_tmodel_3state_sparse_dimsum128_subsample100p/tri_state_equilibrium/model_parameters.txt, Res/PSD95-PDZ3/mochi__fit_tmodel_3state_sparse_dimsum128_subsample100p/tri_state_equilibrium/model_performance_perepoch.pdf, Res/PSD95-PDZ3/mochi__fit_tmodel_3state_sparse_dimsum128_subsample100p/tri_state_equilibrium/predicted_fitness.txt, Res/PSD95-PDZ3/mochi__fit_tmodel_3state_sparse_dimsum128_subsample100p/tri_state_equilibrium/model_weights.txt
    wildcards: protein=PSD95-PDZ3, dataset=mochi__fit_tmodel_3state_sparse_dimsum128_subsample100p, model_type=tri_state_equilibrium
    resources: mem_mb=10000, mem_mib=9537, disk_mb=1000, disk_mib=954, tmpdir=/tmp/slurm_52959899, slurm_account=u_froehlichf, slurm_partition=cpu

/camp/home/demetzp/.conda/envs/demetzp/lib/python3.8/site-packages/haiku/_src/data_structures.py:37: FutureWarning: jax.tree_structure is deprecated, and will be removed in a future release. Use jax.tree_util.tree_structure instead.
  PyTreeDef = type(jax.tree_structure(None))
No GPU/TPU found, falling back to CPU. (Set TF_CPP_MIN_LOG_LEVEL=0 and rerun for more info.)
wandb: Currently logged in as: demetz-pierre (lab_frohlich). Use `wandb login --relogin` to force relogin
wandb: wandb version 0.15.4 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.15.3
wandb: Run data is saved locally in /nemo/lab/froehlichf/home/users/demetzp/PierreDeMetz/inst/python/wandb/run-20230610_013432-d7xathwu
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run Run 1
wandb: ⭐️ View project at https://wandb.ai/lab_frohlich/pierre_mochi__fit_tmodel_3state_doubledeepms
wandb: 🚀 View run at https://wandb.ai/lab_frohlich/pierre_mochi__fit_tmodel_3state_doubledeepms/runs/d7xathwu
2023-06-10 01:34:52.615943: E external/org_tensorflow/tensorflow/compiler/xla/python/pjit.cc:461] fastpath_data is none
2023-06-10 01:35:09.859567: E external/org_tensorflow/tensorflow/compiler/xla/python/pjit.cc:461] fastpath_data is none
/camp/home/demetzp/.conda/envs/demetzp/lib/python3.8/site-packages/haiku/_src/data_structures.py:143: FutureWarning: jax.tree_flatten is deprecated, and will be removed in a future release. Use jax.tree_util.tree_flatten instead.
  leaves, treedef = jax.tree_flatten(tree)
/camp/home/demetzp/.conda/envs/demetzp/lib/python3.8/site-packages/haiku/_src/data_structures.py:144: FutureWarning: jax.tree_unflatten is deprecated, and will be removed in a future release. Use jax.tree_util.tree_unflatten instead.
  return jax.tree_unflatten(treedef, leaves)
/camp/home/demetzp/.conda/envs/demetzp/lib/python3.8/site-packages/haiku/_src/data_structures.py:143: FutureWarning: jax.tree_flatten is deprecated, and will be removed in a future release. Use jax.tree_util.tree_flatten instead.
  leaves, treedef = jax.tree_flatten(tree)
/camp/home/demetzp/.conda/envs/demetzp/lib/python3.8/site-packages/haiku/_src/data_structures.py:144: FutureWarning: jax.tree_unflatten is deprecated, and will be removed in a future release. Use jax.tree_util.tree_unflatten instead.
  return jax.tree_unflatten(treedef, leaves)
wandb: Waiting for W&B process to finish... (success).
wandb: 
wandb: Run history:
wandb: val_loss_fit ██▇▇▆▆▆▅▅▄▄▄▄▃▃▃▃▃▂▂▂▂▂▂▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁
wandb: 
wandb: Run summary:
wandb: val_loss_fit 0.364
wandb: 
wandb: 🚀 View run Run 1 at: https://wandb.ai/lab_frohlich/pierre_mochi__fit_tmodel_3state_doubledeepms/runs/d7xathwu
wandb: Synced 6 W&B file(s), 0 media file(s), 2 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20230610_013432-d7xathwu/logs
wandb: - Waiting for wandb.init()...wandb: \ Waiting for wandb.init()...wandb: wandb version 0.15.4 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.15.3
wandb: Run data is saved locally in /nemo/lab/froehlichf/home/users/demetzp/PierreDeMetz/inst/python/wandb/run-20230610_013854-rcbsaoaj
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run Run 2
wandb: ⭐️ View project at https://wandb.ai/lab_frohlich/pierre_mochi__fit_tmodel_3state_doubledeepms
wandb: 🚀 View run at https://wandb.ai/lab_frohlich/pierre_mochi__fit_tmodel_3state_doubledeepms/runs/rcbsaoaj
/camp/home/demetzp/.conda/envs/demetzp/lib/python3.8/site-packages/haiku/_src/data_structures.py:143: FutureWarning: jax.tree_flatten is deprecated, and will be removed in a future release. Use jax.tree_util.tree_flatten instead.
  leaves, treedef = jax.tree_flatten(tree)
/camp/home/demetzp/.conda/envs/demetzp/lib/python3.8/site-packages/haiku/_src/data_structures.py:144: FutureWarning: jax.tree_unflatten is deprecated, and will be removed in a future release. Use jax.tree_util.tree_unflatten instead.
  return jax.tree_unflatten(treedef, leaves)
wandb: Waiting for W&B process to finish... (success).
wandb: - 0.005 MB of 0.005 MB uploaded (0.000 MB deduped)wandb: \ 0.005 MB of 0.017 MB uploaded (0.000 MB deduped)wandb: | 0.005 MB of 0.017 MB uploaded (0.000 MB deduped)wandb: 
wandb: Run history:
wandb: val_loss_fit ██▇▇▆▆▅▅▅▄▄▄▄▃▃▃▃▃▂▂▂▂▂▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb: 
wandb: Run summary:
wandb: val_loss_fit 0.363
wandb: 
wandb: 🚀 View run Run 2 at: https://wandb.ai/lab_frohlich/pierre_mochi__fit_tmodel_3state_doubledeepms/runs/rcbsaoaj
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20230610_013854-rcbsaoaj/logs
wandb: - Waiting for wandb.init()...wandb: \ Waiting for wandb.init()...wandb: wandb version 0.15.4 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.15.3
wandb: Run data is saved locally in /nemo/lab/froehlichf/home/users/demetzp/PierreDeMetz/inst/python/wandb/run-20230610_014259-1jumvkpp
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run Run 3
wandb: ⭐️ View project at https://wandb.ai/lab_frohlich/pierre_mochi__fit_tmodel_3state_doubledeepms
wandb: 🚀 View run at https://wandb.ai/lab_frohlich/pierre_mochi__fit_tmodel_3state_doubledeepms/runs/1jumvkpp
/camp/home/demetzp/.conda/envs/demetzp/lib/python3.8/site-packages/haiku/_src/data_structures.py:143: FutureWarning: jax.tree_flatten is deprecated, and will be removed in a future release. Use jax.tree_util.tree_flatten instead.
  leaves, treedef = jax.tree_flatten(tree)
/camp/home/demetzp/.conda/envs/demetzp/lib/python3.8/site-packages/haiku/_src/data_structures.py:144: FutureWarning: jax.tree_unflatten is deprecated, and will be removed in a future release. Use jax.tree_util.tree_unflatten instead.
  return jax.tree_unflatten(treedef, leaves)
wandb: Waiting for W&B process to finish... (success).
wandb: 
wandb: Run history:
wandb: val_loss_fit ██▇▇▆▆▆▅▅▄▄▄▄▃▃▃▃▃▂▂▂▂▂▂▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁
wandb: 
wandb: Run summary:
wandb: val_loss_fit 0.364
wandb: 
wandb: 🚀 View run Run 3 at: https://wandb.ai/lab_frohlich/pierre_mochi__fit_tmodel_3state_doubledeepms/runs/1jumvkpp
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20230610_014259-1jumvkpp/logs
wandb: - Waiting for wandb.init()...wandb: \ Waiting for wandb.init()...wandb: wandb version 0.15.4 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.15.3
wandb: Run data is saved locally in /nemo/lab/froehlichf/home/users/demetzp/PierreDeMetz/inst/python/wandb/run-20230610_014658-011ayhyn
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run Run 4
wandb: ⭐️ View project at https://wandb.ai/lab_frohlich/pierre_mochi__fit_tmodel_3state_doubledeepms
wandb: 🚀 View run at https://wandb.ai/lab_frohlich/pierre_mochi__fit_tmodel_3state_doubledeepms/runs/011ayhyn
/camp/home/demetzp/.conda/envs/demetzp/lib/python3.8/site-packages/haiku/_src/data_structures.py:143: FutureWarning: jax.tree_flatten is deprecated, and will be removed in a future release. Use jax.tree_util.tree_flatten instead.
  leaves, treedef = jax.tree_flatten(tree)
/camp/home/demetzp/.conda/envs/demetzp/lib/python3.8/site-packages/haiku/_src/data_structures.py:144: FutureWarning: jax.tree_unflatten is deprecated, and will be removed in a future release. Use jax.tree_util.tree_unflatten instead.
  return jax.tree_unflatten(treedef, leaves)
wandb: Waiting for W&B process to finish... (success).
wandb: 
wandb: Run history:
wandb: val_loss_fit ██▇▇▆▆▆▅▅▄▄▄▄▃▃▃▃▃▂▂▂▂▂▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb: 
wandb: Run summary:
wandb: val_loss_fit 0.362
wandb: 
wandb: 🚀 View run Run 4 at: https://wandb.ai/lab_frohlich/pierre_mochi__fit_tmodel_3state_doubledeepms/runs/011ayhyn
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20230610_014658-011ayhyn/logs
wandb: - Waiting for wandb.init()...wandb: \ Waiting for wandb.init()...wandb: | Waiting for wandb.init()...wandb: wandb version 0.15.4 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.15.3
wandb: Run data is saved locally in /nemo/lab/froehlichf/home/users/demetzp/PierreDeMetz/inst/python/wandb/run-20230610_015055-ptdx4tzj
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run Run 5
wandb: ⭐️ View project at https://wandb.ai/lab_frohlich/pierre_mochi__fit_tmodel_3state_doubledeepms
wandb: 🚀 View run at https://wandb.ai/lab_frohlich/pierre_mochi__fit_tmodel_3state_doubledeepms/runs/ptdx4tzj
Warning: Output model directory already exists.
Warning: Output plot directory already exists.
Warning: Output weights directory already exists.
Warning: Output boostrap directory already exists.
Grid search using {'num_samples': 128, 'learning_rate': 0.0001, 'l1_regularization_factor': 0.0001, 'l2_regularization_factor': 0.0001, 'number_additive_traits': 1, 'model_type': 'tri_state_equilibrium', 'specs': (False, False)}
epoch done with 0.802
epoch done with 0.787
epoch done with 0.772
epoch done with 0.758
epoch done with 0.743
epoch done with 0.729
epoch done with 0.715
epoch done with 0.701
epoch done with 0.688
epoch done with 0.674
epoch done with 0.661
epoch done with 0.649
epoch done with 0.636
epoch done with 0.624
epoch done with 0.612
epoch done with 0.600
epoch done with 0.589
epoch done with 0.578
epoch done with 0.567
epoch done with 0.557
epoch done with 0.547
epoch done with 0.537
epoch done with 0.528
epoch done with 0.520
epoch done with 0.512
epoch done with 0.505
epoch done with 0.497
epoch done with 0.490
epoch done with 0.484
epoch done with 0.478
epoch done with 0.472
epoch done with 0.466
epoch done with 0.461
epoch done with 0.456
epoch done with 0.451
epoch done with 0.446
epoch done with 0.442
epoch done with 0.438
epoch done with 0.434
epoch done with 0.430
epoch done with 0.426
epoch done with 0.423
epoch done with 0.420
epoch done with 0.417
epoch done with 0.414
epoch done with 0.411
epoch done with 0.408
epoch done with 0.405
epoch done with 0.403
epoch done with 0.400
epoch done with 0.398
epoch done with 0.396
epoch done with 0.393
epoch done with 0.391
epoch done with 0.389
epoch done with 0.387
epoch done with 0.385
epoch done with 0.384
epoch done with 0.382
epoch done with 0.380
epoch done with 0.379
epoch done with 0.377
epoch done with 0.376
epoch done with 0.375
epoch done with 0.373
epoch done with 0.372
epoch done with 0.371
epoch done with 0.370
epoch done with 0.369
epoch done with 0.368
epoch done with 0.367
epoch done with 0.366
epoch done with 0.366
epoch done with 0.365
epoch done with 0.364
Grid search using {'num_samples': 128, 'learning_rate': 0.0001, 'l1_regularization_factor': 0.0001, 'l2_regularization_factor': 0.001, 'number_additive_traits': 1, 'model_type': 'tri_state_equilibrium', 'specs': (False, False)}
epoch done with 0.815
epoch done with 0.798
epoch done with 0.782
epoch done with 0.766
epoch done with 0.751
epoch done with 0.735
epoch done with 0.720
epoch done with 0.706
epoch done with 0.691
epoch done with 0.677
epoch done with 0.664
epoch done with 0.650
epoch done with 0.637
epoch done with 0.625
epoch done with 0.612
epoch done with 0.600
epoch done with 0.589
epoch done with 0.578
epoch done with 0.567
epoch done with 0.556
epoch done with 0.546
epoch done with 0.537
epoch done with 0.528
epoch done with 0.519
epoch done with 0.511
epoch done with 0.504
epoch done with 0.497
epoch done with 0.490
epoch done with 0.483
epoch done with 0.477
epoch done with 0.471
epoch done with 0.466
epoch done with 0.460
epoch done with 0.455
epoch done with 0.450
epoch done with 0.445
epoch done with 0.441
epoch done with 0.437
epoch done with 0.433
epoch done with 0.428
epoch done with 0.425
epoch done with 0.421
epoch done with 0.418
epoch done with 0.415
epoch done with 0.412
epoch done with 0.409
epoch done with 0.406
epoch done with 0.403
epoch done with 0.401
epoch done with 0.398
epoch done with 0.396
epoch done with 0.394
epoch done with 0.392
epoch done with 0.390
epoch done with 0.388
epoch done with 0.386
epoch done with 0.385
epoch done with 0.383
epoch done with 0.382
epoch done with 0.380
epoch done with 0.379
epoch done with 0.377
epoch done with 0.376
epoch done with 0.375
epoch done with 0.374
epoch done with 0.372
epoch done with 0.371
epoch done with 0.370
epoch done with 0.369
epoch done with 0.368
epoch done with 0.367
epoch done with 0.366
epoch done with 0.365
epoch done with 0.364
epoch done with 0.363
Grid search using {'num_samples': 128, 'learning_rate': 0.0001, 'l1_regularization_factor': 0.0001, 'l2_regularization_factor': 0.01, 'number_additive_traits': 1, 'model_type': 'tri_state_equilibrium', 'specs': (False, False)}
epoch done with 0.798
epoch done with 0.783
epoch done with 0.769
epoch done with 0.754
epoch done with 0.740
epoch done with 0.726
epoch done with 0.712
epoch done with 0.698
epoch done with 0.684
epoch done with 0.671
epoch done with 0.658
epoch done with 0.645
epoch done with 0.633
epoch done with 0.621
epoch done with 0.609
epoch done with 0.597
epoch done with 0.586
epoch done with 0.575
epoch done with 0.565
epoch done with 0.555
epoch done with 0.545
epoch done with 0.536
epoch done with 0.527
epoch done with 0.518
epoch done with 0.511
epoch done with 0.503
epoch done with 0.496
epoch done with 0.489
epoch done with 0.483
epoch done with 0.477
epoch done with 0.471
epoch done with 0.466
epoch done with 0.460
epoch done with 0.455
epoch done with 0.451
epoch done with 0.446
epoch done with 0.441
epoch done with 0.437
epoch done with 0.433
epoch done with 0.430
epoch done with 0.426
epoch done with 0.423
epoch done with 0.419
epoch done with 0.416
epoch done with 0.413
epoch done with 0.410
epoch done with 0.408
epoch done with 0.405
epoch done with 0.402
epoch done with 0.400
epoch done with 0.398
epoch done with 0.395
epoch done with 0.393
epoch done with 0.391
epoch done with 0.389
epoch done with 0.387
epoch done with 0.385
epoch done with 0.384
epoch done with 0.382
epoch done with 0.380
epoch done with 0.379
epoch done with 0.377
epoch done with 0.376
epoch done with 0.375
epoch done with 0.373
epoch done with 0.372
epoch done with 0.371
epoch done with 0.370
epoch done with 0.369
epoch done with 0.368
epoch done with 0.367
epoch done with 0.366
epoch done with 0.366
epoch done with 0.365
epoch done with 0.364
Grid search using {'num_samples': 128, 'learning_rate': 0.0001, 'l1_regularization_factor': 0.0001, 'l2_regularization_factor': 0.1, 'number_additive_traits': 1, 'model_type': 'tri_state_equilibrium', 'specs': (False, False)}
epoch done with 0.865
epoch done with 0.847
epoch done with 0.830
epoch done with 0.813
epoch done with 0.796
epoch done with 0.780
epoch done with 0.764
epoch done with 0.748
epoch done with 0.732
epoch done with 0.717
epoch done with 0.702
epoch done with 0.688
epoch done with 0.673
epoch done with 0.660
epoch done with 0.646
epoch done with 0.633
epoch done with 0.620
epoch done with 0.607
epoch done with 0.595
epoch done with 0.583
epoch done with 0.572
epoch done with 0.561
epoch done with 0.550
epoch done with 0.540
epoch done with 0.530
epoch done with 0.521
epoch done with 0.512
epoch done with 0.505
epoch done with 0.497
epoch done with 0.490
epoch done with 0.483
epoch done with 0.477
epoch done with 0.471
epoch done with 0.465
epoch done with 0.459
epoch done with 0.454
epoch done with 0.449
epoch done with 0.444
epoch done with 0.439
epoch done with 0.435
epoch done with 0.431
epoch done with 0.426
epoch done with 0.423
epoch done with 0.419
epoch done with 0.416
epoch done with 0.412
epoch done with 0.409
epoch done with 0.406
epoch done with 0.403
epoch done with 0.401
epoch done with 0.398
epoch done with 0.396
epoch done with 0.394
epoch done with 0.392
epoch done with 0.390
epoch done with 0.388
epoch done with 0.386
epoch done with 0.384
epoch done with 0.383
epoch done with 0.381
epoch done with 0.379
epoch done with 0.378
epoch done with 0.376
epoch done with 0.375
epoch done with 0.374
epoch done with 0.372
epoch done with 0.371
epoch done with 0.370
epoch done with 0.369
epoch done with 0.368
epoch done with 0.367
epoch done with 0.365
epoch done with 0.364
epoch done with 0.363
epoch done with 0.362
Grid search using {'num_samples': 128, 'learning_rate': 0.0001, 'l1_regularization_factor': 0.001, 'l2_regularization_factor': 0.0001, 'number_additive_traits': 1, 'model_type': 'tri_state_equilibrium', 'specs': (False, False)}
epoch done with 0.954
epoch done with 0.938
epoch done with 0.921
epoch done with 0.905
epoch done with 0.888
epoch done with 0.872
epoch done with 0.856
epoch done with 0.841
epoch done with 0.825
epoch done with 0.810
epoch done with 0.795
/camp/home/demetzp/.conda/envs/demetzp/lib/python3.8/site-packages/haiku/_src/data_structures.py:143: FutureWarning: jax.tree_flatten is deprecated, and will be removed in a future release. Use jax.tree_util.tree_flatten instead.
  leaves, treedef = jax.tree_flatten(tree)
/camp/home/demetzp/.conda/envs/demetzp/lib/python3.8/site-packages/haiku/_src/data_structures.py:144: FutureWarning: jax.tree_unflatten is deprecated, and will be removed in a future release. Use jax.tree_util.tree_unflatten instead.
  return jax.tree_unflatten(treedef, leaves)
wandb: Waiting for W&B process to finish... (success).
wandb: 
wandb: Run history:
wandb: val_loss_fit ██▇▇▇▆▆▆▅▅▅▄▄▄▄▃▃▃▃▂▂▂▂▂▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁
wandb: 
wandb: Run summary:
wandb: val_loss_fit 0.365
wandb: 
wandb: 🚀 View run Run 5 at: https://wandb.ai/lab_frohlich/pierre_mochi__fit_tmodel_3state_doubledeepms/runs/ptdx4tzj
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20230610_015055-ptdx4tzj/logs
wandb: - Waiting for wandb.init()...wandb: \ Waiting for wandb.init()...wandb: wandb version 0.15.4 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.15.3
wandb: Run data is saved locally in /nemo/lab/froehlichf/home/users/demetzp/PierreDeMetz/inst/python/wandb/run-20230610_015458-fzphebkv
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run Run 6
wandb: ⭐️ View project at https://wandb.ai/lab_frohlich/pierre_mochi__fit_tmodel_3state_doubledeepms
wandb: 🚀 View run at https://wandb.ai/lab_frohlich/pierre_mochi__fit_tmodel_3state_doubledeepms/runs/fzphebkv
/camp/home/demetzp/.conda/envs/demetzp/lib/python3.8/site-packages/haiku/_src/data_structures.py:143: FutureWarning: jax.tree_flatten is deprecated, and will be removed in a future release. Use jax.tree_util.tree_flatten instead.
  leaves, treedef = jax.tree_flatten(tree)
/camp/home/demetzp/.conda/envs/demetzp/lib/python3.8/site-packages/haiku/_src/data_structures.py:144: FutureWarning: jax.tree_unflatten is deprecated, and will be removed in a future release. Use jax.tree_util.tree_unflatten instead.
  return jax.tree_unflatten(treedef, leaves)
wandb: Waiting for W&B process to finish... (success).
wandb: 
wandb: Run history:
wandb: val_loss_fit ██▇▇▆▆▆▅▅▄▄▄▄▃▃▃▃▃▂▂▂▂▂▂▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁
wandb: 
wandb: Run summary:
wandb: val_loss_fit 0.364
wandb: 
wandb: 🚀 View run Run 6 at: https://wandb.ai/lab_frohlich/pierre_mochi__fit_tmodel_3state_doubledeepms/runs/fzphebkv
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20230610_015458-fzphebkv/logs
wandb: - Waiting for wandb.init()...wandb: \ Waiting for wandb.init()...wandb: wandb version 0.15.4 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.15.3
wandb: Run data is saved locally in /nemo/lab/froehlichf/home/users/demetzp/PierreDeMetz/inst/python/wandb/run-20230610_015858-whmr2nsg
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run Run 7
wandb: ⭐️ View project at https://wandb.ai/lab_frohlich/pierre_mochi__fit_tmodel_3state_doubledeepms
wandb: 🚀 View run at https://wandb.ai/lab_frohlich/pierre_mochi__fit_tmodel_3state_doubledeepms/runs/whmr2nsg
/camp/home/demetzp/.conda/envs/demetzp/lib/python3.8/site-packages/haiku/_src/data_structures.py:143: FutureWarning: jax.tree_flatten is deprecated, and will be removed in a future release. Use jax.tree_util.tree_flatten instead.
  leaves, treedef = jax.tree_flatten(tree)
/camp/home/demetzp/.conda/envs/demetzp/lib/python3.8/site-packages/haiku/_src/data_structures.py:144: FutureWarning: jax.tree_unflatten is deprecated, and will be removed in a future release. Use jax.tree_util.tree_unflatten instead.
  return jax.tree_unflatten(treedef, leaves)
wandb: Waiting for W&B process to finish... (success).
wandb: 
wandb: Run history:
wandb: val_loss_fit ██▇▇▆▆▆▅▅▄▄▄▄▃▃▃▃▃▂▂▂▂▂▂▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁
wandb: 
wandb: Run summary:
wandb: val_loss_fit 0.364
wandb: 
wandb: 🚀 View run Run 7 at: https://wandb.ai/lab_frohlich/pierre_mochi__fit_tmodel_3state_doubledeepms/runs/whmr2nsg
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20230610_015858-whmr2nsg/logs
wandb: - Waiting for wandb.init()...wandb: \ Waiting for wandb.init()...wandb: wandb version 0.15.4 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.15.3
wandb: Run data is saved locally in /nemo/lab/froehlichf/home/users/demetzp/PierreDeMetz/inst/python/wandb/run-20230610_020311-8evfena8
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run Run 8
wandb: ⭐️ View project at https://wandb.ai/lab_frohlich/pierre_mochi__fit_tmodel_3state_doubledeepms
wandb: 🚀 View run at https://wandb.ai/lab_frohlich/pierre_mochi__fit_tmodel_3state_doubledeepms/runs/8evfena8
/camp/home/demetzp/.conda/envs/demetzp/lib/python3.8/site-packages/haiku/_src/data_structures.py:143: FutureWarning: jax.tree_flatten is deprecated, and will be removed in a future release. Use jax.tree_util.tree_flatten instead.
  leaves, treedef = jax.tree_flatten(tree)
/camp/home/demetzp/.conda/envs/demetzp/lib/python3.8/site-packages/haiku/_src/data_structures.py:144: FutureWarning: jax.tree_unflatten is deprecated, and will be removed in a future release. Use jax.tree_util.tree_unflatten instead.
  return jax.tree_unflatten(treedef, leaves)
wandb: Waiting for W&B process to finish... (success).
wandb: - 0.005 MB of 0.005 MB uploaded (0.000 MB deduped)wandb: \ 0.005 MB of 0.005 MB uploaded (0.000 MB deduped)wandb: | 0.005 MB of 0.005 MB uploaded (0.000 MB deduped)wandb: / 0.005 MB of 0.005 MB uploaded (0.000 MB deduped)wandb: 
wandb: Run history:
wandb: val_loss_fit ██▇▇▆▆▆▅▅▅▄▄▄▄▃▃▃▃▃▂▂▂▂▂▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁
wandb: 
wandb: Run summary:
wandb: val_loss_fit 0.367
wandb: 
wandb: 🚀 View run Run 8 at: https://wandb.ai/lab_frohlich/pierre_mochi__fit_tmodel_3state_doubledeepms/runs/8evfena8
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20230610_020311-8evfena8/logs
wandb: - Waiting for wandb.init()...wandb: \ Waiting for wandb.init()...wandb: wandb version 0.15.4 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.15.3
wandb: Run data is saved locally in /nemo/lab/froehlichf/home/users/demetzp/PierreDeMetz/inst/python/wandb/run-20230610_020716-y70ugh7q
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run Run 9
wandb: ⭐️ View project at https://wandb.ai/lab_frohlich/pierre_mochi__fit_tmodel_3state_doubledeepms
wandb: 🚀 View run at https://wandb.ai/lab_frohlich/pierre_mochi__fit_tmodel_3state_doubledeepms/runs/y70ugh7q
epoch done with 0.781
epoch done with 0.766
epoch done with 0.752
epoch done with 0.738
epoch done with 0.724
epoch done with 0.711
epoch done with 0.697
epoch done with 0.684
epoch done with 0.671
epoch done with 0.658
epoch done with 0.645
epoch done with 0.633
epoch done with 0.621
epoch done with 0.609
epoch done with 0.597
epoch done with 0.585
epoch done with 0.574
epoch done with 0.563
epoch done with 0.552
epoch done with 0.542
epoch done with 0.532
epoch done with 0.522
epoch done with 0.513
epoch done with 0.504
epoch done with 0.495
epoch done with 0.488
epoch done with 0.480
epoch done with 0.473
epoch done with 0.467
epoch done with 0.460
epoch done with 0.455
epoch done with 0.450
epoch done with 0.445
epoch done with 0.440
epoch done with 0.436
epoch done with 0.431
epoch done with 0.427
epoch done with 0.423
epoch done with 0.419
epoch done with 0.416
epoch done with 0.413
epoch done with 0.409
epoch done with 0.406
epoch done with 0.403
epoch done with 0.401
epoch done with 0.398
epoch done with 0.395
epoch done with 0.393
epoch done with 0.390
epoch done with 0.388
epoch done with 0.386
epoch done with 0.384
epoch done with 0.382
epoch done with 0.380
epoch done with 0.378
epoch done with 0.376
epoch done with 0.375
epoch done with 0.373
epoch done with 0.372
epoch done with 0.370
epoch done with 0.369
epoch done with 0.367
epoch done with 0.366
epoch done with 0.365
Grid search using {'num_samples': 128, 'learning_rate': 0.0001, 'l1_regularization_factor': 0.001, 'l2_regularization_factor': 0.001, 'number_additive_traits': 1, 'model_type': 'tri_state_equilibrium', 'specs': (False, False)}
epoch done with 0.799
epoch done with 0.784
epoch done with 0.769
epoch done with 0.754
epoch done with 0.740
epoch done with 0.726
epoch done with 0.712
epoch done with 0.698
epoch done with 0.685
epoch done with 0.671
epoch done with 0.658
epoch done with 0.646
epoch done with 0.633
epoch done with 0.621
epoch done with 0.609
epoch done with 0.598
epoch done with 0.586
epoch done with 0.576
epoch done with 0.565
epoch done with 0.555
epoch done with 0.545
epoch done with 0.536
epoch done with 0.527
epoch done with 0.519
epoch done with 0.511
epoch done with 0.504
epoch done with 0.497
epoch done with 0.490
epoch done with 0.483
epoch done with 0.477
epoch done with 0.471
epoch done with 0.466
epoch done with 0.461
epoch done with 0.456
epoch done with 0.451
epoch done with 0.446
epoch done with 0.442
epoch done with 0.438
epoch done with 0.434
epoch done with 0.430
epoch done with 0.426
epoch done with 0.423
epoch done with 0.420
epoch done with 0.417
epoch done with 0.414
epoch done with 0.411
epoch done with 0.408
epoch done with 0.405
epoch done with 0.403
epoch done with 0.400
epoch done with 0.398
epoch done with 0.396
epoch done with 0.394
epoch done with 0.392
epoch done with 0.389
epoch done with 0.388
epoch done with 0.386
epoch done with 0.384
epoch done with 0.382
epoch done with 0.380
epoch done with 0.379
epoch done with 0.377
epoch done with 0.376
epoch done with 0.375
epoch done with 0.374
epoch done with 0.373
epoch done with 0.371
epoch done with 0.370
epoch done with 0.369
epoch done with 0.368
epoch done with 0.368
epoch done with 0.367
epoch done with 0.366
epoch done with 0.365
epoch done with 0.364
Grid search using {'num_samples': 128, 'learning_rate': 0.0001, 'l1_regularization_factor': 0.001, 'l2_regularization_factor': 0.01, 'number_additive_traits': 1, 'model_type': 'tri_state_equilibrium', 'specs': (False, False)}
epoch done with 0.813
epoch done with 0.798
epoch done with 0.783
epoch done with 0.769
epoch done with 0.754
epoch done with 0.740
epoch done with 0.726
epoch done with 0.712
epoch done with 0.698
epoch done with 0.685
epoch done with 0.671
epoch done with 0.658
epoch done with 0.646
epoch done with 0.633
epoch done with 0.621
epoch done with 0.609
epoch done with 0.598
epoch done with 0.586
epoch done with 0.575
epoch done with 0.565
epoch done with 0.555
epoch done with 0.545
epoch done with 0.535
epoch done with 0.526
epoch done with 0.518
epoch done with 0.510
epoch done with 0.503
epoch done with 0.495
epoch done with 0.489
epoch done with 0.482
epoch done with 0.476
epoch done with 0.470
epoch done with 0.465
epoch done with 0.459
epoch done with 0.454
epoch done with 0.449
epoch done with 0.445
epoch done with 0.440
epoch done with 0.436
epoch done with 0.432
epoch done with 0.429
epoch done with 0.425
epoch done with 0.422
epoch done with 0.419
epoch done with 0.415
epoch done with 0.412
epoch done with 0.410
epoch done with 0.407
epoch done with 0.404
epoch done with 0.402
epoch done with 0.399
epoch done with 0.397
epoch done with 0.395
epoch done with 0.392
epoch done with 0.390
epoch done with 0.388
epoch done with 0.386
epoch done with 0.384
epoch done with 0.383
epoch done with 0.381
epoch done with 0.379
epoch done with 0.378
epoch done with 0.376
epoch done with 0.375
epoch done with 0.374
epoch done with 0.372
epoch done with 0.371
epoch done with 0.370
epoch done with 0.369
epoch done with 0.368
epoch done with 0.367
epoch done with 0.366
epoch done with 0.365
epoch done with 0.364
epoch done with 0.364
Grid search using {'num_samples': 128, 'learning_rate': 0.0001, 'l1_regularization_factor': 0.001, 'l2_regularization_factor': 0.1, 'number_additive_traits': 1, 'model_type': 'tri_state_equilibrium', 'specs': (False, False)}
epoch done with 0.624
epoch done with 0.615
epoch done with 0.606
epoch done with 0.597
epoch done with 0.588
epoch done with 0.579
epoch done with 0.571
epoch done with 0.563
epoch done with 0.555
epoch done with 0.547
epoch done with 0.540
epoch done with 0.533
epoch done with 0.527
epoch done with 0.520
epoch done with 0.514
epoch done with 0.508
epoch done with 0.502
epoch done with 0.496
epoch done with 0.491
epoch done with 0.486
epoch done with 0.480
epoch done with 0.476
epoch done with 0.471
epoch done with 0.466
epoch done with 0.462
epoch done with 0.457
epoch done with 0.453
epoch done with 0.449
epoch done with 0.445
epoch done with 0.442
epoch done with 0.438
epoch done with 0.435
epoch done with 0.431
epoch done with 0.428
epoch done with 0.425
epoch done with 0.422
epoch done with 0.419
epoch done with 0.416
epoch done with 0.414
epoch done with 0.411
epoch done with 0.409
epoch done with 0.406
epoch done with 0.404
epoch done with 0.402
epoch done with 0.400
epoch done with 0.398
epoch done with 0.396
epoch done with 0.394
epoch done with 0.392
epoch done with 0.391
epoch done with 0.389
epoch done with 0.388
epoch done with 0.386
epoch done with 0.385
epoch done with 0.384
epoch done with 0.382
epoch done with 0.381
epoch done with 0.380
epoch done with 0.379
epoch done with 0.378
epoch done with 0.377
epoch done with 0.376
epoch done with 0.375
epoch done with 0.374
epoch done with 0.373
epoch done with 0.373
epoch done with 0.372
epoch done with 0.371
epoch done with 0.371
epoch done with 0.370
epoch done with 0.369
epoch done with 0.369
epoch done with 0.368
epoch done with 0.368
epoch done with 0.367
Grid search using {'num_samples': 128, 'learning_rate': 0.0001, 'l1_regularization_factor': 0.01, 'l2_regularization_factor': 0.0001, 'number_additive_traits': 1, 'model_type': 'tri_state_equilibrium', 'specs': (False, False)}
epoch done with 0.684
epoch done with 0.670
epoch done with 0.657
epoch done with 0.645
epoch done with 0.632
epoch done with 0.620
epoch done with 0.609
epoch done with 0.597
epoch done with 0.587
epoch done with 0.577
epoch done with 0.567
epoch done with 0.558
epoch done with 0.549
epoch done with 0.541
epoch done with 0.533
epoch done with 0.525
epoch done with 0.517
epoch done with 0.510
epoch done with 0.504
epoch done with 0.497
epoch done with 0.491
epoch done with 0.485
epoch done with 0.479
epoch done with 0.474
epoch done with 0.469
epoch done with 0.464
epoch done with 0.459
epoch done with 0.454
epoch done with 0.450
epoch done with 0.446
epoch done with 0.442
epoch done with 0.438
epoch done with 0.434
epoch done with 0.430
epoch done with 0.427
epoch done with 0.424
epoch done with 0.421
epoch done with 0.418
epoch done with 0.415
epoch done with 0.412
epoch done with 0.410
epoch done with 0.408
/camp/home/demetzp/.conda/envs/demetzp/lib/python3.8/site-packages/haiku/_src/data_structures.py:143: FutureWarning: jax.tree_flatten is deprecated, and will be removed in a future release. Use jax.tree_util.tree_flatten instead.
  leaves, treedef = jax.tree_flatten(tree)
/camp/home/demetzp/.conda/envs/demetzp/lib/python3.8/site-packages/haiku/_src/data_structures.py:144: FutureWarning: jax.tree_unflatten is deprecated, and will be removed in a future release. Use jax.tree_util.tree_unflatten instead.
  return jax.tree_unflatten(treedef, leaves)
wandb: Waiting for W&B process to finish... (success).
wandb: 
wandb: Run history:
wandb: val_loss_fit ██▇▇▆▆▅▅▄▄▄▄▃▃▃▃▃▂▂▂▂▂▂▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb: 
wandb: Run summary:
wandb: val_loss_fit 0.368
wandb: 
wandb: 🚀 View run Run 9 at: https://wandb.ai/lab_frohlich/pierre_mochi__fit_tmodel_3state_doubledeepms/runs/y70ugh7q
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20230610_020716-y70ugh7q/logs
wandb: - Waiting for wandb.init()...wandb: \ Waiting for wandb.init()...wandb: wandb version 0.15.4 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.15.3
wandb: Run data is saved locally in /nemo/lab/froehlichf/home/users/demetzp/PierreDeMetz/inst/python/wandb/run-20230610_021116-ib0clqfn
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run Run 10
wandb: ⭐️ View project at https://wandb.ai/lab_frohlich/pierre_mochi__fit_tmodel_3state_doubledeepms
wandb: 🚀 View run at https://wandb.ai/lab_frohlich/pierre_mochi__fit_tmodel_3state_doubledeepms/runs/ib0clqfn
/camp/home/demetzp/.conda/envs/demetzp/lib/python3.8/site-packages/haiku/_src/data_structures.py:143: FutureWarning: jax.tree_flatten is deprecated, and will be removed in a future release. Use jax.tree_util.tree_flatten instead.
  leaves, treedef = jax.tree_flatten(tree)
/camp/home/demetzp/.conda/envs/demetzp/lib/python3.8/site-packages/haiku/_src/data_structures.py:144: FutureWarning: jax.tree_unflatten is deprecated, and will be removed in a future release. Use jax.tree_util.tree_unflatten instead.
  return jax.tree_unflatten(treedef, leaves)
wandb: Waiting for W&B process to finish... (success).
wandb: 
wandb: Run history:
wandb: val_loss_fit ██▇▇▇▆▆▅▅▅▅▄▄▄▄▃▃▃▃▂▂▂▂▂▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁
wandb: 
wandb: Run summary:
wandb: val_loss_fit 0.364
wandb: 
wandb: 🚀 View run Run 10 at: https://wandb.ai/lab_frohlich/pierre_mochi__fit_tmodel_3state_doubledeepms/runs/ib0clqfn
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20230610_021116-ib0clqfn/logs
wandb: - Waiting for wandb.init()...wandb: \ Waiting for wandb.init()...wandb: wandb version 0.15.4 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.15.3
wandb: Run data is saved locally in /nemo/lab/froehlichf/home/users/demetzp/PierreDeMetz/inst/python/wandb/run-20230610_021512-96plq2a0
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run Run 11
wandb: ⭐️ View project at https://wandb.ai/lab_frohlich/pierre_mochi__fit_tmodel_3state_doubledeepms
wandb: 🚀 View run at https://wandb.ai/lab_frohlich/pierre_mochi__fit_tmodel_3state_doubledeepms/runs/96plq2a0
/camp/home/demetzp/.conda/envs/demetzp/lib/python3.8/site-packages/haiku/_src/data_structures.py:143: FutureWarning: jax.tree_flatten is deprecated, and will be removed in a future release. Use jax.tree_util.tree_flatten instead.
  leaves, treedef = jax.tree_flatten(tree)
/camp/home/demetzp/.conda/envs/demetzp/lib/python3.8/site-packages/haiku/_src/data_structures.py:144: FutureWarning: jax.tree_unflatten is deprecated, and will be removed in a future release. Use jax.tree_util.tree_unflatten instead.
  return jax.tree_unflatten(treedef, leaves)
wandb: Waiting for W&B process to finish... (success).
wandb: - 0.005 MB of 0.005 MB uploaded (0.000 MB deduped)wandb: \ 0.005 MB of 0.005 MB uploaded (0.000 MB deduped)wandb: | 0.005 MB of 0.005 MB uploaded (0.000 MB deduped)wandb: / 0.005 MB of 0.005 MB uploaded (0.000 MB deduped)wandb: - 0.005 MB of 0.005 MB uploaded (0.000 MB deduped)wandb: \ 0.005 MB of 0.005 MB uploaded (0.000 MB deduped)wandb: 
wandb: Run history:
wandb: val_loss_fit ██▇▇▆▆▆▅▅▅▄▄▄▄▃▃▃▃▃▂▂▂▂▂▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁
wandb: 
wandb: Run summary:
wandb: val_loss_fit 0.367
wandb: 
wandb: 🚀 View run Run 11 at: https://wandb.ai/lab_frohlich/pierre_mochi__fit_tmodel_3state_doubledeepms/runs/96plq2a0
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20230610_021512-96plq2a0/logs
wandb: - Waiting for wandb.init()...wandb: \ Waiting for wandb.init()...wandb: wandb version 0.15.4 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.15.3
wandb: Run data is saved locally in /nemo/lab/froehlichf/home/users/demetzp/PierreDeMetz/inst/python/wandb/run-20230610_021908-xr9w4vk7
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run Run 12
wandb: ⭐️ View project at https://wandb.ai/lab_frohlich/pierre_mochi__fit_tmodel_3state_doubledeepms
wandb: 🚀 View run at https://wandb.ai/lab_frohlich/pierre_mochi__fit_tmodel_3state_doubledeepms/runs/xr9w4vk7
/camp/home/demetzp/.conda/envs/demetzp/lib/python3.8/site-packages/haiku/_src/data_structures.py:143: FutureWarning: jax.tree_flatten is deprecated, and will be removed in a future release. Use jax.tree_util.tree_flatten instead.
  leaves, treedef = jax.tree_flatten(tree)
/camp/home/demetzp/.conda/envs/demetzp/lib/python3.8/site-packages/haiku/_src/data_structures.py:144: FutureWarning: jax.tree_unflatten is deprecated, and will be removed in a future release. Use jax.tree_util.tree_unflatten instead.
  return jax.tree_unflatten(treedef, leaves)
wandb: Waiting for W&B process to finish... (success).
wandb: 
wandb: Run history:
wandb: val_loss_fit ██▇▇▆▆▅▅▄▄▄▄▃▃▃▃▃▂▂▂▂▂▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb: 
wandb: Run summary:
wandb: val_loss_fit 0.364
wandb: 
wandb: 🚀 View run Run 12 at: https://wandb.ai/lab_frohlich/pierre_mochi__fit_tmodel_3state_doubledeepms/runs/xr9w4vk7
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20230610_021908-xr9w4vk7/logs
wandb: - Waiting for wandb.init()...wandb: \ Waiting for wandb.init()...wandb: wandb version 0.15.4 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.15.3
wandb: Run data is saved locally in /nemo/lab/froehlichf/home/users/demetzp/PierreDeMetz/inst/python/wandb/run-20230610_022305-nsv4455f
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run Run 13
wandb: ⭐️ View project at https://wandb.ai/lab_frohlich/pierre_mochi__fit_tmodel_3state_doubledeepms
wandb: 🚀 View run at https://wandb.ai/lab_frohlich/pierre_mochi__fit_tmodel_3state_doubledeepms/runs/nsv4455f
epoch done with 0.406
epoch done with 0.403
epoch done with 0.401
epoch done with 0.399
epoch done with 0.397
epoch done with 0.396
epoch done with 0.394
epoch done with 0.392
epoch done with 0.390
epoch done with 0.389
epoch done with 0.387
epoch done with 0.386
epoch done with 0.385
epoch done with 0.383
epoch done with 0.382
epoch done with 0.381
epoch done with 0.380
epoch done with 0.379
epoch done with 0.378
epoch done with 0.377
epoch done with 0.376
epoch done with 0.375
epoch done with 0.374
epoch done with 0.373
epoch done with 0.373
epoch done with 0.372
epoch done with 0.371
epoch done with 0.370
epoch done with 0.370
epoch done with 0.369
epoch done with 0.369
epoch done with 0.368
epoch done with 0.368
Grid search using {'num_samples': 128, 'learning_rate': 0.0001, 'l1_regularization_factor': 0.01, 'l2_regularization_factor': 0.001, 'number_additive_traits': 1, 'model_type': 'tri_state_equilibrium', 'specs': (False, False)}
epoch done with 0.979
epoch done with 0.959
epoch done with 0.940
epoch done with 0.921
epoch done with 0.903
epoch done with 0.885
epoch done with 0.867
epoch done with 0.850
epoch done with 0.833
epoch done with 0.817
epoch done with 0.800
epoch done with 0.785
epoch done with 0.769
epoch done with 0.754
epoch done with 0.739
epoch done with 0.725
epoch done with 0.711
epoch done with 0.697
epoch done with 0.684
epoch done with 0.671
epoch done with 0.658
epoch done with 0.646
epoch done with 0.633
epoch done with 0.621
epoch done with 0.609
epoch done with 0.597
epoch done with 0.586
epoch done with 0.574
epoch done with 0.563
epoch done with 0.552
epoch done with 0.542
epoch done with 0.532
epoch done with 0.522
epoch done with 0.513
epoch done with 0.504
epoch done with 0.495
epoch done with 0.487
epoch done with 0.480
epoch done with 0.473
epoch done with 0.466
epoch done with 0.460
epoch done with 0.455
epoch done with 0.449
epoch done with 0.444
epoch done with 0.440
epoch done with 0.435
epoch done with 0.431
epoch done with 0.427
epoch done with 0.423
epoch done with 0.419
epoch done with 0.415
epoch done with 0.412
epoch done with 0.409
epoch done with 0.406
epoch done with 0.403
epoch done with 0.400
epoch done with 0.397
epoch done with 0.395
epoch done with 0.392
epoch done with 0.390
epoch done with 0.387
epoch done with 0.385
epoch done with 0.383
epoch done with 0.381
epoch done with 0.379
epoch done with 0.377
epoch done with 0.376
epoch done with 0.374
epoch done with 0.372
epoch done with 0.371
epoch done with 0.369
epoch done with 0.368
epoch done with 0.366
epoch done with 0.365
epoch done with 0.364
Grid search using {'num_samples': 128, 'learning_rate': 0.0001, 'l1_regularization_factor': 0.01, 'l2_regularization_factor': 0.01, 'number_additive_traits': 1, 'model_type': 'tri_state_equilibrium', 'specs': (False, False)}
epoch done with 0.624
epoch done with 0.615
epoch done with 0.606
epoch done with 0.597
epoch done with 0.588
epoch done with 0.579
epoch done with 0.571
epoch done with 0.563
epoch done with 0.555
epoch done with 0.547
epoch done with 0.540
epoch done with 0.533
epoch done with 0.527
epoch done with 0.520
epoch done with 0.514
epoch done with 0.508
epoch done with 0.502
epoch done with 0.496
epoch done with 0.491
epoch done with 0.485
epoch done with 0.480
epoch done with 0.476
epoch done with 0.471
epoch done with 0.466
epoch done with 0.462
epoch done with 0.457
epoch done with 0.453
epoch done with 0.449
epoch done with 0.445
epoch done with 0.441
epoch done with 0.438
epoch done with 0.434
epoch done with 0.431
epoch done with 0.428
epoch done with 0.425
epoch done with 0.422
epoch done with 0.419
epoch done with 0.416
epoch done with 0.413
epoch done with 0.411
epoch done with 0.409
epoch done with 0.406
epoch done with 0.404
epoch done with 0.402
epoch done with 0.400
epoch done with 0.398
epoch done with 0.396
epoch done with 0.394
epoch done with 0.392
epoch done with 0.391
epoch done with 0.389
epoch done with 0.388
epoch done with 0.386
epoch done with 0.385
epoch done with 0.384
epoch done with 0.382
epoch done with 0.381
epoch done with 0.380
epoch done with 0.379
epoch done with 0.378
epoch done with 0.377
epoch done with 0.376
epoch done with 0.375
epoch done with 0.374
epoch done with 0.373
epoch done with 0.373
epoch done with 0.372
epoch done with 0.371
epoch done with 0.371
epoch done with 0.370
epoch done with 0.369
epoch done with 0.369
epoch done with 0.368
epoch done with 0.368
epoch done with 0.367
Grid search using {'num_samples': 128, 'learning_rate': 0.0001, 'l1_regularization_factor': 0.01, 'l2_regularization_factor': 0.1, 'number_additive_traits': 1, 'model_type': 'tri_state_equilibrium', 'specs': (False, False)}
epoch done with 0.866
epoch done with 0.846
epoch done with 0.826
epoch done with 0.806
epoch done with 0.787
epoch done with 0.769
epoch done with 0.750
epoch done with 0.733
epoch done with 0.716
epoch done with 0.699
epoch done with 0.683
epoch done with 0.668
epoch done with 0.653
epoch done with 0.639
epoch done with 0.625
epoch done with 0.611
epoch done with 0.598
epoch done with 0.586
epoch done with 0.574
epoch done with 0.563
epoch done with 0.552
epoch done with 0.542
epoch done with 0.533
epoch done with 0.524
epoch done with 0.516
epoch done with 0.508
epoch done with 0.500
epoch done with 0.493
epoch done with 0.486
epoch done with 0.480
epoch done with 0.473
epoch done with 0.468
epoch done with 0.462
epoch done with 0.457
epoch done with 0.451
epoch done with 0.447
epoch done with 0.442
epoch done with 0.438
epoch done with 0.433
epoch done with 0.429
epoch done with 0.426
epoch done with 0.422
epoch done with 0.418
epoch done with 0.415
epoch done with 0.412
epoch done with 0.409
epoch done with 0.406
epoch done with 0.404
epoch done with 0.401
epoch done with 0.399
epoch done with 0.397
epoch done with 0.395
epoch done with 0.393
epoch done with 0.391
epoch done with 0.389
epoch done with 0.387
epoch done with 0.385
epoch done with 0.384
epoch done with 0.382
epoch done with 0.381
epoch done with 0.379
epoch done with 0.378
epoch done with 0.377
epoch done with 0.375
epoch done with 0.374
epoch done with 0.373
epoch done with 0.372
epoch done with 0.371
epoch done with 0.370
epoch done with 0.368
epoch done with 0.367
epoch done with 0.367
epoch done with 0.366
epoch done with 0.365
epoch done with 0.364
Grid search using {'num_samples': 128, 'learning_rate': 0.0001, 'l1_regularization_factor': 0.1, 'l2_regularization_factor': 0.0001, 'number_additive_traits': 1, 'model_type': 'tri_state_equilibrium', 'specs': (False, False)}
epoch done with 0.891
epoch done with 0.873
epoch done with 0.856
epoch done with 0.838
epoch done with 0.822
epoch done with 0.807
epoch done with 0.792
epoch done with 0.777
epoch done with 0.763
epoch done with 0.748
epoch done with 0.734
epoch done with 0.720
epoch done with 0.707
epoch done with 0.693
epoch done with 0.680
epoch done with 0.667
epoch done with 0.654
epoch done with 0.642
epoch done with 0.629
epoch done with 0.617
epoch done with 0.605
epoch done with 0.594
epoch done with 0.582
epoch done with 0.571
epoch done with 0.560
epoch done with 0.550
epoch done with 0.540
epoch done with 0.530
epoch done with 0.521
epoch done with 0.512
epoch done with 0.504
epoch done with 0.496
epoch done with 0.489
epoch done with 0.482
epoch done with 0.475
epoch done with 0.469
epoch done with 0.463
epoch done with 0.458
epoch done with 0.453
epoch done with 0.448
epoch done with 0.443
epoch done with 0.439
epoch done with 0.434
epoch done with 0.430
epoch done with 0.427
epoch done with 0.423
epoch done with 0.419
epoch done with 0.416
epoch done with 0.413
epoch done with 0.410
epoch done with 0.407
epoch done with 0.404
epoch done with 0.401
epoch done with 0.399
epoch done with 0.396
epoch done with 0.394
epoch done with 0.392
epoch done with 0.389
epoch done with 0.387
epoch done with 0.385
epoch done with 0.383
epoch done with 0.381
epoch done with 0.380
epoch done with 0.378
epoch done with 0.376
epoch done with 0.375
epoch done with 0.373
epoch done with 0.372
epoch done with 0.371
epoch done with 0.369
epoch done with 0.368
epoch done with 0.367
epoch done with 0.366
/camp/home/demetzp/.conda/envs/demetzp/lib/python3.8/site-packages/haiku/_src/data_structures.py:143: FutureWarning: jax.tree_flatten is deprecated, and will be removed in a future release. Use jax.tree_util.tree_flatten instead.
  leaves, treedef = jax.tree_flatten(tree)
/camp/home/demetzp/.conda/envs/demetzp/lib/python3.8/site-packages/haiku/_src/data_structures.py:144: FutureWarning: jax.tree_unflatten is deprecated, and will be removed in a future release. Use jax.tree_util.tree_unflatten instead.
  return jax.tree_unflatten(treedef, leaves)
wandb: Waiting for W&B process to finish... (success).
wandb: 
wandb: Run history:
wandb: val_loss_fit ██▇▇▆▆▆▅▅▅▅▄▄▄▃▃▃▃▂▂▂▂▂▂▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁
wandb: 
wandb: Run summary:
wandb: val_loss_fit 0.364
wandb: 
wandb: 🚀 View run Run 13 at: https://wandb.ai/lab_frohlich/pierre_mochi__fit_tmodel_3state_doubledeepms/runs/nsv4455f
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20230610_022305-nsv4455f/logs
wandb: - Waiting for wandb.init()...wandb: \ Waiting for wandb.init()...wandb: wandb version 0.15.4 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.15.3
wandb: Run data is saved locally in /nemo/lab/froehlichf/home/users/demetzp/PierreDeMetz/inst/python/wandb/run-20230610_022700-d5l5siu9
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run Run 14
wandb: ⭐️ View project at https://wandb.ai/lab_frohlich/pierre_mochi__fit_tmodel_3state_doubledeepms
wandb: 🚀 View run at https://wandb.ai/lab_frohlich/pierre_mochi__fit_tmodel_3state_doubledeepms/runs/d5l5siu9
/camp/home/demetzp/.conda/envs/demetzp/lib/python3.8/site-packages/haiku/_src/data_structures.py:143: FutureWarning: jax.tree_flatten is deprecated, and will be removed in a future release. Use jax.tree_util.tree_flatten instead.
  leaves, treedef = jax.tree_flatten(tree)
/camp/home/demetzp/.conda/envs/demetzp/lib/python3.8/site-packages/haiku/_src/data_structures.py:144: FutureWarning: jax.tree_unflatten is deprecated, and will be removed in a future release. Use jax.tree_util.tree_unflatten instead.
  return jax.tree_unflatten(treedef, leaves)
wandb: Waiting for W&B process to finish... (success).
wandb: - 0.005 MB of 0.005 MB uploaded (0.000 MB deduped)wandb: \ 0.005 MB of 0.017 MB uploaded (0.000 MB deduped)wandb: | 0.012 MB of 0.017 MB uploaded (0.000 MB deduped)wandb: / 0.012 MB of 0.017 MB uploaded (0.000 MB deduped)wandb: 
wandb: Run history:
wandb: val_loss_fit ██▇▇▆▆▅▅▅▄▄▄▄▃▃▃▃▂▂▂▂▂▂▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb: 
wandb: Run summary:
wandb: val_loss_fit 0.363
wandb: 
wandb: 🚀 View run Run 14 at: https://wandb.ai/lab_frohlich/pierre_mochi__fit_tmodel_3state_doubledeepms/runs/d5l5siu9
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20230610_022700-d5l5siu9/logs
wandb: wandb version 0.15.4 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.15.3
wandb: Run data is saved locally in /nemo/lab/froehlichf/home/users/demetzp/PierreDeMetz/inst/python/wandb/run-20230610_023058-c06z371k
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run Run 15
wandb: ⭐️ View project at https://wandb.ai/lab_frohlich/pierre_mochi__fit_tmodel_3state_doubledeepms
wandb: 🚀 View run at https://wandb.ai/lab_frohlich/pierre_mochi__fit_tmodel_3state_doubledeepms/runs/c06z371k
slurmstepd: error: *** STEP 52959899.0 ON ca151 CANCELLED AT 2023-06-10T02:33:37 DUE TO TIME LIMIT ***
srun: Job step aborted: Waiting up to 32 seconds for job step to finish.
slurmstepd: error: *** JOB 52959899 ON ca151 CANCELLED AT 2023-06-10T02:33:37 DUE TO TIME LIMIT ***
Will exit after finishing currently running jobs (scheduler).
Will exit after finishing currently running jobs (scheduler).
