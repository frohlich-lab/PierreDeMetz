Building DAG of jobs...
Using shell: /usr/bin/bash
Provided cores: 8
Rules claiming more threads will be scaled down.
Provided resources: mem_mb=60000, mem_mib=57221, disk_mb=1000, disk_mib=954
Select jobs to execute...
Building DAG of jobs...
Using shell: /usr/bin/bash
Provided cores: 8
Rules claiming more threads will be scaled down.
Provided resources: mem_mb=60000, mem_mib=57221, disk_mb=1000, disk_mib=954
Select jobs to execute...

[Sat Jun 10 17:45:08 2023]
rule run_model:
    output: Res/PSD95-PDZ3/mochi__fit_tmodel_3state_sparse_dimsum128_subsample100p/two_state_non_equilibrium/model_performance_perepoch.pdf, Res/PSD95-PDZ3/mochi__fit_tmodel_3state_sparse_dimsum128_subsample100p/two_state_non_equilibrium/predicted_fitness.txt, Res/PSD95-PDZ3/mochi__fit_tmodel_3state_sparse_dimsum128_subsample100p/two_state_non_equilibrium/model_weights.txt, Res/PSD95-PDZ3/mochi__fit_tmodel_3state_sparse_dimsum128_subsample100p/two_state_non_equilibrium/model_parameters.txt
    jobid: 0
    reason: Missing output files: Res/PSD95-PDZ3/mochi__fit_tmodel_3state_sparse_dimsum128_subsample100p/two_state_non_equilibrium/model_weights.txt, Res/PSD95-PDZ3/mochi__fit_tmodel_3state_sparse_dimsum128_subsample100p/two_state_non_equilibrium/model_performance_perepoch.pdf, Res/PSD95-PDZ3/mochi__fit_tmodel_3state_sparse_dimsum128_subsample100p/two_state_non_equilibrium/model_parameters.txt, Res/PSD95-PDZ3/mochi__fit_tmodel_3state_sparse_dimsum128_subsample100p/two_state_non_equilibrium/predicted_fitness.txt
    wildcards: protein=PSD95-PDZ3, dataset=mochi__fit_tmodel_3state_sparse_dimsum128_subsample100p, model_type=two_state_non_equilibrium
    resources: mem_mb=60000, mem_mib=57221, disk_mb=1000, disk_mib=954, tmpdir=/tmp/slurm_52971034, slurm_account=u_froehlichf, slurm_partition=cpu

/camp/home/demetzp/.conda/envs/demetzp/lib/python3.8/site-packages/haiku/_src/data_structures.py:37: FutureWarning: jax.tree_structure is deprecated, and will be removed in a future release. Use jax.tree_util.tree_structure instead.
  PyTreeDef = type(jax.tree_structure(None))
No GPU/TPU found, falling back to CPU. (Set TF_CPP_MIN_LOG_LEVEL=0 and rerun for more info.)
wandb: Currently logged in as: demetz-pierre (lab_frohlich). Use `wandb login --relogin` to force relogin
wandb: wandb version 0.15.4 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.15.3
wandb: Run data is saved locally in /nemo/lab/froehlichf/home/users/demetzp/PierreDeMetz/inst/python/wandb/run-20230610_174553-4h87jouk
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run Run 1
wandb: ⭐️ View project at https://wandb.ai/lab_frohlich/pierre_mochi__fit_tmodel_3state_doubledeepms
wandb: 🚀 View run at https://wandb.ai/lab_frohlich/pierre_mochi__fit_tmodel_3state_doubledeepms/runs/4h87jouk
2023-06-10 17:46:05.189662: E external/org_tensorflow/tensorflow/compiler/xla/python/pjit.cc:461] fastpath_data is none
2023-06-10 17:46:17.830729: E external/org_tensorflow/tensorflow/compiler/xla/python/pjit.cc:461] fastpath_data is none
/camp/home/demetzp/.conda/envs/demetzp/lib/python3.8/site-packages/haiku/_src/data_structures.py:143: FutureWarning: jax.tree_flatten is deprecated, and will be removed in a future release. Use jax.tree_util.tree_flatten instead.
  leaves, treedef = jax.tree_flatten(tree)
/camp/home/demetzp/.conda/envs/demetzp/lib/python3.8/site-packages/haiku/_src/data_structures.py:144: FutureWarning: jax.tree_unflatten is deprecated, and will be removed in a future release. Use jax.tree_util.tree_unflatten instead.
  return jax.tree_unflatten(treedef, leaves)
/camp/home/demetzp/.conda/envs/demetzp/lib/python3.8/site-packages/haiku/_src/data_structures.py:143: FutureWarning: jax.tree_flatten is deprecated, and will be removed in a future release. Use jax.tree_util.tree_flatten instead.
  leaves, treedef = jax.tree_flatten(tree)
/camp/home/demetzp/.conda/envs/demetzp/lib/python3.8/site-packages/haiku/_src/data_structures.py:144: FutureWarning: jax.tree_unflatten is deprecated, and will be removed in a future release. Use jax.tree_util.tree_unflatten instead.
  return jax.tree_unflatten(treedef, leaves)
wandb: Waiting for W&B process to finish... (success).
wandb: 
wandb: Run history:
wandb: val_loss_fit ██▇▇▆▆▅▅▅▄▄▄▄▃▃▃▃▂▂▂▂▂▂▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb: 
wandb: Run summary:
wandb: val_loss_fit 0.36
wandb: 
wandb: 🚀 View run Run 1 at: https://wandb.ai/lab_frohlich/pierre_mochi__fit_tmodel_3state_doubledeepms/runs/4h87jouk
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20230610_174553-4h87jouk/logs
wandb: - Waiting for wandb.init()...wandb: \ Waiting for wandb.init()...wandb: wandb version 0.15.4 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.15.3
wandb: Run data is saved locally in /nemo/lab/froehlichf/home/users/demetzp/PierreDeMetz/inst/python/wandb/run-20230610_174840-10ye8ga5
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run Run 2
wandb: ⭐️ View project at https://wandb.ai/lab_frohlich/pierre_mochi__fit_tmodel_3state_doubledeepms
wandb: 🚀 View run at https://wandb.ai/lab_frohlich/pierre_mochi__fit_tmodel_3state_doubledeepms/runs/10ye8ga5
/camp/home/demetzp/.conda/envs/demetzp/lib/python3.8/site-packages/haiku/_src/data_structures.py:143: FutureWarning: jax.tree_flatten is deprecated, and will be removed in a future release. Use jax.tree_util.tree_flatten instead.
  leaves, treedef = jax.tree_flatten(tree)
/camp/home/demetzp/.conda/envs/demetzp/lib/python3.8/site-packages/haiku/_src/data_structures.py:144: FutureWarning: jax.tree_unflatten is deprecated, and will be removed in a future release. Use jax.tree_util.tree_unflatten instead.
  return jax.tree_unflatten(treedef, leaves)
wandb: Waiting for W&B process to finish... (success).
wandb: 
wandb: Run history:
wandb: val_loss_fit ██▇▆▆▆▅▅▄▄▄▄▃▃▃▃▂▂▂▂▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb: 
wandb: Run summary:
wandb: val_loss_fit 0.361
wandb: 
wandb: 🚀 View run Run 2 at: https://wandb.ai/lab_frohlich/pierre_mochi__fit_tmodel_3state_doubledeepms/runs/10ye8ga5
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20230610_174840-10ye8ga5/logs
wandb: - Waiting for wandb.init()...wandb: \ Waiting for wandb.init()...wandb: wandb version 0.15.4 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.15.3
wandb: Run data is saved locally in /nemo/lab/froehlichf/home/users/demetzp/PierreDeMetz/inst/python/wandb/run-20230610_175119-0gjbfnh4
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run Run 3
wandb: ⭐️ View project at https://wandb.ai/lab_frohlich/pierre_mochi__fit_tmodel_3state_doubledeepms
wandb: 🚀 View run at https://wandb.ai/lab_frohlich/pierre_mochi__fit_tmodel_3state_doubledeepms/runs/0gjbfnh4
/camp/home/demetzp/.conda/envs/demetzp/lib/python3.8/site-packages/haiku/_src/data_structures.py:143: FutureWarning: jax.tree_flatten is deprecated, and will be removed in a future release. Use jax.tree_util.tree_flatten instead.
  leaves, treedef = jax.tree_flatten(tree)
/camp/home/demetzp/.conda/envs/demetzp/lib/python3.8/site-packages/haiku/_src/data_structures.py:144: FutureWarning: jax.tree_unflatten is deprecated, and will be removed in a future release. Use jax.tree_util.tree_unflatten instead.
  return jax.tree_unflatten(treedef, leaves)
wandb: Waiting for W&B process to finish... (success).
wandb: - 0.005 MB of 0.005 MB uploaded (0.000 MB deduped)wandb: \ 0.005 MB of 0.017 MB uploaded (0.000 MB deduped)wandb: | 0.005 MB of 0.017 MB uploaded (0.000 MB deduped)wandb: 
wandb: Run history:
wandb: val_loss_fit ██▇▇▆▆▅▅▅▄▄▄▄▃▃▃▃▂▂▂▂▂▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb: 
wandb: Run summary:
wandb: val_loss_fit 0.361
wandb: 
wandb: 🚀 View run Run 3 at: https://wandb.ai/lab_frohlich/pierre_mochi__fit_tmodel_3state_doubledeepms/runs/0gjbfnh4
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20230610_175119-0gjbfnh4/logs
wandb: - Waiting for wandb.init()...wandb: \ Waiting for wandb.init()...wandb: wandb version 0.15.4 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.15.3
wandb: Run data is saved locally in /nemo/lab/froehlichf/home/users/demetzp/PierreDeMetz/inst/python/wandb/run-20230610_175355-bwvuctzg
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run Run 4
wandb: ⭐️ View project at https://wandb.ai/lab_frohlich/pierre_mochi__fit_tmodel_3state_doubledeepms
wandb: 🚀 View run at https://wandb.ai/lab_frohlich/pierre_mochi__fit_tmodel_3state_doubledeepms/runs/bwvuctzg
/camp/home/demetzp/.conda/envs/demetzp/lib/python3.8/site-packages/haiku/_src/data_structures.py:143: FutureWarning: jax.tree_flatten is deprecated, and will be removed in a future release. Use jax.tree_util.tree_flatten instead.
  leaves, treedef = jax.tree_flatten(tree)
/camp/home/demetzp/.conda/envs/demetzp/lib/python3.8/site-packages/haiku/_src/data_structures.py:144: FutureWarning: jax.tree_unflatten is deprecated, and will be removed in a future release. Use jax.tree_util.tree_unflatten instead.
  return jax.tree_unflatten(treedef, leaves)
wandb: Waiting for W&B process to finish... (success).
wandb: - 0.005 MB of 0.005 MB uploaded (0.000 MB deduped)wandb: \ 0.005 MB of 0.017 MB uploaded (0.000 MB deduped)wandb: | 0.011 MB of 0.017 MB uploaded (0.000 MB deduped)wandb: / 0.011 MB of 0.017 MB uploaded (0.000 MB deduped)wandb: 
wandb: Run history:
wandb: val_loss_fit ██▇▇▆▆▅▅▄▄▄▄▃▃▃▃▂▂▂▂▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb: 
wandb: Run summary:
wandb: val_loss_fit 0.36
wandb: 
wandb: 🚀 View run Run 4 at: https://wandb.ai/lab_frohlich/pierre_mochi__fit_tmodel_3state_doubledeepms/runs/bwvuctzg
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20230610_175355-bwvuctzg/logs
wandb: - Waiting for wandb.init()...wandb: \ Waiting for wandb.init()...wandb: wandb version 0.15.4 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.15.3
wandb: Run data is saved locally in /nemo/lab/froehlichf/home/users/demetzp/PierreDeMetz/inst/python/wandb/run-20230610_175630-rhaq3ijx
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run Run 5
wandb: ⭐️ View project at https://wandb.ai/lab_frohlich/pierre_mochi__fit_tmodel_3state_doubledeepms
wandb: 🚀 View run at https://wandb.ai/lab_frohlich/pierre_mochi__fit_tmodel_3state_doubledeepms/runs/rhaq3ijx
Warning: Output model directory already exists.
Warning: Output plot directory already exists.
Warning: Output weights directory already exists.
Warning: Output boostrap directory already exists.
Grid search using {'num_samples': 128, 'learning_rate': 0.0001, 'l1_regularization_factor': 0.0001, 'l2_regularization_factor': 0.0001, 'number_additive_traits': 1, 'model_type': 'two_state_non_equilibrium', 'specs': (False, False)}
epoch done with 0.986
epoch done with 0.962
epoch done with 0.939
epoch done with 0.917
epoch done with 0.896
epoch done with 0.875
epoch done with 0.854
epoch done with 0.834
epoch done with 0.815
epoch done with 0.796
epoch done with 0.778
epoch done with 0.760
epoch done with 0.743
epoch done with 0.726
epoch done with 0.709
epoch done with 0.693
epoch done with 0.677
epoch done with 0.662
epoch done with 0.647
epoch done with 0.632
epoch done with 0.618
epoch done with 0.604
epoch done with 0.591
epoch done with 0.578
epoch done with 0.565
epoch done with 0.553
epoch done with 0.541
epoch done with 0.530
epoch done with 0.519
epoch done with 0.509
epoch done with 0.500
epoch done with 0.492
epoch done with 0.484
epoch done with 0.476
epoch done with 0.469
epoch done with 0.463
epoch done with 0.457
epoch done with 0.452
epoch done with 0.447
epoch done with 0.442
epoch done with 0.437
epoch done with 0.433
epoch done with 0.429
epoch done with 0.425
epoch done with 0.421
epoch done with 0.417
epoch done with 0.414
epoch done with 0.411
epoch done with 0.408
epoch done with 0.405
epoch done with 0.402
epoch done with 0.399
epoch done with 0.397
epoch done with 0.394
epoch done with 0.392
epoch done with 0.390
epoch done with 0.388
epoch done with 0.386
epoch done with 0.384
epoch done with 0.382
epoch done with 0.380
epoch done with 0.378
epoch done with 0.377
epoch done with 0.375
epoch done with 0.374
epoch done with 0.372
epoch done with 0.371
epoch done with 0.369
epoch done with 0.368
epoch done with 0.367
epoch done with 0.366
epoch done with 0.364
epoch done with 0.363
epoch done with 0.362
epoch done with 0.360
Grid search using {'num_samples': 128, 'learning_rate': 0.0001, 'l1_regularization_factor': 0.0001, 'l2_regularization_factor': 0.001, 'number_additive_traits': 1, 'model_type': 'two_state_non_equilibrium', 'specs': (False, False)}
epoch done with 1.186
epoch done with 1.146
epoch done with 1.108
epoch done with 1.073
epoch done with 1.039
epoch done with 1.006
epoch done with 0.976
epoch done with 0.947
epoch done with 0.919
epoch done with 0.892
epoch done with 0.867
epoch done with 0.842
epoch done with 0.819
epoch done with 0.797
epoch done with 0.775
epoch done with 0.754
epoch done with 0.734
epoch done with 0.715
epoch done with 0.696
epoch done with 0.678
epoch done with 0.660
epoch done with 0.644
epoch done with 0.627
epoch done with 0.612
epoch done with 0.596
epoch done with 0.582
epoch done with 0.568
epoch done with 0.555
epoch done with 0.542
epoch done with 0.530
epoch done with 0.519
epoch done with 0.509
epoch done with 0.499
epoch done with 0.490
epoch done with 0.481
epoch done with 0.474
epoch done with 0.466
epoch done with 0.459
epoch done with 0.453
epoch done with 0.448
epoch done with 0.442
epoch done with 0.437
epoch done with 0.432
epoch done with 0.428
epoch done with 0.423
epoch done with 0.419
epoch done with 0.415
epoch done with 0.412
epoch done with 0.409
epoch done with 0.405
epoch done with 0.402
epoch done with 0.400
epoch done with 0.397
epoch done with 0.395
epoch done with 0.392
epoch done with 0.390
epoch done with 0.388
epoch done with 0.386
epoch done with 0.384
epoch done with 0.382
epoch done with 0.380
epoch done with 0.379
epoch done with 0.377
epoch done with 0.375
epoch done with 0.374
epoch done with 0.372
epoch done with 0.371
epoch done with 0.370
epoch done with 0.368
epoch done with 0.367
epoch done with 0.366
epoch done with 0.364
epoch done with 0.363
epoch done with 0.362
epoch done with 0.361
Grid search using {'num_samples': 128, 'learning_rate': 0.0001, 'l1_regularization_factor': 0.0001, 'l2_regularization_factor': 0.01, 'number_additive_traits': 1, 'model_type': 'two_state_non_equilibrium', 'specs': (False, False)}
epoch done with 0.973
epoch done with 0.950
epoch done with 0.928
epoch done with 0.906
epoch done with 0.885
epoch done with 0.864
epoch done with 0.844
epoch done with 0.824
epoch done with 0.805
epoch done with 0.787
epoch done with 0.769
epoch done with 0.751
epoch done with 0.734
epoch done with 0.717
epoch done with 0.701
epoch done with 0.685
epoch done with 0.670
epoch done with 0.655
epoch done with 0.640
epoch done with 0.626
epoch done with 0.612
epoch done with 0.598
epoch done with 0.585
epoch done with 0.572
epoch done with 0.560
epoch done with 0.548
epoch done with 0.537
epoch done with 0.526
epoch done with 0.516
epoch done with 0.506
epoch done with 0.497
epoch done with 0.489
epoch done with 0.482
epoch done with 0.474
epoch done with 0.468
epoch done with 0.462
epoch done with 0.456
epoch done with 0.451
epoch done with 0.445
epoch done with 0.441
epoch done with 0.436
epoch done with 0.432
epoch done with 0.428
epoch done with 0.424
epoch done with 0.420
epoch done with 0.417
epoch done with 0.413
epoch done with 0.410
epoch done with 0.407
epoch done with 0.404
epoch done with 0.402
epoch done with 0.399
epoch done with 0.397
epoch done with 0.394
epoch done with 0.392
epoch done with 0.390
epoch done with 0.387
epoch done with 0.385
epoch done with 0.383
epoch done with 0.381
epoch done with 0.380
epoch done with 0.378
epoch done with 0.376
epoch done with 0.375
epoch done with 0.373
epoch done with 0.372
epoch done with 0.371
epoch done with 0.369
epoch done with 0.368
epoch done with 0.367
epoch done with 0.366
epoch done with 0.365
epoch done with 0.363
epoch done with 0.362
epoch done with 0.361
Grid search using {'num_samples': 128, 'learning_rate': 0.0001, 'l1_regularization_factor': 0.0001, 'l2_regularization_factor': 0.1, 'number_additive_traits': 1, 'model_type': 'two_state_non_equilibrium', 'specs': (False, False)}
epoch done with 1.322
epoch done with 1.276
epoch done with 1.233
epoch done with 1.192
epoch done with 1.153
epoch done with 1.117
epoch done with 1.082
epoch done with 1.048
epoch done with 1.017
epoch done with 0.986
epoch done with 0.957
epoch done with 0.929
epoch done with 0.903
epoch done with 0.878
epoch done with 0.853
epoch done with 0.829
epoch done with 0.807
epoch done with 0.785
epoch done with 0.764
epoch done with 0.743
epoch done with 0.723
epoch done with 0.704
epoch done with 0.685
epoch done with 0.667
epoch done with 0.650
epoch done with 0.633
epoch done with 0.617
epoch done with 0.601
epoch done with 0.586
epoch done with 0.572
epoch done with 0.558
epoch done with 0.545
epoch done with 0.532
epoch done with 0.520
epoch done with 0.509
epoch done with 0.499
epoch done with 0.489
epoch done with 0.480
epoch done with 0.472
epoch done with 0.465
epoch done with 0.457
epoch done with 0.451
epoch done with 0.444
epoch done with 0.438
epoch done with 0.433
epoch done with 0.428
epoch done with 0.423
epoch done with 0.419
epoch done with 0.415
epoch done with 0.411
epoch done with 0.408
epoch done with 0.404
epoch done with 0.401
epoch done with 0.398
epoch done with 0.396
epoch done with 0.393
epoch done with 0.391
epoch done with 0.388
epoch done with 0.386
epoch done with 0.384
epoch done with 0.382
epoch done with 0.380
epoch done with 0.378
epoch done with 0.377
epoch done with 0.375
epoch done with 0.373
epoch done with 0.372
epoch done with 0.370
epoch done with 0.369
epoch done with 0.367
epoch done with 0.366
epoch done with 0.364
epoch done with 0.363
epoch done with 0.362
epoch done with 0.360
Grid search using {'num_samples': 128, 'learning_rate': 0.0001, 'l1_regularization_factor': 0.001, 'l2_regularization_factor': 0.0001, 'number_additive_traits': 1, 'model_type': 'two_state_non_equilibrium', 'specs': (False, False)}
epoch done with 1.275
epoch done with 1.246
epoch done with 1.217
epoch done with 1.188
epoch done with 1.161
epoch done with 1.135
epoch done with 1.109
epoch done with 1.083
epoch done with 1.059
epoch done with 1.035
/camp/home/demetzp/.conda/envs/demetzp/lib/python3.8/site-packages/haiku/_src/data_structures.py:143: FutureWarning: jax.tree_flatten is deprecated, and will be removed in a future release. Use jax.tree_util.tree_flatten instead.
  leaves, treedef = jax.tree_flatten(tree)
/camp/home/demetzp/.conda/envs/demetzp/lib/python3.8/site-packages/haiku/_src/data_structures.py:144: FutureWarning: jax.tree_unflatten is deprecated, and will be removed in a future release. Use jax.tree_util.tree_unflatten instead.
  return jax.tree_unflatten(treedef, leaves)
wandb: Waiting for W&B process to finish... (success).
wandb: - 0.005 MB of 0.005 MB uploaded (0.000 MB deduped)wandb: \ 0.005 MB of 0.017 MB uploaded (0.000 MB deduped)wandb: | 0.011 MB of 0.017 MB uploaded (0.000 MB deduped)wandb: / 0.011 MB of 0.017 MB uploaded (0.000 MB deduped)wandb: 
wandb: Run history:
wandb: val_loss_fit ██▇▇▇▆▆▅▅▅▅▄▄▄▄▃▃▃▃▃▂▂▂▂▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁
wandb: 
wandb: Run summary:
wandb: val_loss_fit 0.366
wandb: 
wandb: 🚀 View run Run 5 at: https://wandb.ai/lab_frohlich/pierre_mochi__fit_tmodel_3state_doubledeepms/runs/rhaq3ijx
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20230610_175630-rhaq3ijx/logs
wandb: - Waiting for wandb.init()...wandb: \ Waiting for wandb.init()...wandb: wandb version 0.15.4 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.15.3
wandb: Run data is saved locally in /nemo/lab/froehlichf/home/users/demetzp/PierreDeMetz/inst/python/wandb/run-20230610_175905-i6ey22v3
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run Run 6
wandb: ⭐️ View project at https://wandb.ai/lab_frohlich/pierre_mochi__fit_tmodel_3state_doubledeepms
wandb: 🚀 View run at https://wandb.ai/lab_frohlich/pierre_mochi__fit_tmodel_3state_doubledeepms/runs/i6ey22v3
/camp/home/demetzp/.conda/envs/demetzp/lib/python3.8/site-packages/haiku/_src/data_structures.py:143: FutureWarning: jax.tree_flatten is deprecated, and will be removed in a future release. Use jax.tree_util.tree_flatten instead.
  leaves, treedef = jax.tree_flatten(tree)
/camp/home/demetzp/.conda/envs/demetzp/lib/python3.8/site-packages/haiku/_src/data_structures.py:144: FutureWarning: jax.tree_unflatten is deprecated, and will be removed in a future release. Use jax.tree_util.tree_unflatten instead.
  return jax.tree_unflatten(treedef, leaves)
wandb: Waiting for W&B process to finish... (success).
wandb: 
wandb: Run history:
wandb: val_loss_fit ██▇▇▆▆▅▅▅▄▄▄▄▃▃▃▃▂▂▂▂▂▂▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb: 
wandb: Run summary:
wandb: val_loss_fit 0.361
wandb: 
wandb: 🚀 View run Run 6 at: https://wandb.ai/lab_frohlich/pierre_mochi__fit_tmodel_3state_doubledeepms/runs/i6ey22v3
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20230610_175905-i6ey22v3/logs
wandb: - Waiting for wandb.init()...wandb: \ Waiting for wandb.init()...wandb: wandb version 0.15.4 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.15.3
wandb: Run data is saved locally in /nemo/lab/froehlichf/home/users/demetzp/PierreDeMetz/inst/python/wandb/run-20230610_180139-3u4rz729
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run Run 7
wandb: ⭐️ View project at https://wandb.ai/lab_frohlich/pierre_mochi__fit_tmodel_3state_doubledeepms
wandb: 🚀 View run at https://wandb.ai/lab_frohlich/pierre_mochi__fit_tmodel_3state_doubledeepms/runs/3u4rz729
/camp/home/demetzp/.conda/envs/demetzp/lib/python3.8/site-packages/haiku/_src/data_structures.py:143: FutureWarning: jax.tree_flatten is deprecated, and will be removed in a future release. Use jax.tree_util.tree_flatten instead.
  leaves, treedef = jax.tree_flatten(tree)
/camp/home/demetzp/.conda/envs/demetzp/lib/python3.8/site-packages/haiku/_src/data_structures.py:144: FutureWarning: jax.tree_unflatten is deprecated, and will be removed in a future release. Use jax.tree_util.tree_unflatten instead.
  return jax.tree_unflatten(treedef, leaves)
wandb: Waiting for W&B process to finish... (success).
wandb: 
wandb: Run history:
wandb: val_loss_fit ██▇▇▆▆▆▅▅▄▄▄▄▃▃▃▃▂▂▂▂▂▂▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb: 
wandb: Run summary:
wandb: val_loss_fit 0.361
wandb: 
wandb: 🚀 View run Run 7 at: https://wandb.ai/lab_frohlich/pierre_mochi__fit_tmodel_3state_doubledeepms/runs/3u4rz729
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20230610_180139-3u4rz729/logs
wandb: - Waiting for wandb.init()...wandb: \ Waiting for wandb.init()...wandb: wandb version 0.15.4 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.15.3
wandb: Run data is saved locally in /nemo/lab/froehlichf/home/users/demetzp/PierreDeMetz/inst/python/wandb/run-20230610_180411-jc3w9t18
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run Run 8
wandb: ⭐️ View project at https://wandb.ai/lab_frohlich/pierre_mochi__fit_tmodel_3state_doubledeepms
wandb: 🚀 View run at https://wandb.ai/lab_frohlich/pierre_mochi__fit_tmodel_3state_doubledeepms/runs/jc3w9t18
/camp/home/demetzp/.conda/envs/demetzp/lib/python3.8/site-packages/haiku/_src/data_structures.py:143: FutureWarning: jax.tree_flatten is deprecated, and will be removed in a future release. Use jax.tree_util.tree_flatten instead.
  leaves, treedef = jax.tree_flatten(tree)
/camp/home/demetzp/.conda/envs/demetzp/lib/python3.8/site-packages/haiku/_src/data_structures.py:144: FutureWarning: jax.tree_unflatten is deprecated, and will be removed in a future release. Use jax.tree_util.tree_unflatten instead.
  return jax.tree_unflatten(treedef, leaves)
wandb: Waiting for W&B process to finish... (success).
wandb: - 0.005 MB of 0.005 MB uploaded (0.000 MB deduped)wandb: \ 0.005 MB of 0.017 MB uploaded (0.000 MB deduped)wandb: | 0.005 MB of 0.017 MB uploaded (0.000 MB deduped)wandb: 
wandb: Run history:
wandb: val_loss_fit ██▇▇▆▆▆▅▅▅▄▄▄▄▃▃▃▃▃▂▂▂▂▂▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁
wandb: 
wandb: Run summary:
wandb: val_loss_fit 0.367
wandb: 
wandb: 🚀 View run Run 8 at: https://wandb.ai/lab_frohlich/pierre_mochi__fit_tmodel_3state_doubledeepms/runs/jc3w9t18
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20230610_180411-jc3w9t18/logs
wandb: - Waiting for wandb.init()...wandb: \ Waiting for wandb.init()...wandb: wandb version 0.15.4 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.15.3
wandb: Run data is saved locally in /nemo/lab/froehlichf/home/users/demetzp/PierreDeMetz/inst/python/wandb/run-20230610_180640-ep342wtd
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run Run 9
wandb: ⭐️ View project at https://wandb.ai/lab_frohlich/pierre_mochi__fit_tmodel_3state_doubledeepms
wandb: 🚀 View run at https://wandb.ai/lab_frohlich/pierre_mochi__fit_tmodel_3state_doubledeepms/runs/ep342wtd
epoch done with 1.012
epoch done with 0.989
epoch done with 0.967
epoch done with 0.946
epoch done with 0.925
epoch done with 0.905
epoch done with 0.885
epoch done with 0.865
epoch done with 0.846
epoch done with 0.828
epoch done with 0.809
epoch done with 0.792
epoch done with 0.774
epoch done with 0.757
epoch done with 0.740
epoch done with 0.724
epoch done with 0.708
epoch done with 0.692
epoch done with 0.677
epoch done with 0.662
epoch done with 0.647
epoch done with 0.632
epoch done with 0.618
epoch done with 0.604
epoch done with 0.591
epoch done with 0.578
epoch done with 0.565
epoch done with 0.552
epoch done with 0.540
epoch done with 0.529
epoch done with 0.517
epoch done with 0.507
epoch done with 0.497
epoch done with 0.488
epoch done with 0.479
epoch done with 0.471
epoch done with 0.463
epoch done with 0.456
epoch done with 0.449
epoch done with 0.443
epoch done with 0.437
epoch done with 0.432
epoch done with 0.427
epoch done with 0.422
epoch done with 0.417
epoch done with 0.413
epoch done with 0.409
epoch done with 0.405
epoch done with 0.402
epoch done with 0.399
epoch done with 0.396
epoch done with 0.393
epoch done with 0.390
epoch done with 0.388
epoch done with 0.385
epoch done with 0.383
epoch done with 0.381
epoch done with 0.378
epoch done with 0.376
epoch done with 0.374
epoch done with 0.373
epoch done with 0.371
epoch done with 0.369
epoch done with 0.367
epoch done with 0.366
Grid search using {'num_samples': 128, 'learning_rate': 0.0001, 'l1_regularization_factor': 0.001, 'l2_regularization_factor': 0.001, 'number_additive_traits': 1, 'model_type': 'two_state_non_equilibrium', 'specs': (False, False)}
epoch done with 0.971
epoch done with 0.948
epoch done with 0.925
epoch done with 0.904
epoch done with 0.883
epoch done with 0.862
epoch done with 0.842
epoch done with 0.823
epoch done with 0.804
epoch done with 0.785
epoch done with 0.767
epoch done with 0.750
epoch done with 0.733
epoch done with 0.716
epoch done with 0.700
epoch done with 0.684
epoch done with 0.669
epoch done with 0.654
epoch done with 0.639
epoch done with 0.625
epoch done with 0.611
epoch done with 0.597
epoch done with 0.584
epoch done with 0.572
epoch done with 0.559
epoch done with 0.547
epoch done with 0.536
epoch done with 0.526
epoch done with 0.516
epoch done with 0.506
epoch done with 0.497
epoch done with 0.489
epoch done with 0.482
epoch done with 0.475
epoch done with 0.468
epoch done with 0.462
epoch done with 0.456
epoch done with 0.451
epoch done with 0.446
epoch done with 0.441
epoch done with 0.437
epoch done with 0.432
epoch done with 0.428
epoch done with 0.424
epoch done with 0.421
epoch done with 0.417
epoch done with 0.414
epoch done with 0.411
epoch done with 0.408
epoch done with 0.405
epoch done with 0.402
epoch done with 0.400
epoch done with 0.397
epoch done with 0.395
epoch done with 0.392
epoch done with 0.390
epoch done with 0.388
epoch done with 0.386
epoch done with 0.384
epoch done with 0.382
epoch done with 0.380
epoch done with 0.378
epoch done with 0.377
epoch done with 0.375
epoch done with 0.374
epoch done with 0.373
epoch done with 0.371
epoch done with 0.370
epoch done with 0.369
epoch done with 0.367
epoch done with 0.366
epoch done with 0.365
epoch done with 0.364
epoch done with 0.363
epoch done with 0.361
Grid search using {'num_samples': 128, 'learning_rate': 0.0001, 'l1_regularization_factor': 0.001, 'l2_regularization_factor': 0.01, 'number_additive_traits': 1, 'model_type': 'two_state_non_equilibrium', 'specs': (False, False)}
epoch done with 0.999
epoch done with 0.975
epoch done with 0.952
epoch done with 0.930
epoch done with 0.909
epoch done with 0.888
epoch done with 0.867
epoch done with 0.847
epoch done with 0.828
epoch done with 0.809
epoch done with 0.790
epoch done with 0.772
epoch done with 0.755
epoch done with 0.738
epoch done with 0.721
epoch done with 0.705
epoch done with 0.689
epoch done with 0.674
epoch done with 0.659
epoch done with 0.644
epoch done with 0.629
epoch done with 0.615
epoch done with 0.602
epoch done with 0.588
epoch done with 0.575
epoch done with 0.563
epoch done with 0.551
epoch done with 0.539
epoch done with 0.528
epoch done with 0.518
epoch done with 0.508
epoch done with 0.499
epoch done with 0.491
epoch done with 0.483
epoch done with 0.475
epoch done with 0.468
epoch done with 0.462
epoch done with 0.456
epoch done with 0.451
epoch done with 0.446
epoch done with 0.441
epoch done with 0.436
epoch done with 0.432
epoch done with 0.428
epoch done with 0.424
epoch done with 0.420
epoch done with 0.416
epoch done with 0.413
epoch done with 0.410
epoch done with 0.407
epoch done with 0.404
epoch done with 0.401
epoch done with 0.399
epoch done with 0.396
epoch done with 0.394
epoch done with 0.391
epoch done with 0.389
epoch done with 0.387
epoch done with 0.385
epoch done with 0.383
epoch done with 0.381
epoch done with 0.379
epoch done with 0.377
epoch done with 0.376
epoch done with 0.374
epoch done with 0.373
epoch done with 0.371
epoch done with 0.370
epoch done with 0.368
epoch done with 0.367
epoch done with 0.366
epoch done with 0.365
epoch done with 0.363
epoch done with 0.362
epoch done with 0.361
Grid search using {'num_samples': 128, 'learning_rate': 0.0001, 'l1_regularization_factor': 0.001, 'l2_regularization_factor': 0.1, 'number_additive_traits': 1, 'model_type': 'two_state_non_equilibrium', 'specs': (False, False)}
epoch done with 0.624
epoch done with 0.615
epoch done with 0.606
epoch done with 0.597
epoch done with 0.588
epoch done with 0.579
epoch done with 0.571
epoch done with 0.563
epoch done with 0.555
epoch done with 0.547
epoch done with 0.540
epoch done with 0.533
epoch done with 0.527
epoch done with 0.520
epoch done with 0.514
epoch done with 0.508
epoch done with 0.502
epoch done with 0.496
epoch done with 0.491
epoch done with 0.486
epoch done with 0.480
epoch done with 0.476
epoch done with 0.471
epoch done with 0.466
epoch done with 0.462
epoch done with 0.457
epoch done with 0.453
epoch done with 0.449
epoch done with 0.445
epoch done with 0.442
epoch done with 0.438
epoch done with 0.435
epoch done with 0.431
epoch done with 0.428
epoch done with 0.425
epoch done with 0.422
epoch done with 0.419
epoch done with 0.416
epoch done with 0.414
epoch done with 0.411
epoch done with 0.409
epoch done with 0.406
epoch done with 0.404
epoch done with 0.402
epoch done with 0.400
epoch done with 0.398
epoch done with 0.396
epoch done with 0.394
epoch done with 0.392
epoch done with 0.391
epoch done with 0.389
epoch done with 0.388
epoch done with 0.386
epoch done with 0.385
epoch done with 0.384
epoch done with 0.382
epoch done with 0.381
epoch done with 0.380
epoch done with 0.379
epoch done with 0.378
epoch done with 0.377
epoch done with 0.376
epoch done with 0.375
epoch done with 0.374
epoch done with 0.373
epoch done with 0.373
epoch done with 0.372
epoch done with 0.371
epoch done with 0.371
epoch done with 0.370
epoch done with 0.369
epoch done with 0.369
epoch done with 0.368
epoch done with 0.368
epoch done with 0.367
Grid search using {'num_samples': 128, 'learning_rate': 0.0001, 'l1_regularization_factor': 0.01, 'l2_regularization_factor': 0.0001, 'number_additive_traits': 1, 'model_type': 'two_state_non_equilibrium', 'specs': (False, False)}
epoch done with 0.745
epoch done with 0.727
epoch done with 0.710
epoch done with 0.693
epoch done with 0.677
epoch done with 0.661
epoch done with 0.646
epoch done with 0.631
epoch done with 0.617
epoch done with 0.604
epoch done with 0.591
epoch done with 0.579
epoch done with 0.567
epoch done with 0.557
epoch done with 0.547
epoch done with 0.538
epoch done with 0.529
epoch done with 0.521
epoch done with 0.513
epoch done with 0.505
epoch done with 0.498
epoch done with 0.491
epoch done with 0.485
epoch done with 0.479
epoch done with 0.473
epoch done with 0.467
epoch done with 0.462
epoch done with 0.457
epoch done with 0.452
epoch done with 0.448
epoch done with 0.443
epoch done with 0.439
epoch done with 0.435
epoch done with 0.432
epoch done with 0.428
epoch done with 0.425
epoch done with 0.421
epoch done with 0.418
epoch done with 0.415
epoch done with 0.412
/camp/home/demetzp/.conda/envs/demetzp/lib/python3.8/site-packages/haiku/_src/data_structures.py:143: FutureWarning: jax.tree_flatten is deprecated, and will be removed in a future release. Use jax.tree_util.tree_flatten instead.
  leaves, treedef = jax.tree_flatten(tree)
/camp/home/demetzp/.conda/envs/demetzp/lib/python3.8/site-packages/haiku/_src/data_structures.py:144: FutureWarning: jax.tree_unflatten is deprecated, and will be removed in a future release. Use jax.tree_util.tree_unflatten instead.
  return jax.tree_unflatten(treedef, leaves)
wandb: Waiting for W&B process to finish... (success).
wandb: 
wandb: Run history:
wandb: val_loss_fit ██▇▆▆▅▅▅▄▄▄▃▃▃▃▃▂▂▂▂▂▂▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb: 
wandb: Run summary:
wandb: val_loss_fit 0.368
wandb: 
wandb: 🚀 View run Run 9 at: https://wandb.ai/lab_frohlich/pierre_mochi__fit_tmodel_3state_doubledeepms/runs/ep342wtd
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20230610_180640-ep342wtd/logs
wandb: - Waiting for wandb.init()...wandb: \ Waiting for wandb.init()...wandb: wandb version 0.15.4 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.15.3
wandb: Run data is saved locally in /nemo/lab/froehlichf/home/users/demetzp/PierreDeMetz/inst/python/wandb/run-20230610_180910-loteca8j
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run Run 10
wandb: ⭐️ View project at https://wandb.ai/lab_frohlich/pierre_mochi__fit_tmodel_3state_doubledeepms
wandb: 🚀 View run at https://wandb.ai/lab_frohlich/pierre_mochi__fit_tmodel_3state_doubledeepms/runs/loteca8j
/camp/home/demetzp/.conda/envs/demetzp/lib/python3.8/site-packages/haiku/_src/data_structures.py:143: FutureWarning: jax.tree_flatten is deprecated, and will be removed in a future release. Use jax.tree_util.tree_flatten instead.
  leaves, treedef = jax.tree_flatten(tree)
/camp/home/demetzp/.conda/envs/demetzp/lib/python3.8/site-packages/haiku/_src/data_structures.py:144: FutureWarning: jax.tree_unflatten is deprecated, and will be removed in a future release. Use jax.tree_util.tree_unflatten instead.
  return jax.tree_unflatten(treedef, leaves)
wandb: Waiting for W&B process to finish... (success).
wandb: 
wandb: Run history:
wandb: val_loss_fit ██▇▇▆▆▆▅▅▅▄▄▄▄▃▃▃▃▃▂▂▂▂▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb: 
wandb: Run summary:
wandb: val_loss_fit 0.365
wandb: 
wandb: 🚀 View run Run 10 at: https://wandb.ai/lab_frohlich/pierre_mochi__fit_tmodel_3state_doubledeepms/runs/loteca8j
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20230610_180910-loteca8j/logs
wandb: - Waiting for wandb.init()...wandb: \ Waiting for wandb.init()...wandb: wandb version 0.15.4 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.15.3
wandb: Run data is saved locally in /nemo/lab/froehlichf/home/users/demetzp/PierreDeMetz/inst/python/wandb/run-20230610_181148-n5r0e07y
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run Run 11
wandb: ⭐️ View project at https://wandb.ai/lab_frohlich/pierre_mochi__fit_tmodel_3state_doubledeepms
wandb: 🚀 View run at https://wandb.ai/lab_frohlich/pierre_mochi__fit_tmodel_3state_doubledeepms/runs/n5r0e07y
/camp/home/demetzp/.conda/envs/demetzp/lib/python3.8/site-packages/haiku/_src/data_structures.py:143: FutureWarning: jax.tree_flatten is deprecated, and will be removed in a future release. Use jax.tree_util.tree_flatten instead.
  leaves, treedef = jax.tree_flatten(tree)
/camp/home/demetzp/.conda/envs/demetzp/lib/python3.8/site-packages/haiku/_src/data_structures.py:144: FutureWarning: jax.tree_unflatten is deprecated, and will be removed in a future release. Use jax.tree_util.tree_unflatten instead.
  return jax.tree_unflatten(treedef, leaves)
wandb: Waiting for W&B process to finish... (success).
wandb: - 0.005 MB of 0.005 MB uploaded (0.000 MB deduped)wandb: \ 0.005 MB of 0.017 MB uploaded (0.000 MB deduped)wandb: | 0.011 MB of 0.017 MB uploaded (0.000 MB deduped)wandb: / 0.011 MB of 0.017 MB uploaded (0.000 MB deduped)wandb: 
wandb: Run history:
wandb: val_loss_fit ██▇▇▆▆▆▅▅▅▄▄▄▄▃▃▃▃▃▂▂▂▂▂▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁
wandb: 
wandb: Run summary:
wandb: val_loss_fit 0.367
wandb: 
wandb: 🚀 View run Run 11 at: https://wandb.ai/lab_frohlich/pierre_mochi__fit_tmodel_3state_doubledeepms/runs/n5r0e07y
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20230610_181148-n5r0e07y/logs
wandb: - Waiting for wandb.init()...wandb: \ Waiting for wandb.init()...wandb: wandb version 0.15.4 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.15.3
wandb: Run data is saved locally in /nemo/lab/froehlichf/home/users/demetzp/PierreDeMetz/inst/python/wandb/run-20230610_181421-lp2fxqe1
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run Run 12
wandb: ⭐️ View project at https://wandb.ai/lab_frohlich/pierre_mochi__fit_tmodel_3state_doubledeepms
wandb: 🚀 View run at https://wandb.ai/lab_frohlich/pierre_mochi__fit_tmodel_3state_doubledeepms/runs/lp2fxqe1
/camp/home/demetzp/.conda/envs/demetzp/lib/python3.8/site-packages/haiku/_src/data_structures.py:143: FutureWarning: jax.tree_flatten is deprecated, and will be removed in a future release. Use jax.tree_util.tree_flatten instead.
  leaves, treedef = jax.tree_flatten(tree)
/camp/home/demetzp/.conda/envs/demetzp/lib/python3.8/site-packages/haiku/_src/data_structures.py:144: FutureWarning: jax.tree_unflatten is deprecated, and will be removed in a future release. Use jax.tree_util.tree_unflatten instead.
  return jax.tree_unflatten(treedef, leaves)
wandb: Waiting for W&B process to finish... (success).
wandb: 
wandb: Run history:
wandb: val_loss_fit ██▇▆▆▅▅▄▄▄▄▃▃▃▃▂▂▂▂▂▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb: 
wandb: Run summary:
wandb: val_loss_fit 0.361
wandb: 
wandb: 🚀 View run Run 12 at: https://wandb.ai/lab_frohlich/pierre_mochi__fit_tmodel_3state_doubledeepms/runs/lp2fxqe1
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20230610_181421-lp2fxqe1/logs
wandb: - Waiting for wandb.init()...wandb: \ Waiting for wandb.init()...wandb: wandb version 0.15.4 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.15.3
wandb: Run data is saved locally in /nemo/lab/froehlichf/home/users/demetzp/PierreDeMetz/inst/python/wandb/run-20230610_181657-fqti09dz
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run Run 13
wandb: ⭐️ View project at https://wandb.ai/lab_frohlich/pierre_mochi__fit_tmodel_3state_doubledeepms
wandb: 🚀 View run at https://wandb.ai/lab_frohlich/pierre_mochi__fit_tmodel_3state_doubledeepms/runs/fqti09dz
epoch done with 0.410
epoch done with 0.408
epoch done with 0.406
epoch done with 0.403
epoch done with 0.401
epoch done with 0.400
epoch done with 0.398
epoch done with 0.396
epoch done with 0.394
epoch done with 0.392
epoch done with 0.391
epoch done with 0.389
epoch done with 0.388
epoch done with 0.386
epoch done with 0.385
epoch done with 0.384
epoch done with 0.382
epoch done with 0.381
epoch done with 0.380
epoch done with 0.379
epoch done with 0.378
epoch done with 0.377
epoch done with 0.376
epoch done with 0.375
epoch done with 0.374
epoch done with 0.373
epoch done with 0.373
epoch done with 0.372
epoch done with 0.371
epoch done with 0.371
epoch done with 0.370
epoch done with 0.369
epoch done with 0.369
epoch done with 0.368
epoch done with 0.368
Grid search using {'num_samples': 128, 'learning_rate': 0.0001, 'l1_regularization_factor': 0.01, 'l2_regularization_factor': 0.001, 'number_additive_traits': 1, 'model_type': 'two_state_non_equilibrium', 'specs': (False, False)}
epoch done with 1.357
epoch done with 1.317
epoch done with 1.280
epoch done with 1.244
epoch done with 1.209
epoch done with 1.176
epoch done with 1.145
epoch done with 1.115
epoch done with 1.086
epoch done with 1.059
epoch done with 1.032
epoch done with 1.007
epoch done with 0.982
epoch done with 0.958
epoch done with 0.936
epoch done with 0.913
epoch done with 0.892
epoch done with 0.871
epoch done with 0.851
epoch done with 0.833
epoch done with 0.815
epoch done with 0.797
epoch done with 0.780
epoch done with 0.763
epoch done with 0.746
epoch done with 0.730
epoch done with 0.714
epoch done with 0.698
epoch done with 0.682
epoch done with 0.667
epoch done with 0.652
epoch done with 0.638
epoch done with 0.623
epoch done with 0.609
epoch done with 0.596
epoch done with 0.582
epoch done with 0.569
epoch done with 0.556
epoch done with 0.544
epoch done with 0.532
epoch done with 0.521
epoch done with 0.510
epoch done with 0.499
epoch done with 0.490
epoch done with 0.480
epoch done with 0.472
epoch done with 0.464
epoch done with 0.456
epoch done with 0.450
epoch done with 0.443
epoch done with 0.437
epoch done with 0.432
epoch done with 0.426
epoch done with 0.422
epoch done with 0.417
epoch done with 0.413
epoch done with 0.409
epoch done with 0.405
epoch done with 0.401
epoch done with 0.398
epoch done with 0.395
epoch done with 0.392
epoch done with 0.389
epoch done with 0.387
epoch done with 0.384
epoch done with 0.382
epoch done with 0.380
epoch done with 0.377
epoch done with 0.375
epoch done with 0.373
epoch done with 0.371
epoch done with 0.370
epoch done with 0.368
epoch done with 0.366
epoch done with 0.365
Grid search using {'num_samples': 128, 'learning_rate': 0.0001, 'l1_regularization_factor': 0.01, 'l2_regularization_factor': 0.01, 'number_additive_traits': 1, 'model_type': 'two_state_non_equilibrium', 'specs': (False, False)}
epoch done with 0.624
epoch done with 0.615
epoch done with 0.606
epoch done with 0.597
epoch done with 0.588
epoch done with 0.579
epoch done with 0.571
epoch done with 0.563
epoch done with 0.555
epoch done with 0.547
epoch done with 0.540
epoch done with 0.533
epoch done with 0.527
epoch done with 0.520
epoch done with 0.514
epoch done with 0.508
epoch done with 0.502
epoch done with 0.496
epoch done with 0.491
epoch done with 0.485
epoch done with 0.480
epoch done with 0.476
epoch done with 0.471
epoch done with 0.466
epoch done with 0.462
epoch done with 0.457
epoch done with 0.453
epoch done with 0.449
epoch done with 0.445
epoch done with 0.441
epoch done with 0.438
epoch done with 0.434
epoch done with 0.431
epoch done with 0.428
epoch done with 0.425
epoch done with 0.422
epoch done with 0.419
epoch done with 0.416
epoch done with 0.413
epoch done with 0.411
epoch done with 0.409
epoch done with 0.406
epoch done with 0.404
epoch done with 0.402
epoch done with 0.400
epoch done with 0.398
epoch done with 0.396
epoch done with 0.394
epoch done with 0.392
epoch done with 0.391
epoch done with 0.389
epoch done with 0.388
epoch done with 0.386
epoch done with 0.385
epoch done with 0.384
epoch done with 0.382
epoch done with 0.381
epoch done with 0.380
epoch done with 0.379
epoch done with 0.378
epoch done with 0.377
epoch done with 0.376
epoch done with 0.375
epoch done with 0.374
epoch done with 0.373
epoch done with 0.373
epoch done with 0.372
epoch done with 0.371
epoch done with 0.371
epoch done with 0.370
epoch done with 0.369
epoch done with 0.369
epoch done with 0.368
epoch done with 0.368
epoch done with 0.367
Grid search using {'num_samples': 128, 'learning_rate': 0.0001, 'l1_regularization_factor': 0.01, 'l2_regularization_factor': 0.1, 'number_additive_traits': 1, 'model_type': 'two_state_non_equilibrium', 'specs': (False, False)}
epoch done with 1.261
epoch done with 1.215
epoch done with 1.171
epoch done with 1.129
epoch done with 1.089
epoch done with 1.052
epoch done with 1.016
epoch done with 0.981
epoch done with 0.949
epoch done with 0.917
epoch done with 0.888
epoch done with 0.859
epoch done with 0.832
epoch done with 0.807
epoch done with 0.783
epoch done with 0.760
epoch done with 0.738
epoch done with 0.717
epoch done with 0.697
epoch done with 0.677
epoch done with 0.659
epoch done with 0.641
epoch done with 0.624
epoch done with 0.608
epoch done with 0.592
epoch done with 0.578
epoch done with 0.564
epoch done with 0.550
epoch done with 0.538
epoch done with 0.526
epoch done with 0.515
epoch done with 0.505
epoch done with 0.496
epoch done with 0.487
epoch done with 0.479
epoch done with 0.472
epoch done with 0.464
epoch done with 0.458
epoch done with 0.452
epoch done with 0.447
epoch done with 0.441
epoch done with 0.436
epoch done with 0.431
epoch done with 0.427
epoch done with 0.423
epoch done with 0.419
epoch done with 0.415
epoch done with 0.411
epoch done with 0.408
epoch done with 0.405
epoch done with 0.402
epoch done with 0.400
epoch done with 0.397
epoch done with 0.395
epoch done with 0.392
epoch done with 0.390
epoch done with 0.388
epoch done with 0.386
epoch done with 0.384
epoch done with 0.382
epoch done with 0.381
epoch done with 0.379
epoch done with 0.377
epoch done with 0.376
epoch done with 0.374
epoch done with 0.373
epoch done with 0.371
epoch done with 0.370
epoch done with 0.369
epoch done with 0.367
epoch done with 0.366
epoch done with 0.365
epoch done with 0.363
epoch done with 0.362
epoch done with 0.361
Grid search using {'num_samples': 128, 'learning_rate': 0.0001, 'l1_regularization_factor': 0.1, 'l2_regularization_factor': 0.0001, 'number_additive_traits': 1, 'model_type': 'two_state_non_equilibrium', 'specs': (False, False)}
epoch done with 1.176
epoch done with 1.142
epoch done with 1.110
epoch done with 1.080
epoch done with 1.051
epoch done with 1.027
epoch done with 1.003
epoch done with 0.980
epoch done with 0.958
epoch done with 0.936
epoch done with 0.915
epoch done with 0.894
epoch done with 0.874
epoch done with 0.855
epoch done with 0.835
epoch done with 0.817
epoch done with 0.798
epoch done with 0.781
epoch done with 0.763
epoch done with 0.746
epoch done with 0.729
epoch done with 0.713
epoch done with 0.697
epoch done with 0.681
epoch done with 0.666
epoch done with 0.651
epoch done with 0.636
epoch done with 0.622
epoch done with 0.608
epoch done with 0.594
epoch done with 0.580
epoch done with 0.567
epoch done with 0.555
epoch done with 0.543
epoch done with 0.531
epoch done with 0.520
epoch done with 0.509
epoch done with 0.499
epoch done with 0.490
epoch done with 0.481
epoch done with 0.473
epoch done with 0.466
epoch done with 0.459
epoch done with 0.453
epoch done with 0.447
epoch done with 0.441
epoch done with 0.436
epoch done with 0.431
epoch done with 0.426
epoch done with 0.422
epoch done with 0.418
epoch done with 0.415
epoch done with 0.411
epoch done with 0.408
epoch done with 0.404
epoch done with 0.401
epoch done with 0.398
epoch done with 0.396
epoch done with 0.393
epoch done with 0.390
epoch done with 0.388
epoch done with 0.386
epoch done with 0.383
epoch done with 0.381
epoch done with 0.379
epoch done with 0.377
epoch done with 0.376
epoch done with 0.374
epoch done with 0.372
epoch done with 0.371
/camp/home/demetzp/.conda/envs/demetzp/lib/python3.8/site-packages/haiku/_src/data_structures.py:143: FutureWarning: jax.tree_flatten is deprecated, and will be removed in a future release. Use jax.tree_util.tree_flatten instead.
  leaves, treedef = jax.tree_flatten(tree)
/camp/home/demetzp/.conda/envs/demetzp/lib/python3.8/site-packages/haiku/_src/data_structures.py:144: FutureWarning: jax.tree_unflatten is deprecated, and will be removed in a future release. Use jax.tree_util.tree_unflatten instead.
  return jax.tree_unflatten(treedef, leaves)
wandb: Waiting for W&B process to finish... (success).
wandb: - 0.005 MB of 0.017 MB uploaded (0.000 MB deduped)wandb: \ 0.005 MB of 0.017 MB uploaded (0.000 MB deduped)wandb: 
wandb: Run history:
wandb: val_loss_fit ██▇▇▆▆▆▅▅▅▄▄▄▄▃▃▃▃▂▂▂▂▂▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb: 
wandb: Run summary:
wandb: val_loss_fit 0.363
wandb: 
wandb: 🚀 View run Run 13 at: https://wandb.ai/lab_frohlich/pierre_mochi__fit_tmodel_3state_doubledeepms/runs/fqti09dz
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20230610_181657-fqti09dz/logs
wandb: - Waiting for wandb.init()...wandb: \ Waiting for wandb.init()...wandb: wandb version 0.15.4 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.15.3
wandb: Run data is saved locally in /nemo/lab/froehlichf/home/users/demetzp/PierreDeMetz/inst/python/wandb/run-20230610_181930-cx8zw6ut
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run Run 14
wandb: ⭐️ View project at https://wandb.ai/lab_frohlich/pierre_mochi__fit_tmodel_3state_doubledeepms
wandb: 🚀 View run at https://wandb.ai/lab_frohlich/pierre_mochi__fit_tmodel_3state_doubledeepms/runs/cx8zw6ut
/camp/home/demetzp/.conda/envs/demetzp/lib/python3.8/site-packages/haiku/_src/data_structures.py:143: FutureWarning: jax.tree_flatten is deprecated, and will be removed in a future release. Use jax.tree_util.tree_flatten instead.
  leaves, treedef = jax.tree_flatten(tree)
/camp/home/demetzp/.conda/envs/demetzp/lib/python3.8/site-packages/haiku/_src/data_structures.py:144: FutureWarning: jax.tree_unflatten is deprecated, and will be removed in a future release. Use jax.tree_util.tree_unflatten instead.
  return jax.tree_unflatten(treedef, leaves)
wandb: Waiting for W&B process to finish... (success).
wandb: 
wandb: Run history:
wandb: val_loss_fit ██▇▆▆▆▅▅▄▄▄▄▃▃▃▃▂▂▂▂▂▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb: 
wandb: Run summary:
wandb: val_loss_fit 0.36
wandb: 
wandb: 🚀 View run Run 14 at: https://wandb.ai/lab_frohlich/pierre_mochi__fit_tmodel_3state_doubledeepms/runs/cx8zw6ut
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20230610_181930-cx8zw6ut/logs
wandb: - Waiting for wandb.init()...wandb: \ Waiting for wandb.init()...wandb: wandb version 0.15.4 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.15.3
wandb: Run data is saved locally in /nemo/lab/froehlichf/home/users/demetzp/PierreDeMetz/inst/python/wandb/run-20230610_182201-68vj9cjy
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run Run 15
wandb: ⭐️ View project at https://wandb.ai/lab_frohlich/pierre_mochi__fit_tmodel_3state_doubledeepms
wandb: 🚀 View run at https://wandb.ai/lab_frohlich/pierre_mochi__fit_tmodel_3state_doubledeepms/runs/68vj9cjy
/camp/home/demetzp/.conda/envs/demetzp/lib/python3.8/site-packages/haiku/_src/data_structures.py:143: FutureWarning: jax.tree_flatten is deprecated, and will be removed in a future release. Use jax.tree_util.tree_flatten instead.
  leaves, treedef = jax.tree_flatten(tree)
/camp/home/demetzp/.conda/envs/demetzp/lib/python3.8/site-packages/haiku/_src/data_structures.py:144: FutureWarning: jax.tree_unflatten is deprecated, and will be removed in a future release. Use jax.tree_util.tree_unflatten instead.
  return jax.tree_unflatten(treedef, leaves)
wandb: Waiting for W&B process to finish... (success).
wandb: 
wandb: Run history:
wandb: val_loss_fit ██▇▇▆▆▅▅▄▄▄▄▃▃▃▃▂▂▂▂▂▂▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb: 
wandb: Run summary:
wandb: val_loss_fit 0.362
wandb: 
wandb: 🚀 View run Run 15 at: https://wandb.ai/lab_frohlich/pierre_mochi__fit_tmodel_3state_doubledeepms/runs/68vj9cjy
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20230610_182201-68vj9cjy/logs
wandb: - Waiting for wandb.init()...wandb: \ Waiting for wandb.init()...wandb: wandb version 0.15.4 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.15.3
wandb: Run data is saved locally in /nemo/lab/froehlichf/home/users/demetzp/PierreDeMetz/inst/python/wandb/run-20230610_182434-tqaa5uon
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run Run 16
wandb: ⭐️ View project at https://wandb.ai/lab_frohlich/pierre_mochi__fit_tmodel_3state_doubledeepms
wandb: 🚀 View run at https://wandb.ai/lab_frohlich/pierre_mochi__fit_tmodel_3state_doubledeepms/runs/tqaa5uon
/camp/home/demetzp/.conda/envs/demetzp/lib/python3.8/site-packages/haiku/_src/data_structures.py:143: FutureWarning: jax.tree_flatten is deprecated, and will be removed in a future release. Use jax.tree_util.tree_flatten instead.
  leaves, treedef = jax.tree_flatten(tree)
/camp/home/demetzp/.conda/envs/demetzp/lib/python3.8/site-packages/haiku/_src/data_structures.py:144: FutureWarning: jax.tree_unflatten is deprecated, and will be removed in a future release. Use jax.tree_util.tree_unflatten instead.
  return jax.tree_unflatten(treedef, leaves)
wandb: Waiting for W&B process to finish... (success).
wandb: 
wandb: Run history:
wandb: val_loss_fit ██▇▆▆▅▅▅▄▄▄▃▃▃▃▂▂▂▂▂▂▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb: 
wandb: Run summary:
wandb: val_loss_fit 0.362
wandb: 
wandb: 🚀 View run Run 16 at: https://wandb.ai/lab_frohlich/pierre_mochi__fit_tmodel_3state_doubledeepms/runs/tqaa5uon
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20230610_182434-tqaa5uon/logs
wandb: - Waiting for wandb.init()...wandb: \ Waiting for wandb.init()...wandb: wandb version 0.15.4 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.15.3
wandb: Run data is saved locally in /nemo/lab/froehlichf/home/users/demetzp/PierreDeMetz/inst/python/wandb/run-20230610_182707-tzfgeodm
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run Run 17
wandb: ⭐️ View project at https://wandb.ai/lab_frohlich/pierre_mochi__fit_tmodel_3state_doubledeepms
wandb: 🚀 View run at https://wandb.ai/lab_frohlich/pierre_mochi__fit_tmodel_3state_doubledeepms/runs/tzfgeodm
/camp/home/demetzp/.conda/envs/demetzp/lib/python3.8/site-packages/haiku/_src/data_structures.py:143: FutureWarning: jax.tree_flatten is deprecated, and will be removed in a future release. Use jax.tree_util.tree_flatten instead.
  leaves, treedef = jax.tree_flatten(tree)
/camp/home/demetzp/.conda/envs/demetzp/lib/python3.8/site-packages/haiku/_src/data_structures.py:144: FutureWarning: jax.tree_unflatten is deprecated, and will be removed in a future release. Use jax.tree_util.tree_unflatten instead.
  return jax.tree_unflatten(treedef, leaves)
wandb: Waiting for W&B process to finish... (success).
wandb: - 0.005 MB of 0.005 MB uploaded (0.000 MB deduped)wandb: \ 0.005 MB of 0.005 MB uploaded (0.000 MB deduped)wandb: | 0.005 MB of 0.005 MB uploaded (0.000 MB deduped)wandb: / 0.005 MB of 0.005 MB uploaded (0.000 MB deduped)wandb: 
wandb: Run history:
wandb: val_loss_fit █▆▄▃▃▃▃▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb: 
wandb: Run summary:
wandb: val_loss_fit 0.131
wandb: 
wandb: 🚀 View run Run 17 at: https://wandb.ai/lab_frohlich/pierre_mochi__fit_tmodel_3state_doubledeepms/runs/tzfgeodm
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20230610_182707-tzfgeodm/logs
wandb: - Waiting for wandb.init()...wandb: \ Waiting for wandb.init()...wandb: wandb version 0.15.4 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.15.3
wandb: Run data is saved locally in /nemo/lab/froehlichf/home/users/demetzp/PierreDeMetz/inst/python/wandb/run-20230610_182938-uhtkt4lg
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run Run 18
wandb: ⭐️ View project at https://wandb.ai/lab_frohlich/pierre_mochi__fit_tmodel_3state_doubledeepms
wandb: 🚀 View run at https://wandb.ai/lab_frohlich/pierre_mochi__fit_tmodel_3state_doubledeepms/runs/uhtkt4lg
epoch done with 0.369
epoch done with 0.368
epoch done with 0.366
epoch done with 0.365
epoch done with 0.363
Grid search using {'num_samples': 128, 'learning_rate': 0.0001, 'l1_regularization_factor': 0.1, 'l2_regularization_factor': 0.001, 'number_additive_traits': 1, 'model_type': 'two_state_non_equilibrium', 'specs': (False, False)}
epoch done with 1.157
epoch done with 1.118
epoch done with 1.081
epoch done with 1.047
epoch done with 1.014
epoch done with 0.983
epoch done with 0.954
epoch done with 0.925
epoch done with 0.898
epoch done with 0.873
epoch done with 0.848
epoch done with 0.824
epoch done with 0.802
epoch done with 0.780
epoch done with 0.759
epoch done with 0.739
epoch done with 0.719
epoch done with 0.700
epoch done with 0.682
epoch done with 0.665
epoch done with 0.648
epoch done with 0.631
epoch done with 0.616
epoch done with 0.600
epoch done with 0.586
epoch done with 0.572
epoch done with 0.558
epoch done with 0.546
epoch done with 0.534
epoch done with 0.522
epoch done with 0.512
epoch done with 0.502
epoch done with 0.493
epoch done with 0.484
epoch done with 0.476
epoch done with 0.469
epoch done with 0.462
epoch done with 0.456
epoch done with 0.450
epoch done with 0.444
epoch done with 0.439
epoch done with 0.434
epoch done with 0.430
epoch done with 0.425
epoch done with 0.421
epoch done with 0.417
epoch done with 0.414
epoch done with 0.410
epoch done with 0.407
epoch done with 0.404
epoch done with 0.401
epoch done with 0.398
epoch done with 0.396
epoch done with 0.393
epoch done with 0.391
epoch done with 0.389
epoch done with 0.387
epoch done with 0.385
epoch done with 0.383
epoch done with 0.381
epoch done with 0.379
epoch done with 0.378
epoch done with 0.376
epoch done with 0.375
epoch done with 0.373
epoch done with 0.372
epoch done with 0.370
epoch done with 0.369
epoch done with 0.368
epoch done with 0.366
epoch done with 0.365
epoch done with 0.364
epoch done with 0.362
epoch done with 0.361
epoch done with 0.360
Grid search using {'num_samples': 128, 'learning_rate': 0.0001, 'l1_regularization_factor': 0.1, 'l2_regularization_factor': 0.01, 'number_additive_traits': 1, 'model_type': 'two_state_non_equilibrium', 'specs': (False, False)}
epoch done with 0.841
epoch done with 0.821
epoch done with 0.802
epoch done with 0.783
epoch done with 0.764
epoch done with 0.746
epoch done with 0.729
epoch done with 0.712
epoch done with 0.695
epoch done with 0.679
epoch done with 0.664
epoch done with 0.648
epoch done with 0.634
epoch done with 0.619
epoch done with 0.606
epoch done with 0.592
epoch done with 0.580
epoch done with 0.567
epoch done with 0.556
epoch done with 0.545
epoch done with 0.534
epoch done with 0.525
epoch done with 0.516
epoch done with 0.508
epoch done with 0.500
epoch done with 0.493
epoch done with 0.485
epoch done with 0.479
epoch done with 0.473
epoch done with 0.467
epoch done with 0.461
epoch done with 0.456
epoch done with 0.451
epoch done with 0.446
epoch done with 0.442
epoch done with 0.438
epoch done with 0.434
epoch done with 0.430
epoch done with 0.426
epoch done with 0.422
epoch done with 0.419
epoch done with 0.416
epoch done with 0.413
epoch done with 0.410
epoch done with 0.408
epoch done with 0.405
epoch done with 0.402
epoch done with 0.400
epoch done with 0.398
epoch done with 0.396
epoch done with 0.393
epoch done with 0.391
epoch done with 0.390
epoch done with 0.388
epoch done with 0.386
epoch done with 0.384
epoch done with 0.383
epoch done with 0.381
epoch done with 0.380
epoch done with 0.378
epoch done with 0.377
epoch done with 0.376
epoch done with 0.375
epoch done with 0.374
epoch done with 0.373
epoch done with 0.372
epoch done with 0.371
epoch done with 0.370
epoch done with 0.369
epoch done with 0.368
epoch done with 0.367
epoch done with 0.366
epoch done with 0.365
epoch done with 0.363
epoch done with 0.362
Grid search using {'num_samples': 128, 'learning_rate': 0.0001, 'l1_regularization_factor': 0.1, 'l2_regularization_factor': 0.1, 'number_additive_traits': 1, 'model_type': 'two_state_non_equilibrium', 'specs': (False, False)}
epoch done with 0.948
epoch done with 0.918
epoch done with 0.890
epoch done with 0.864
epoch done with 0.838
epoch done with 0.814
epoch done with 0.792
epoch done with 0.770
epoch done with 0.749
epoch done with 0.729
epoch done with 0.710
epoch done with 0.691
epoch done with 0.674
epoch done with 0.657
epoch done with 0.641
epoch done with 0.625
epoch done with 0.610
epoch done with 0.596
epoch done with 0.582
epoch done with 0.569
epoch done with 0.557
epoch done with 0.546
epoch done with 0.535
epoch done with 0.524
epoch done with 0.515
epoch done with 0.506
epoch done with 0.498
epoch done with 0.491
epoch done with 0.484
epoch done with 0.477
epoch done with 0.471
epoch done with 0.465
epoch done with 0.459
epoch done with 0.453
epoch done with 0.448
epoch done with 0.444
epoch done with 0.439
epoch done with 0.435
epoch done with 0.430
epoch done with 0.427
epoch done with 0.423
epoch done with 0.419
epoch done with 0.416
epoch done with 0.413
epoch done with 0.410
epoch done with 0.407
epoch done with 0.404
epoch done with 0.402
epoch done with 0.399
epoch done with 0.397
epoch done with 0.395
epoch done with 0.393
epoch done with 0.391
epoch done with 0.389
epoch done with 0.387
epoch done with 0.385
epoch done with 0.384
epoch done with 0.382
epoch done with 0.381
epoch done with 0.380
epoch done with 0.378
epoch done with 0.377
epoch done with 0.376
epoch done with 0.374
epoch done with 0.373
epoch done with 0.372
epoch done with 0.371
epoch done with 0.370
epoch done with 0.369
epoch done with 0.368
epoch done with 0.367
epoch done with 0.366
epoch done with 0.364
epoch done with 0.363
epoch done with 0.362
Grid search using {'num_samples': 128, 'learning_rate': 0.001, 'l1_regularization_factor': 0.0001, 'l2_regularization_factor': 0.0001, 'number_additive_traits': 1, 'model_type': 'two_state_non_equilibrium', 'specs': (False, False)}
epoch done with 0.946
epoch done with 0.757
epoch done with 0.612
epoch done with 0.502
epoch done with 0.439
epoch done with 0.408
epoch done with 0.389
epoch done with 0.376
epoch done with 0.366
epoch done with 0.358
epoch done with 0.343
epoch done with 0.323
epoch done with 0.300
epoch done with 0.274
epoch done with 0.247
epoch done with 0.225
epoch done with 0.206
epoch done with 0.192
epoch done with 0.179
epoch done with 0.169
epoch done with 0.160
epoch done with 0.153
epoch done with 0.147
epoch done with 0.144
epoch done with 0.141
epoch done with 0.140
epoch done with 0.138
epoch done with 0.137
epoch done with 0.136
epoch done with 0.134
epoch done with 0.134
epoch done with 0.133
epoch done with 0.133
epoch done with 0.132
epoch done with 0.132
epoch done with 0.131
epoch done with 0.131
epoch done with 0.131
epoch done with 0.131
epoch done with 0.131
epoch done with 0.131
epoch done with 0.130
epoch done with 0.131
epoch done with 0.130
epoch done with 0.130
epoch done with 0.130
epoch done with 0.131
epoch done with 0.130
epoch done with 0.130
epoch done with 0.130
epoch done with 0.130
epoch done with 0.130
epoch done with 0.130
epoch done with 0.130
epoch done with 0.130
epoch done with 0.130
epoch done with 0.130
epoch done with 0.130
epoch done with 0.130
epoch done with 0.130
epoch done with 0.131
epoch done with 0.130
epoch done with 0.130
epoch done with 0.131
epoch done with 0.131
epoch done with 0.131
epoch done with 0.131
epoch done with 0.131
epoch done with 0.131
epoch done with 0.131
epoch done with 0.131
epoch done with 0.131
epoch done with 0.131
epoch done with 0.131
epoch done with 0.131
Grid search using {'num_samples': 128, 'learning_rate': 0.001, 'l1_regularization_factor': 0.0001, 'l2_regularization_factor': 0.001, 'number_additive_traits': 1, 'model_type': 'two_state_non_equilibrium', 'specs': (False, False)}
epoch done with 0.640
epoch done with 0.523
epoch done with 0.462
epoch done with 0.427
epoch done with 0.405
epoch done with 0.391
epoch done with 0.381
epoch done with 0.375
epoch done with 0.371
epoch done with 0.367
epoch done with 0.354
epoch done with 0.333
epoch done with 0.308
epoch done with 0.279
epoch done with 0.250
/camp/home/demetzp/.conda/envs/demetzp/lib/python3.8/site-packages/haiku/_src/data_structures.py:143: FutureWarning: jax.tree_flatten is deprecated, and will be removed in a future release. Use jax.tree_util.tree_flatten instead.
  leaves, treedef = jax.tree_flatten(tree)
/camp/home/demetzp/.conda/envs/demetzp/lib/python3.8/site-packages/haiku/_src/data_structures.py:144: FutureWarning: jax.tree_unflatten is deprecated, and will be removed in a future release. Use jax.tree_util.tree_unflatten instead.
  return jax.tree_unflatten(treedef, leaves)
wandb: Waiting for W&B process to finish... (success).
wandb: 
wandb: Run history:
wandb: val_loss_fit █▆▅▅▄▄▄▃▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb: 
wandb: Run summary:
wandb: val_loss_fit 0.13
wandb: 
wandb: 🚀 View run Run 18 at: https://wandb.ai/lab_frohlich/pierre_mochi__fit_tmodel_3state_doubledeepms/runs/uhtkt4lg
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20230610_182938-uhtkt4lg/logs
wandb: - Waiting for wandb.init()...wandb: \ Waiting for wandb.init()...wandb: wandb version 0.15.4 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.15.3
wandb: Run data is saved locally in /nemo/lab/froehlichf/home/users/demetzp/PierreDeMetz/inst/python/wandb/run-20230610_183208-l10auigz
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run Run 19
wandb: ⭐️ View project at https://wandb.ai/lab_frohlich/pierre_mochi__fit_tmodel_3state_doubledeepms
wandb: 🚀 View run at https://wandb.ai/lab_frohlich/pierre_mochi__fit_tmodel_3state_doubledeepms/runs/l10auigz
/camp/home/demetzp/.conda/envs/demetzp/lib/python3.8/site-packages/haiku/_src/data_structures.py:143: FutureWarning: jax.tree_flatten is deprecated, and will be removed in a future release. Use jax.tree_util.tree_flatten instead.
  leaves, treedef = jax.tree_flatten(tree)
/camp/home/demetzp/.conda/envs/demetzp/lib/python3.8/site-packages/haiku/_src/data_structures.py:144: FutureWarning: jax.tree_unflatten is deprecated, and will be removed in a future release. Use jax.tree_util.tree_unflatten instead.
  return jax.tree_unflatten(treedef, leaves)
wandb: Waiting for W&B process to finish... (success).
wandb: - 0.005 MB of 0.005 MB uploaded (0.000 MB deduped)wandb: \ 0.005 MB of 0.017 MB uploaded (0.000 MB deduped)wandb: | 0.005 MB of 0.017 MB uploaded (0.000 MB deduped)wandb: 
wandb: Run history:
wandb: val_loss_fit █▇▆▅▅▅▅▅▃▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb: 
wandb: Run summary:
wandb: val_loss_fit 0.131
wandb: 
wandb: 🚀 View run Run 19 at: https://wandb.ai/lab_frohlich/pierre_mochi__fit_tmodel_3state_doubledeepms/runs/l10auigz
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20230610_183208-l10auigz/logs
wandb: - Waiting for wandb.init()...wandb: \ Waiting for wandb.init()...wandb: wandb version 0.15.4 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.15.3
wandb: Run data is saved locally in /nemo/lab/froehlichf/home/users/demetzp/PierreDeMetz/inst/python/wandb/run-20230610_183442-xqj8r2cr
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run Run 20
wandb: ⭐️ View project at https://wandb.ai/lab_frohlich/pierre_mochi__fit_tmodel_3state_doubledeepms
wandb: 🚀 View run at https://wandb.ai/lab_frohlich/pierre_mochi__fit_tmodel_3state_doubledeepms/runs/xqj8r2cr
/camp/home/demetzp/.conda/envs/demetzp/lib/python3.8/site-packages/haiku/_src/data_structures.py:143: FutureWarning: jax.tree_flatten is deprecated, and will be removed in a future release. Use jax.tree_util.tree_flatten instead.
  leaves, treedef = jax.tree_flatten(tree)
/camp/home/demetzp/.conda/envs/demetzp/lib/python3.8/site-packages/haiku/_src/data_structures.py:144: FutureWarning: jax.tree_unflatten is deprecated, and will be removed in a future release. Use jax.tree_util.tree_unflatten instead.
  return jax.tree_unflatten(treedef, leaves)
wandb: Waiting for W&B process to finish... (success).
wandb: - 0.005 MB of 0.005 MB uploaded (0.000 MB deduped)wandb: \ 0.005 MB of 0.005 MB uploaded (0.000 MB deduped)wandb: | 0.005 MB of 0.005 MB uploaded (0.000 MB deduped)wandb: / 0.005 MB of 0.005 MB uploaded (0.000 MB deduped)wandb: 
wandb: Run history:
wandb: val_loss_fit █▇▆▅▅▅▄▃▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb: 
wandb: Run summary:
wandb: val_loss_fit 0.131
wandb: 
wandb: 🚀 View run Run 20 at: https://wandb.ai/lab_frohlich/pierre_mochi__fit_tmodel_3state_doubledeepms/runs/xqj8r2cr
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20230610_183442-xqj8r2cr/logs
wandb: - Waiting for wandb.init()...wandb: \ Waiting for wandb.init()...wandb: wandb version 0.15.4 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.15.3
wandb: Run data is saved locally in /nemo/lab/froehlichf/home/users/demetzp/PierreDeMetz/inst/python/wandb/run-20230610_183714-fwkhwww4
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run Run 21
wandb: ⭐️ View project at https://wandb.ai/lab_frohlich/pierre_mochi__fit_tmodel_3state_doubledeepms
wandb: 🚀 View run at https://wandb.ai/lab_frohlich/pierre_mochi__fit_tmodel_3state_doubledeepms/runs/fwkhwww4
/camp/home/demetzp/.conda/envs/demetzp/lib/python3.8/site-packages/haiku/_src/data_structures.py:143: FutureWarning: jax.tree_flatten is deprecated, and will be removed in a future release. Use jax.tree_util.tree_flatten instead.
  leaves, treedef = jax.tree_flatten(tree)
/camp/home/demetzp/.conda/envs/demetzp/lib/python3.8/site-packages/haiku/_src/data_structures.py:144: FutureWarning: jax.tree_unflatten is deprecated, and will be removed in a future release. Use jax.tree_util.tree_unflatten instead.
  return jax.tree_unflatten(treedef, leaves)
wandb: Waiting for W&B process to finish... (success).
wandb: - 0.005 MB of 0.005 MB uploaded (0.000 MB deduped)wandb: \ 0.005 MB of 0.005 MB uploaded (0.000 MB deduped)wandb: | 0.005 MB of 0.005 MB uploaded (0.000 MB deduped)wandb: / 0.005 MB of 0.005 MB uploaded (0.000 MB deduped)wandb: 
wandb: Run history:
wandb: val_loss_fit █▆▄▃▃▃▃▃▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb: 
wandb: Run summary:
wandb: val_loss_fit 0.132
wandb: 
wandb: 🚀 View run Run 21 at: https://wandb.ai/lab_frohlich/pierre_mochi__fit_tmodel_3state_doubledeepms/runs/fwkhwww4
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20230610_183714-fwkhwww4/logs
wandb: - Waiting for wandb.init()...wandb: \ Waiting for wandb.init()...wandb: wandb version 0.15.4 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.15.3
wandb: Run data is saved locally in /nemo/lab/froehlichf/home/users/demetzp/PierreDeMetz/inst/python/wandb/run-20230610_183946-7i4bm3k6
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run Run 22
wandb: ⭐️ View project at https://wandb.ai/lab_frohlich/pierre_mochi__fit_tmodel_3state_doubledeepms
wandb: 🚀 View run at https://wandb.ai/lab_frohlich/pierre_mochi__fit_tmodel_3state_doubledeepms/runs/7i4bm3k6
epoch done with 0.227
epoch done with 0.209
epoch done with 0.193
epoch done with 0.177
epoch done with 0.162
epoch done with 0.152
epoch done with 0.145
epoch done with 0.142
epoch done with 0.140
epoch done with 0.138
epoch done with 0.137
epoch done with 0.136
epoch done with 0.135
epoch done with 0.135
epoch done with 0.134
epoch done with 0.133
epoch done with 0.132
epoch done with 0.132
epoch done with 0.131
epoch done with 0.131
epoch done with 0.131
epoch done with 0.131
epoch done with 0.131
epoch done with 0.131
epoch done with 0.131
epoch done with 0.130
epoch done with 0.130
epoch done with 0.130
epoch done with 0.130
epoch done with 0.130
epoch done with 0.130
epoch done with 0.130
epoch done with 0.130
epoch done with 0.130
epoch done with 0.130
epoch done with 0.130
epoch done with 0.130
epoch done with 0.130
epoch done with 0.130
epoch done with 0.130
epoch done with 0.130
epoch done with 0.130
epoch done with 0.130
epoch done with 0.130
epoch done with 0.130
epoch done with 0.130
epoch done with 0.130
epoch done with 0.130
epoch done with 0.130
epoch done with 0.130
epoch done with 0.130
epoch done with 0.130
epoch done with 0.130
epoch done with 0.130
epoch done with 0.130
epoch done with 0.130
epoch done with 0.130
epoch done with 0.130
epoch done with 0.130
epoch done with 0.130
Grid search using {'num_samples': 128, 'learning_rate': 0.001, 'l1_regularization_factor': 0.0001, 'l2_regularization_factor': 0.01, 'number_additive_traits': 1, 'model_type': 'two_state_non_equilibrium', 'specs': (False, False)}
epoch done with 0.553
epoch done with 0.489
epoch done with 0.448
epoch done with 0.421
epoch done with 0.403
epoch done with 0.390
epoch done with 0.381
epoch done with 0.375
epoch done with 0.371
epoch done with 0.368
epoch done with 0.366
epoch done with 0.364
epoch done with 0.363
epoch done with 0.351
epoch done with 0.320
epoch done with 0.280
epoch done with 0.239
epoch done with 0.206
epoch done with 0.182
epoch done with 0.166
epoch done with 0.155
epoch done with 0.149
epoch done with 0.145
epoch done with 0.142
epoch done with 0.140
epoch done with 0.139
epoch done with 0.137
epoch done with 0.136
epoch done with 0.135
epoch done with 0.135
epoch done with 0.134
epoch done with 0.134
epoch done with 0.133
epoch done with 0.133
epoch done with 0.133
epoch done with 0.132
epoch done with 0.132
epoch done with 0.132
epoch done with 0.132
epoch done with 0.131
epoch done with 0.132
epoch done with 0.131
epoch done with 0.132
epoch done with 0.131
epoch done with 0.131
epoch done with 0.131
epoch done with 0.131
epoch done with 0.131
epoch done with 0.131
epoch done with 0.130
epoch done with 0.131
epoch done with 0.131
epoch done with 0.131
epoch done with 0.131
epoch done with 0.131
epoch done with 0.131
epoch done with 0.131
epoch done with 0.131
epoch done with 0.131
epoch done with 0.131
epoch done with 0.131
epoch done with 0.131
epoch done with 0.131
epoch done with 0.131
epoch done with 0.131
epoch done with 0.131
epoch done with 0.131
epoch done with 0.131
epoch done with 0.131
epoch done with 0.131
epoch done with 0.131
epoch done with 0.131
epoch done with 0.131
epoch done with 0.131
epoch done with 0.131
Grid search using {'num_samples': 128, 'learning_rate': 0.001, 'l1_regularization_factor': 0.0001, 'l2_regularization_factor': 0.1, 'number_additive_traits': 1, 'model_type': 'two_state_non_equilibrium', 'specs': (False, False)}
epoch done with 0.546
epoch done with 0.486
epoch done with 0.446
epoch done with 0.418
epoch done with 0.401
epoch done with 0.388
epoch done with 0.380
epoch done with 0.374
epoch done with 0.370
epoch done with 0.367
epoch done with 0.365
epoch done with 0.335
epoch done with 0.289
epoch done with 0.242
epoch done with 0.202
epoch done with 0.178
epoch done with 0.164
epoch done with 0.155
epoch done with 0.151
epoch done with 0.147
epoch done with 0.143
epoch done with 0.141
epoch done with 0.139
epoch done with 0.138
epoch done with 0.137
epoch done with 0.136
epoch done with 0.135
epoch done with 0.134
epoch done with 0.134
epoch done with 0.134
epoch done with 0.133
epoch done with 0.133
epoch done with 0.132
epoch done with 0.132
epoch done with 0.132
epoch done with 0.132
epoch done with 0.131
epoch done with 0.132
epoch done with 0.131
epoch done with 0.131
epoch done with 0.131
epoch done with 0.131
epoch done with 0.131
epoch done with 0.131
epoch done with 0.131
epoch done with 0.131
epoch done with 0.131
epoch done with 0.131
epoch done with 0.131
epoch done with 0.131
epoch done with 0.131
epoch done with 0.131
epoch done with 0.131
epoch done with 0.131
epoch done with 0.131
epoch done with 0.131
epoch done with 0.131
epoch done with 0.131
epoch done with 0.131
epoch done with 0.131
epoch done with 0.131
epoch done with 0.131
epoch done with 0.131
epoch done with 0.131
epoch done with 0.131
epoch done with 0.131
epoch done with 0.131
epoch done with 0.131
epoch done with 0.131
epoch done with 0.131
epoch done with 0.131
epoch done with 0.131
epoch done with 0.131
epoch done with 0.131
epoch done with 0.131
Grid search using {'num_samples': 128, 'learning_rate': 0.001, 'l1_regularization_factor': 0.001, 'l2_regularization_factor': 0.0001, 'number_additive_traits': 1, 'model_type': 'two_state_non_equilibrium', 'specs': (False, False)}
epoch done with 0.956
epoch done with 0.729
epoch done with 0.573
epoch done with 0.475
epoch done with 0.425
epoch done with 0.399
epoch done with 0.384
epoch done with 0.373
epoch done with 0.365
epoch done with 0.358
epoch done with 0.352
epoch done with 0.345
epoch done with 0.337
epoch done with 0.326
epoch done with 0.311
epoch done with 0.291
epoch done with 0.267
epoch done with 0.240
epoch done with 0.217
epoch done with 0.196
epoch done with 0.179
epoch done with 0.165
epoch done with 0.156
epoch done with 0.150
epoch done with 0.146
epoch done with 0.144
epoch done with 0.143
epoch done with 0.141
epoch done with 0.140
epoch done with 0.139
epoch done with 0.138
epoch done with 0.137
epoch done with 0.136
epoch done with 0.136
epoch done with 0.136
epoch done with 0.136
epoch done with 0.135
epoch done with 0.135
epoch done with 0.134
epoch done with 0.134
epoch done with 0.134
epoch done with 0.134
epoch done with 0.133
epoch done with 0.133
epoch done with 0.133
epoch done with 0.133
epoch done with 0.133
epoch done with 0.133
epoch done with 0.133
epoch done with 0.133
epoch done with 0.133
epoch done with 0.133
epoch done with 0.133
epoch done with 0.133
epoch done with 0.133
epoch done with 0.133
epoch done with 0.133
epoch done with 0.133
epoch done with 0.133
epoch done with 0.133
epoch done with 0.133
epoch done with 0.133
epoch done with 0.132
epoch done with 0.133
epoch done with 0.133
epoch done with 0.132
epoch done with 0.132
epoch done with 0.133
epoch done with 0.133
epoch done with 0.132
epoch done with 0.132
epoch done with 0.133
epoch done with 0.132
epoch done with 0.132
epoch done with 0.132
Grid search using {'num_samples': 128, 'learning_rate': 0.001, 'l1_regularization_factor': 0.001, 'l2_regularization_factor': 0.001, 'number_additive_traits': 1, 'model_type': 'two_state_non_equilibrium', 'specs': (False, False)}
epoch done with 0.547
epoch done with 0.487
epoch done with 0.446
epoch done with 0.419
epoch done with 0.401
epoch done with 0.388
epoch done with 0.380
epoch done with 0.374
epoch done with 0.370
epoch done with 0.349
epoch done with 0.308
epoch done with 0.263
epoch done with 0.220
epoch done with 0.189
epoch done with 0.169
epoch done with 0.159
epoch done with 0.153
epoch done with 0.149
epoch done with 0.145
epoch done with 0.142
epoch done with 0.140
epoch done with 0.139
epoch done with 0.138
epoch done with 0.137
epoch done with 0.136
epoch done with 0.135
epoch done with 0.135
epoch done with 0.134
epoch done with 0.133
epoch done with 0.133
epoch done with 0.132
epoch done with 0.132
epoch done with 0.132
epoch done with 0.132
epoch done with 0.132
epoch done with 0.132
epoch done with 0.131
epoch done with 0.131
epoch done with 0.131
epoch done with 0.131
epoch done with 0.131
epoch done with 0.131
epoch done with 0.131
epoch done with 0.131
epoch done with 0.131
/camp/home/demetzp/.conda/envs/demetzp/lib/python3.8/site-packages/haiku/_src/data_structures.py:143: FutureWarning: jax.tree_flatten is deprecated, and will be removed in a future release. Use jax.tree_util.tree_flatten instead.
  leaves, treedef = jax.tree_flatten(tree)
/camp/home/demetzp/.conda/envs/demetzp/lib/python3.8/site-packages/haiku/_src/data_structures.py:144: FutureWarning: jax.tree_unflatten is deprecated, and will be removed in a future release. Use jax.tree_util.tree_unflatten instead.
  return jax.tree_unflatten(treedef, leaves)
wandb: Waiting for W&B process to finish... (success).
wandb: 
wandb: Run history:
wandb: val_loss_fit █▇▆▅▅▅▃▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb: 
wandb: Run summary:
wandb: val_loss_fit 0.131
wandb: 
wandb: 🚀 View run Run 22 at: https://wandb.ai/lab_frohlich/pierre_mochi__fit_tmodel_3state_doubledeepms/runs/7i4bm3k6
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20230610_183946-7i4bm3k6/logs
wandb: - Waiting for wandb.init()...wandb: \ Waiting for wandb.init()...wandb: wandb version 0.15.4 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.15.3
wandb: Run data is saved locally in /nemo/lab/froehlichf/home/users/demetzp/PierreDeMetz/inst/python/wandb/run-20230610_184218-ncky4ju1
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run Run 23
wandb: ⭐️ View project at https://wandb.ai/lab_frohlich/pierre_mochi__fit_tmodel_3state_doubledeepms
wandb: 🚀 View run at https://wandb.ai/lab_frohlich/pierre_mochi__fit_tmodel_3state_doubledeepms/runs/ncky4ju1
/camp/home/demetzp/.conda/envs/demetzp/lib/python3.8/site-packages/haiku/_src/data_structures.py:143: FutureWarning: jax.tree_flatten is deprecated, and will be removed in a future release. Use jax.tree_util.tree_flatten instead.
  leaves, treedef = jax.tree_flatten(tree)
/camp/home/demetzp/.conda/envs/demetzp/lib/python3.8/site-packages/haiku/_src/data_structures.py:144: FutureWarning: jax.tree_unflatten is deprecated, and will be removed in a future release. Use jax.tree_util.tree_unflatten instead.
  return jax.tree_unflatten(treedef, leaves)
wandb: Waiting for W&B process to finish... (success).
wandb: 
wandb: Run history:
wandb: val_loss_fit █▇▆▅▅▅▅▄▃▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb: 
wandb: Run summary:
wandb: val_loss_fit 0.131
wandb: 
wandb: 🚀 View run Run 23 at: https://wandb.ai/lab_frohlich/pierre_mochi__fit_tmodel_3state_doubledeepms/runs/ncky4ju1
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20230610_184218-ncky4ju1/logs
wandb: - Waiting for wandb.init()...wandb: \ Waiting for wandb.init()...wandb: wandb version 0.15.4 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.15.3
wandb: Run data is saved locally in /nemo/lab/froehlichf/home/users/demetzp/PierreDeMetz/inst/python/wandb/run-20230610_184450-zmncklmv
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run Run 24
wandb: ⭐️ View project at https://wandb.ai/lab_frohlich/pierre_mochi__fit_tmodel_3state_doubledeepms
wandb: 🚀 View run at https://wandb.ai/lab_frohlich/pierre_mochi__fit_tmodel_3state_doubledeepms/runs/zmncklmv
srun: Job step aborted: Waiting up to 32 seconds for job step to finish.
slurmstepd: error: *** JOB 52971034 ON ca161 CANCELLED AT 2023-06-10T18:45:01 DUE TO TIME LIMIT ***
slurmstepd: error: *** STEP 52971034.0 ON ca161 CANCELLED AT 2023-06-10T18:45:01 DUE TO TIME LIMIT ***
Will exit after finishing currently running jobs (scheduler).
Will exit after finishing currently running jobs (scheduler).
