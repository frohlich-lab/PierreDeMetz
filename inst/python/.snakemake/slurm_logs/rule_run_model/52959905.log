Building DAG of jobs...
Using shell: /usr/bin/bash
Provided cores: 2
Rules claiming more threads will be scaled down.
Provided resources: mem_mb=10000, mem_mib=9537, disk_mb=1000, disk_mib=954
Select jobs to execute...
Building DAG of jobs...
Using shell: /usr/bin/bash
Provided cores: 2
Rules claiming more threads will be scaled down.
Provided resources: mem_mb=10000, mem_mib=9537, disk_mb=1000, disk_mib=954
Select jobs to execute...

[Sat Jun 10 01:34:27 2023]
rule run_model:
    output: Res/PSD95-PDZ3/mochi__fit_tmodel_3state_sparse_dimsum128_subsample100p/two_state_non_equilibrium/model_performance_perepoch.pdf, Res/PSD95-PDZ3/mochi__fit_tmodel_3state_sparse_dimsum128_subsample100p/two_state_non_equilibrium/predicted_fitness.txt, Res/PSD95-PDZ3/mochi__fit_tmodel_3state_sparse_dimsum128_subsample100p/two_state_non_equilibrium/model_weights.txt, Res/PSD95-PDZ3/mochi__fit_tmodel_3state_sparse_dimsum128_subsample100p/two_state_non_equilibrium/model_parameters.txt
    jobid: 0
    reason: Missing output files: Res/PSD95-PDZ3/mochi__fit_tmodel_3state_sparse_dimsum128_subsample100p/two_state_non_equilibrium/predicted_fitness.txt, Res/PSD95-PDZ3/mochi__fit_tmodel_3state_sparse_dimsum128_subsample100p/two_state_non_equilibrium/model_performance_perepoch.pdf, Res/PSD95-PDZ3/mochi__fit_tmodel_3state_sparse_dimsum128_subsample100p/two_state_non_equilibrium/model_weights.txt, Res/PSD95-PDZ3/mochi__fit_tmodel_3state_sparse_dimsum128_subsample100p/two_state_non_equilibrium/model_parameters.txt
    wildcards: protein=PSD95-PDZ3, dataset=mochi__fit_tmodel_3state_sparse_dimsum128_subsample100p, model_type=two_state_non_equilibrium
    resources: mem_mb=10000, mem_mib=9537, disk_mb=1000, disk_mib=954, tmpdir=/tmp/slurm_52959905, slurm_account=u_froehlichf, slurm_partition=cpu

/camp/home/demetzp/.conda/envs/demetzp/lib/python3.8/site-packages/haiku/_src/data_structures.py:37: FutureWarning: jax.tree_structure is deprecated, and will be removed in a future release. Use jax.tree_util.tree_structure instead.
  PyTreeDef = type(jax.tree_structure(None))
No GPU/TPU found, falling back to CPU. (Set TF_CPP_MIN_LOG_LEVEL=0 and rerun for more info.)
wandb: Currently logged in as: demetz-pierre (lab_frohlich). Use `wandb login --relogin` to force relogin
wandb: wandb version 0.15.4 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.15.3
wandb: Run data is saved locally in /nemo/lab/froehlichf/home/users/demetzp/PierreDeMetz/inst/python/wandb/run-20230610_013626-bs9o1gy2
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run Run 1
wandb: ‚≠êÔ∏è View project at https://wandb.ai/lab_frohlich/pierre_mochi__fit_tmodel_3state_doubledeepms
wandb: üöÄ View run at https://wandb.ai/lab_frohlich/pierre_mochi__fit_tmodel_3state_doubledeepms/runs/bs9o1gy2
2023-06-10 01:36:47.121175: E external/org_tensorflow/tensorflow/compiler/xla/python/pjit.cc:461] fastpath_data is none
2023-06-10 01:37:00.741138: E external/org_tensorflow/tensorflow/compiler/xla/python/pjit.cc:461] fastpath_data is none
/camp/home/demetzp/.conda/envs/demetzp/lib/python3.8/site-packages/haiku/_src/data_structures.py:143: FutureWarning: jax.tree_flatten is deprecated, and will be removed in a future release. Use jax.tree_util.tree_flatten instead.
  leaves, treedef = jax.tree_flatten(tree)
/camp/home/demetzp/.conda/envs/demetzp/lib/python3.8/site-packages/haiku/_src/data_structures.py:144: FutureWarning: jax.tree_unflatten is deprecated, and will be removed in a future release. Use jax.tree_util.tree_unflatten instead.
  return jax.tree_unflatten(treedef, leaves)
/camp/home/demetzp/.conda/envs/demetzp/lib/python3.8/site-packages/haiku/_src/data_structures.py:143: FutureWarning: jax.tree_flatten is deprecated, and will be removed in a future release. Use jax.tree_util.tree_flatten instead.
  leaves, treedef = jax.tree_flatten(tree)
/camp/home/demetzp/.conda/envs/demetzp/lib/python3.8/site-packages/haiku/_src/data_structures.py:144: FutureWarning: jax.tree_unflatten is deprecated, and will be removed in a future release. Use jax.tree_util.tree_unflatten instead.
  return jax.tree_unflatten(treedef, leaves)
wandb: Waiting for W&B process to finish... (success).
wandb: 
wandb: Run history:
wandb: val_loss_fit ‚ñà‚ñà‚ñá‚ñá‚ñÜ‚ñÜ‚ñÖ‚ñÖ‚ñÖ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ
wandb: 
wandb: Run summary:
wandb: val_loss_fit 0.36
wandb: 
wandb: üöÄ View run Run 1 at: https://wandb.ai/lab_frohlich/pierre_mochi__fit_tmodel_3state_doubledeepms/runs/bs9o1gy2
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20230610_013626-bs9o1gy2/logs
wandb: - Waiting for wandb.init()...wandb: \ Waiting for wandb.init()...wandb: wandb version 0.15.4 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.15.3
wandb: Run data is saved locally in /nemo/lab/froehlichf/home/users/demetzp/PierreDeMetz/inst/python/wandb/run-20230610_013931-pg3tnqdi
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run Run 2
wandb: ‚≠êÔ∏è View project at https://wandb.ai/lab_frohlich/pierre_mochi__fit_tmodel_3state_doubledeepms
wandb: üöÄ View run at https://wandb.ai/lab_frohlich/pierre_mochi__fit_tmodel_3state_doubledeepms/runs/pg3tnqdi
/camp/home/demetzp/.conda/envs/demetzp/lib/python3.8/site-packages/haiku/_src/data_structures.py:143: FutureWarning: jax.tree_flatten is deprecated, and will be removed in a future release. Use jax.tree_util.tree_flatten instead.
  leaves, treedef = jax.tree_flatten(tree)
/camp/home/demetzp/.conda/envs/demetzp/lib/python3.8/site-packages/haiku/_src/data_structures.py:144: FutureWarning: jax.tree_unflatten is deprecated, and will be removed in a future release. Use jax.tree_util.tree_unflatten instead.
  return jax.tree_unflatten(treedef, leaves)
wandb: Waiting for W&B process to finish... (success).
wandb: 
wandb: Run history:
wandb: val_loss_fit ‚ñà‚ñà‚ñá‚ñÜ‚ñÜ‚ñÜ‚ñÖ‚ñÖ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ
wandb: 
wandb: Run summary:
wandb: val_loss_fit 0.361
wandb: 
wandb: üöÄ View run Run 2 at: https://wandb.ai/lab_frohlich/pierre_mochi__fit_tmodel_3state_doubledeepms/runs/pg3tnqdi
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20230610_013931-pg3tnqdi/logs
wandb: wandb version 0.15.4 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.15.3
wandb: Run data is saved locally in /nemo/lab/froehlichf/home/users/demetzp/PierreDeMetz/inst/python/wandb/run-20230610_014220-52wjenyi
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run Run 3
wandb: ‚≠êÔ∏è View project at https://wandb.ai/lab_frohlich/pierre_mochi__fit_tmodel_3state_doubledeepms
wandb: üöÄ View run at https://wandb.ai/lab_frohlich/pierre_mochi__fit_tmodel_3state_doubledeepms/runs/52wjenyi
/camp/home/demetzp/.conda/envs/demetzp/lib/python3.8/site-packages/haiku/_src/data_structures.py:143: FutureWarning: jax.tree_flatten is deprecated, and will be removed in a future release. Use jax.tree_util.tree_flatten instead.
  leaves, treedef = jax.tree_flatten(tree)
/camp/home/demetzp/.conda/envs/demetzp/lib/python3.8/site-packages/haiku/_src/data_structures.py:144: FutureWarning: jax.tree_unflatten is deprecated, and will be removed in a future release. Use jax.tree_util.tree_unflatten instead.
  return jax.tree_unflatten(treedef, leaves)
wandb: Waiting for W&B process to finish... (success).
wandb: 
wandb: Run history:
wandb: val_loss_fit ‚ñà‚ñà‚ñá‚ñá‚ñÜ‚ñÜ‚ñÖ‚ñÖ‚ñÖ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ
wandb: 
wandb: Run summary:
wandb: val_loss_fit 0.361
wandb: 
wandb: üöÄ View run Run 3 at: https://wandb.ai/lab_frohlich/pierre_mochi__fit_tmodel_3state_doubledeepms/runs/52wjenyi
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20230610_014220-52wjenyi/logs
wandb: - Waiting for wandb.init()...wandb: \ Waiting for wandb.init()...wandb: wandb version 0.15.4 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.15.3
wandb: Run data is saved locally in /nemo/lab/froehlichf/home/users/demetzp/PierreDeMetz/inst/python/wandb/run-20230610_014504-4ckcjg3g
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run Run 4
wandb: ‚≠êÔ∏è View project at https://wandb.ai/lab_frohlich/pierre_mochi__fit_tmodel_3state_doubledeepms
wandb: üöÄ View run at https://wandb.ai/lab_frohlich/pierre_mochi__fit_tmodel_3state_doubledeepms/runs/4ckcjg3g
/camp/home/demetzp/.conda/envs/demetzp/lib/python3.8/site-packages/haiku/_src/data_structures.py:143: FutureWarning: jax.tree_flatten is deprecated, and will be removed in a future release. Use jax.tree_util.tree_flatten instead.
  leaves, treedef = jax.tree_flatten(tree)
/camp/home/demetzp/.conda/envs/demetzp/lib/python3.8/site-packages/haiku/_src/data_structures.py:144: FutureWarning: jax.tree_unflatten is deprecated, and will be removed in a future release. Use jax.tree_util.tree_unflatten instead.
  return jax.tree_unflatten(treedef, leaves)
wandb: Waiting for W&B process to finish... (success).
wandb: - 0.005 MB of 0.005 MB uploaded (0.000 MB deduped)wandb: \ 0.005 MB of 0.005 MB uploaded (0.000 MB deduped)wandb: | 0.005 MB of 0.005 MB uploaded (0.000 MB deduped)wandb: / 0.005 MB of 0.005 MB uploaded (0.000 MB deduped)wandb: 
wandb: Run history:
wandb: val_loss_fit ‚ñà‚ñà‚ñá‚ñá‚ñÜ‚ñÜ‚ñÖ‚ñÖ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ
wandb: 
wandb: Run summary:
wandb: val_loss_fit 0.36
wandb: 
wandb: üöÄ View run Run 4 at: https://wandb.ai/lab_frohlich/pierre_mochi__fit_tmodel_3state_doubledeepms/runs/4ckcjg3g
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20230610_014504-4ckcjg3g/logs
wandb: - Waiting for wandb.init()...wandb: \ Waiting for wandb.init()...wandb: wandb version 0.15.4 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.15.3
wandb: Run data is saved locally in /nemo/lab/froehlichf/home/users/demetzp/PierreDeMetz/inst/python/wandb/run-20230610_014752-zygnh1wn
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run Run 5
wandb: ‚≠êÔ∏è View project at https://wandb.ai/lab_frohlich/pierre_mochi__fit_tmodel_3state_doubledeepms
wandb: üöÄ View run at https://wandb.ai/lab_frohlich/pierre_mochi__fit_tmodel_3state_doubledeepms/runs/zygnh1wn
Warning: Output model directory already exists.
Warning: Output plot directory already exists.
Warning: Output weights directory already exists.
Warning: Output boostrap directory already exists.
Grid search using {'num_samples': 128, 'learning_rate': 0.0001, 'l1_regularization_factor': 0.0001, 'l2_regularization_factor': 0.0001, 'number_additive_traits': 1, 'model_type': 'two_state_non_equilibrium', 'specs': (False, False)}
epoch done with 0.986
epoch done with 0.962
epoch done with 0.939
epoch done with 0.917
epoch done with 0.896
epoch done with 0.875
epoch done with 0.854
epoch done with 0.834
epoch done with 0.815
epoch done with 0.796
epoch done with 0.778
epoch done with 0.760
epoch done with 0.743
epoch done with 0.726
epoch done with 0.709
epoch done with 0.693
epoch done with 0.677
epoch done with 0.662
epoch done with 0.647
epoch done with 0.632
epoch done with 0.618
epoch done with 0.604
epoch done with 0.591
epoch done with 0.578
epoch done with 0.565
epoch done with 0.553
epoch done with 0.541
epoch done with 0.530
epoch done with 0.519
epoch done with 0.509
epoch done with 0.500
epoch done with 0.492
epoch done with 0.484
epoch done with 0.476
epoch done with 0.469
epoch done with 0.463
epoch done with 0.457
epoch done with 0.452
epoch done with 0.447
epoch done with 0.442
epoch done with 0.437
epoch done with 0.433
epoch done with 0.429
epoch done with 0.425
epoch done with 0.421
epoch done with 0.417
epoch done with 0.414
epoch done with 0.411
epoch done with 0.408
epoch done with 0.405
epoch done with 0.402
epoch done with 0.399
epoch done with 0.397
epoch done with 0.394
epoch done with 0.392
epoch done with 0.390
epoch done with 0.388
epoch done with 0.386
epoch done with 0.384
epoch done with 0.382
epoch done with 0.380
epoch done with 0.378
epoch done with 0.377
epoch done with 0.375
epoch done with 0.374
epoch done with 0.372
epoch done with 0.371
epoch done with 0.369
epoch done with 0.368
epoch done with 0.367
epoch done with 0.366
epoch done with 0.364
epoch done with 0.363
epoch done with 0.362
epoch done with 0.360
Grid search using {'num_samples': 128, 'learning_rate': 0.0001, 'l1_regularization_factor': 0.0001, 'l2_regularization_factor': 0.001, 'number_additive_traits': 1, 'model_type': 'two_state_non_equilibrium', 'specs': (False, False)}
epoch done with 1.186
epoch done with 1.146
epoch done with 1.108
epoch done with 1.073
epoch done with 1.039
epoch done with 1.006
epoch done with 0.976
epoch done with 0.947
epoch done with 0.919
epoch done with 0.892
epoch done with 0.867
epoch done with 0.842
epoch done with 0.819
epoch done with 0.797
epoch done with 0.775
epoch done with 0.754
epoch done with 0.734
epoch done with 0.715
epoch done with 0.696
epoch done with 0.678
epoch done with 0.660
epoch done with 0.644
epoch done with 0.627
epoch done with 0.612
epoch done with 0.596
epoch done with 0.582
epoch done with 0.568
epoch done with 0.555
epoch done with 0.542
epoch done with 0.530
epoch done with 0.519
epoch done with 0.509
epoch done with 0.499
epoch done with 0.490
epoch done with 0.481
epoch done with 0.474
epoch done with 0.466
epoch done with 0.459
epoch done with 0.453
epoch done with 0.448
epoch done with 0.442
epoch done with 0.437
epoch done with 0.432
epoch done with 0.428
epoch done with 0.423
epoch done with 0.419
epoch done with 0.415
epoch done with 0.412
epoch done with 0.409
epoch done with 0.405
epoch done with 0.402
epoch done with 0.400
epoch done with 0.397
epoch done with 0.395
epoch done with 0.392
epoch done with 0.390
epoch done with 0.388
epoch done with 0.386
epoch done with 0.384
epoch done with 0.382
epoch done with 0.380
epoch done with 0.379
epoch done with 0.377
epoch done with 0.375
epoch done with 0.374
epoch done with 0.372
epoch done with 0.371
epoch done with 0.370
epoch done with 0.368
epoch done with 0.367
epoch done with 0.366
epoch done with 0.364
epoch done with 0.363
epoch done with 0.362
epoch done with 0.361
Grid search using {'num_samples': 128, 'learning_rate': 0.0001, 'l1_regularization_factor': 0.0001, 'l2_regularization_factor': 0.01, 'number_additive_traits': 1, 'model_type': 'two_state_non_equilibrium', 'specs': (False, False)}
epoch done with 0.973
epoch done with 0.950
epoch done with 0.928
epoch done with 0.906
epoch done with 0.885
epoch done with 0.864
epoch done with 0.844
epoch done with 0.824
epoch done with 0.805
epoch done with 0.787
epoch done with 0.769
epoch done with 0.751
epoch done with 0.734
epoch done with 0.717
epoch done with 0.701
epoch done with 0.685
epoch done with 0.670
epoch done with 0.655
epoch done with 0.640
epoch done with 0.626
epoch done with 0.612
epoch done with 0.598
epoch done with 0.585
epoch done with 0.572
epoch done with 0.560
epoch done with 0.548
epoch done with 0.537
epoch done with 0.526
epoch done with 0.516
epoch done with 0.506
epoch done with 0.497
epoch done with 0.489
epoch done with 0.482
epoch done with 0.474
epoch done with 0.468
epoch done with 0.462
epoch done with 0.456
epoch done with 0.451
epoch done with 0.445
epoch done with 0.441
epoch done with 0.436
epoch done with 0.432
epoch done with 0.428
epoch done with 0.424
epoch done with 0.420
epoch done with 0.417
epoch done with 0.413
epoch done with 0.410
epoch done with 0.407
epoch done with 0.404
epoch done with 0.402
epoch done with 0.399
epoch done with 0.397
epoch done with 0.394
epoch done with 0.392
epoch done with 0.390
epoch done with 0.387
epoch done with 0.385
epoch done with 0.383
epoch done with 0.381
epoch done with 0.380
epoch done with 0.378
epoch done with 0.376
epoch done with 0.375
epoch done with 0.373
epoch done with 0.372
epoch done with 0.371
epoch done with 0.369
epoch done with 0.368
epoch done with 0.367
epoch done with 0.366
epoch done with 0.365
epoch done with 0.363
epoch done with 0.362
epoch done with 0.361
Grid search using {'num_samples': 128, 'learning_rate': 0.0001, 'l1_regularization_factor': 0.0001, 'l2_regularization_factor': 0.1, 'number_additive_traits': 1, 'model_type': 'two_state_non_equilibrium', 'specs': (False, False)}
epoch done with 1.322
epoch done with 1.276
epoch done with 1.233
epoch done with 1.192
epoch done with 1.153
epoch done with 1.117
epoch done with 1.082
epoch done with 1.048
epoch done with 1.017
epoch done with 0.986
epoch done with 0.957
epoch done with 0.929
epoch done with 0.903
epoch done with 0.878
epoch done with 0.853
epoch done with 0.829
epoch done with 0.807
epoch done with 0.785
epoch done with 0.764
epoch done with 0.743
epoch done with 0.723
epoch done with 0.704
epoch done with 0.685
epoch done with 0.667
epoch done with 0.650
epoch done with 0.633
epoch done with 0.617
epoch done with 0.601
epoch done with 0.586
epoch done with 0.572
epoch done with 0.558
epoch done with 0.545
epoch done with 0.532
epoch done with 0.520
epoch done with 0.509
epoch done with 0.499
epoch done with 0.489
epoch done with 0.480
epoch done with 0.472
epoch done with 0.465
epoch done with 0.457
epoch done with 0.451
epoch done with 0.444
epoch done with 0.438
epoch done with 0.433
epoch done with 0.428
epoch done with 0.423
epoch done with 0.419
epoch done with 0.415
epoch done with 0.411
epoch done with 0.408
epoch done with 0.404
epoch done with 0.401
epoch done with 0.398
epoch done with 0.396
epoch done with 0.393
epoch done with 0.391
epoch done with 0.388
epoch done with 0.386
epoch done with 0.384
epoch done with 0.382
epoch done with 0.380
epoch done with 0.378
epoch done with 0.377
epoch done with 0.375
epoch done with 0.373
epoch done with 0.372
epoch done with 0.370
epoch done with 0.369
epoch done with 0.367
epoch done with 0.366
epoch done with 0.364
epoch done with 0.363
epoch done with 0.362
epoch done with 0.360
Grid search using {'num_samples': 128, 'learning_rate': 0.0001, 'l1_regularization_factor': 0.001, 'l2_regularization_factor': 0.0001, 'number_additive_traits': 1, 'model_type': 'two_state_non_equilibrium', 'specs': (False, False)}
epoch done with 1.275
epoch done with 1.246
epoch done with 1.217
epoch done with 1.188
epoch done with 1.161
epoch done with 1.135
epoch done with 1.109
epoch done with 1.083
epoch done with 1.059
epoch done with 1.035
/camp/home/demetzp/.conda/envs/demetzp/lib/python3.8/site-packages/haiku/_src/data_structures.py:143: FutureWarning: jax.tree_flatten is deprecated, and will be removed in a future release. Use jax.tree_util.tree_flatten instead.
  leaves, treedef = jax.tree_flatten(tree)
/camp/home/demetzp/.conda/envs/demetzp/lib/python3.8/site-packages/haiku/_src/data_structures.py:144: FutureWarning: jax.tree_unflatten is deprecated, and will be removed in a future release. Use jax.tree_util.tree_unflatten instead.
  return jax.tree_unflatten(treedef, leaves)
wandb: Waiting for W&B process to finish... (success).
wandb: - 0.005 MB of 0.005 MB uploaded (0.000 MB deduped)wandb: \ 0.005 MB of 0.017 MB uploaded (0.000 MB deduped)wandb: | 0.005 MB of 0.017 MB uploaded (0.000 MB deduped)wandb: / 0.017 MB of 0.017 MB uploaded (0.000 MB deduped)wandb: - 0.017 MB of 0.017 MB uploaded (0.000 MB deduped)wandb: 
wandb: Run history:
wandb: val_loss_fit ‚ñà‚ñà‚ñá‚ñá‚ñá‚ñÜ‚ñÜ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ
wandb: 
wandb: Run summary:
wandb: val_loss_fit 0.366
wandb: 
wandb: üöÄ View run Run 5 at: https://wandb.ai/lab_frohlich/pierre_mochi__fit_tmodel_3state_doubledeepms/runs/zygnh1wn
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20230610_014752-zygnh1wn/logs
wandb: - Waiting for wandb.init()...wandb: \ Waiting for wandb.init()...wandb: wandb version 0.15.4 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.15.3
wandb: Run data is saved locally in /nemo/lab/froehlichf/home/users/demetzp/PierreDeMetz/inst/python/wandb/run-20230610_015058-g8t1k7j1
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run Run 6
wandb: ‚≠êÔ∏è View project at https://wandb.ai/lab_frohlich/pierre_mochi__fit_tmodel_3state_doubledeepms
wandb: üöÄ View run at https://wandb.ai/lab_frohlich/pierre_mochi__fit_tmodel_3state_doubledeepms/runs/g8t1k7j1
/camp/home/demetzp/.conda/envs/demetzp/lib/python3.8/site-packages/haiku/_src/data_structures.py:143: FutureWarning: jax.tree_flatten is deprecated, and will be removed in a future release. Use jax.tree_util.tree_flatten instead.
  leaves, treedef = jax.tree_flatten(tree)
/camp/home/demetzp/.conda/envs/demetzp/lib/python3.8/site-packages/haiku/_src/data_structures.py:144: FutureWarning: jax.tree_unflatten is deprecated, and will be removed in a future release. Use jax.tree_util.tree_unflatten instead.
  return jax.tree_unflatten(treedef, leaves)
wandb: Waiting for W&B process to finish... (success).
wandb: - 0.005 MB of 0.005 MB uploaded (0.000 MB deduped)wandb: \ 0.005 MB of 0.015 MB uploaded (0.000 MB deduped)wandb: | 0.005 MB of 0.015 MB uploaded (0.000 MB deduped)wandb: 
wandb: Run history:
wandb: val_loss_fit ‚ñà‚ñà‚ñá‚ñá‚ñÜ‚ñÜ‚ñÖ‚ñÖ‚ñÖ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ
wandb: 
wandb: Run summary:
wandb: val_loss_fit 0.361
wandb: 
wandb: üöÄ View run Run 6 at: https://wandb.ai/lab_frohlich/pierre_mochi__fit_tmodel_3state_doubledeepms/runs/g8t1k7j1
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20230610_015058-g8t1k7j1/logs
wandb: - Waiting for wandb.init()...wandb: \ Waiting for wandb.init()...wandb: wandb version 0.15.4 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.15.3
wandb: Run data is saved locally in /nemo/lab/froehlichf/home/users/demetzp/PierreDeMetz/inst/python/wandb/run-20230610_015353-anz1e5fh
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run Run 7
wandb: ‚≠êÔ∏è View project at https://wandb.ai/lab_frohlich/pierre_mochi__fit_tmodel_3state_doubledeepms
wandb: üöÄ View run at https://wandb.ai/lab_frohlich/pierre_mochi__fit_tmodel_3state_doubledeepms/runs/anz1e5fh
/camp/home/demetzp/.conda/envs/demetzp/lib/python3.8/site-packages/haiku/_src/data_structures.py:143: FutureWarning: jax.tree_flatten is deprecated, and will be removed in a future release. Use jax.tree_util.tree_flatten instead.
  leaves, treedef = jax.tree_flatten(tree)
/camp/home/demetzp/.conda/envs/demetzp/lib/python3.8/site-packages/haiku/_src/data_structures.py:144: FutureWarning: jax.tree_unflatten is deprecated, and will be removed in a future release. Use jax.tree_util.tree_unflatten instead.
  return jax.tree_unflatten(treedef, leaves)
wandb: Waiting for W&B process to finish... (success).
wandb: 
wandb: Run history:
wandb: val_loss_fit ‚ñà‚ñà‚ñá‚ñá‚ñÜ‚ñÜ‚ñÜ‚ñÖ‚ñÖ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ
wandb: 
wandb: Run summary:
wandb: val_loss_fit 0.361
wandb: 
wandb: üöÄ View run Run 7 at: https://wandb.ai/lab_frohlich/pierre_mochi__fit_tmodel_3state_doubledeepms/runs/anz1e5fh
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20230610_015353-anz1e5fh/logs
wandb: - Waiting for wandb.init()...wandb: \ Waiting for wandb.init()...wandb: wandb version 0.15.4 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.15.3
wandb: Run data is saved locally in /nemo/lab/froehlichf/home/users/demetzp/PierreDeMetz/inst/python/wandb/run-20230610_015647-nvs1f7ql
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run Run 8
wandb: ‚≠êÔ∏è View project at https://wandb.ai/lab_frohlich/pierre_mochi__fit_tmodel_3state_doubledeepms
wandb: üöÄ View run at https://wandb.ai/lab_frohlich/pierre_mochi__fit_tmodel_3state_doubledeepms/runs/nvs1f7ql
/camp/home/demetzp/.conda/envs/demetzp/lib/python3.8/site-packages/haiku/_src/data_structures.py:143: FutureWarning: jax.tree_flatten is deprecated, and will be removed in a future release. Use jax.tree_util.tree_flatten instead.
  leaves, treedef = jax.tree_flatten(tree)
/camp/home/demetzp/.conda/envs/demetzp/lib/python3.8/site-packages/haiku/_src/data_structures.py:144: FutureWarning: jax.tree_unflatten is deprecated, and will be removed in a future release. Use jax.tree_util.tree_unflatten instead.
  return jax.tree_unflatten(treedef, leaves)
wandb: Waiting for W&B process to finish... (success).
wandb: - 0.005 MB of 0.005 MB uploaded (0.000 MB deduped)wandb: \ 0.005 MB of 0.015 MB uploaded (0.000 MB deduped)wandb: | 0.005 MB of 0.015 MB uploaded (0.000 MB deduped)wandb: 
wandb: Run history:
wandb: val_loss_fit ‚ñà‚ñà‚ñá‚ñá‚ñÜ‚ñÜ‚ñÜ‚ñÖ‚ñÖ‚ñÖ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ
wandb: 
wandb: Run summary:
wandb: val_loss_fit 0.367
wandb: 
wandb: üöÄ View run Run 8 at: https://wandb.ai/lab_frohlich/pierre_mochi__fit_tmodel_3state_doubledeepms/runs/nvs1f7ql
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20230610_015647-nvs1f7ql/logs
wandb: - Waiting for wandb.init()...wandb: \ Waiting for wandb.init()...wandb: wandb version 0.15.4 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.15.3
wandb: Run data is saved locally in /nemo/lab/froehlichf/home/users/demetzp/PierreDeMetz/inst/python/wandb/run-20230610_015940-ehzv8h8t
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run Run 9
wandb: ‚≠êÔ∏è View project at https://wandb.ai/lab_frohlich/pierre_mochi__fit_tmodel_3state_doubledeepms
wandb: üöÄ View run at https://wandb.ai/lab_frohlich/pierre_mochi__fit_tmodel_3state_doubledeepms/runs/ehzv8h8t
epoch done with 1.012
epoch done with 0.989
epoch done with 0.967
epoch done with 0.946
epoch done with 0.925
epoch done with 0.905
epoch done with 0.885
epoch done with 0.865
epoch done with 0.846
epoch done with 0.828
epoch done with 0.809
epoch done with 0.792
epoch done with 0.774
epoch done with 0.757
epoch done with 0.740
epoch done with 0.724
epoch done with 0.708
epoch done with 0.692
epoch done with 0.677
epoch done with 0.662
epoch done with 0.647
epoch done with 0.632
epoch done with 0.618
epoch done with 0.604
epoch done with 0.591
epoch done with 0.578
epoch done with 0.565
epoch done with 0.552
epoch done with 0.540
epoch done with 0.529
epoch done with 0.517
epoch done with 0.507
epoch done with 0.497
epoch done with 0.488
epoch done with 0.479
epoch done with 0.471
epoch done with 0.463
epoch done with 0.456
epoch done with 0.449
epoch done with 0.443
epoch done with 0.437
epoch done with 0.432
epoch done with 0.427
epoch done with 0.422
epoch done with 0.417
epoch done with 0.413
epoch done with 0.409
epoch done with 0.405
epoch done with 0.402
epoch done with 0.399
epoch done with 0.396
epoch done with 0.393
epoch done with 0.390
epoch done with 0.388
epoch done with 0.385
epoch done with 0.383
epoch done with 0.381
epoch done with 0.378
epoch done with 0.376
epoch done with 0.374
epoch done with 0.373
epoch done with 0.371
epoch done with 0.369
epoch done with 0.367
epoch done with 0.366
Grid search using {'num_samples': 128, 'learning_rate': 0.0001, 'l1_regularization_factor': 0.001, 'l2_regularization_factor': 0.001, 'number_additive_traits': 1, 'model_type': 'two_state_non_equilibrium', 'specs': (False, False)}
epoch done with 0.971
epoch done with 0.948
epoch done with 0.925
epoch done with 0.904
epoch done with 0.883
epoch done with 0.862
epoch done with 0.842
epoch done with 0.823
epoch done with 0.804
epoch done with 0.785
epoch done with 0.767
epoch done with 0.750
epoch done with 0.733
epoch done with 0.716
epoch done with 0.700
epoch done with 0.684
epoch done with 0.669
epoch done with 0.654
epoch done with 0.639
epoch done with 0.625
epoch done with 0.611
epoch done with 0.597
epoch done with 0.584
epoch done with 0.572
epoch done with 0.559
epoch done with 0.547
epoch done with 0.536
epoch done with 0.526
epoch done with 0.516
epoch done with 0.506
epoch done with 0.497
epoch done with 0.489
epoch done with 0.482
epoch done with 0.475
epoch done with 0.468
epoch done with 0.462
epoch done with 0.456
epoch done with 0.451
epoch done with 0.446
epoch done with 0.441
epoch done with 0.437
epoch done with 0.432
epoch done with 0.428
epoch done with 0.424
epoch done with 0.421
epoch done with 0.417
epoch done with 0.414
epoch done with 0.411
epoch done with 0.408
epoch done with 0.405
epoch done with 0.402
epoch done with 0.400
epoch done with 0.397
epoch done with 0.395
epoch done with 0.392
epoch done with 0.390
epoch done with 0.388
epoch done with 0.386
epoch done with 0.384
epoch done with 0.382
epoch done with 0.380
epoch done with 0.378
epoch done with 0.377
epoch done with 0.375
epoch done with 0.374
epoch done with 0.373
epoch done with 0.371
epoch done with 0.370
epoch done with 0.369
epoch done with 0.367
epoch done with 0.366
epoch done with 0.365
epoch done with 0.364
epoch done with 0.363
epoch done with 0.361
Grid search using {'num_samples': 128, 'learning_rate': 0.0001, 'l1_regularization_factor': 0.001, 'l2_regularization_factor': 0.01, 'number_additive_traits': 1, 'model_type': 'two_state_non_equilibrium', 'specs': (False, False)}
epoch done with 0.999
epoch done with 0.975
epoch done with 0.952
epoch done with 0.930
epoch done with 0.909
epoch done with 0.888
epoch done with 0.867
epoch done with 0.847
epoch done with 0.828
epoch done with 0.809
epoch done with 0.790
epoch done with 0.772
epoch done with 0.755
epoch done with 0.738
epoch done with 0.721
epoch done with 0.705
epoch done with 0.689
epoch done with 0.674
epoch done with 0.659
epoch done with 0.644
epoch done with 0.629
epoch done with 0.615
epoch done with 0.602
epoch done with 0.588
epoch done with 0.575
epoch done with 0.563
epoch done with 0.551
epoch done with 0.539
epoch done with 0.528
epoch done with 0.518
epoch done with 0.508
epoch done with 0.499
epoch done with 0.491
epoch done with 0.483
epoch done with 0.475
epoch done with 0.468
epoch done with 0.462
epoch done with 0.456
epoch done with 0.451
epoch done with 0.446
epoch done with 0.441
epoch done with 0.436
epoch done with 0.432
epoch done with 0.428
epoch done with 0.424
epoch done with 0.420
epoch done with 0.416
epoch done with 0.413
epoch done with 0.410
epoch done with 0.407
epoch done with 0.404
epoch done with 0.401
epoch done with 0.399
epoch done with 0.396
epoch done with 0.394
epoch done with 0.391
epoch done with 0.389
epoch done with 0.387
epoch done with 0.385
epoch done with 0.383
epoch done with 0.381
epoch done with 0.379
epoch done with 0.377
epoch done with 0.376
epoch done with 0.374
epoch done with 0.373
epoch done with 0.371
epoch done with 0.370
epoch done with 0.368
epoch done with 0.367
epoch done with 0.366
epoch done with 0.365
epoch done with 0.363
epoch done with 0.362
epoch done with 0.361
Grid search using {'num_samples': 128, 'learning_rate': 0.0001, 'l1_regularization_factor': 0.001, 'l2_regularization_factor': 0.1, 'number_additive_traits': 1, 'model_type': 'two_state_non_equilibrium', 'specs': (False, False)}
epoch done with 0.624
epoch done with 0.615
epoch done with 0.606
epoch done with 0.597
epoch done with 0.588
epoch done with 0.579
epoch done with 0.571
epoch done with 0.563
epoch done with 0.555
epoch done with 0.547
epoch done with 0.540
epoch done with 0.533
epoch done with 0.527
epoch done with 0.520
epoch done with 0.514
epoch done with 0.508
epoch done with 0.502
epoch done with 0.496
epoch done with 0.491
epoch done with 0.486
epoch done with 0.480
epoch done with 0.476
epoch done with 0.471
epoch done with 0.466
epoch done with 0.462
epoch done with 0.457
epoch done with 0.453
epoch done with 0.449
epoch done with 0.445
epoch done with 0.442
epoch done with 0.438
epoch done with 0.435
epoch done with 0.431
epoch done with 0.428
epoch done with 0.425
epoch done with 0.422
epoch done with 0.419
epoch done with 0.416
epoch done with 0.414
epoch done with 0.411
epoch done with 0.409
epoch done with 0.406
epoch done with 0.404
epoch done with 0.402
epoch done with 0.400
epoch done with 0.398
epoch done with 0.396
epoch done with 0.394
epoch done with 0.392
epoch done with 0.391
epoch done with 0.389
epoch done with 0.388
epoch done with 0.386
epoch done with 0.385
epoch done with 0.384
epoch done with 0.382
epoch done with 0.381
epoch done with 0.380
epoch done with 0.379
epoch done with 0.378
epoch done with 0.377
epoch done with 0.376
epoch done with 0.375
epoch done with 0.374
epoch done with 0.373
epoch done with 0.373
epoch done with 0.372
epoch done with 0.371
epoch done with 0.371
epoch done with 0.370
epoch done with 0.369
epoch done with 0.369
epoch done with 0.368
epoch done with 0.368
epoch done with 0.367
Grid search using {'num_samples': 128, 'learning_rate': 0.0001, 'l1_regularization_factor': 0.01, 'l2_regularization_factor': 0.0001, 'number_additive_traits': 1, 'model_type': 'two_state_non_equilibrium', 'specs': (False, False)}
epoch done with 0.745
epoch done with 0.727
epoch done with 0.710
epoch done with 0.693
epoch done with 0.677
epoch done with 0.661
epoch done with 0.646
epoch done with 0.631
epoch done with 0.617
epoch done with 0.604
epoch done with 0.591
epoch done with 0.579
epoch done with 0.567
epoch done with 0.557
epoch done with 0.547
epoch done with 0.538
epoch done with 0.529
epoch done with 0.521
epoch done with 0.513
epoch done with 0.505
epoch done with 0.498
epoch done with 0.491
epoch done with 0.485
epoch done with 0.479
epoch done with 0.473
epoch done with 0.467
epoch done with 0.462
epoch done with 0.457
epoch done with 0.452
epoch done with 0.448
epoch done with 0.443
epoch done with 0.439
epoch done with 0.435
epoch done with 0.432
epoch done with 0.428
epoch done with 0.425
epoch done with 0.421
epoch done with 0.418
epoch done with 0.415
epoch done with 0.412
/camp/home/demetzp/.conda/envs/demetzp/lib/python3.8/site-packages/haiku/_src/data_structures.py:143: FutureWarning: jax.tree_flatten is deprecated, and will be removed in a future release. Use jax.tree_util.tree_flatten instead.
  leaves, treedef = jax.tree_flatten(tree)
/camp/home/demetzp/.conda/envs/demetzp/lib/python3.8/site-packages/haiku/_src/data_structures.py:144: FutureWarning: jax.tree_unflatten is deprecated, and will be removed in a future release. Use jax.tree_util.tree_unflatten instead.
  return jax.tree_unflatten(treedef, leaves)
wandb: Waiting for W&B process to finish... (success).
wandb: - 0.005 MB of 0.017 MB uploaded (0.000 MB deduped)wandb: \ 0.017 MB of 0.017 MB uploaded (0.000 MB deduped)wandb: | 0.017 MB of 0.017 MB uploaded (0.000 MB deduped)wandb: 
wandb: Run history:
wandb: val_loss_fit ‚ñà‚ñà‚ñá‚ñÜ‚ñÜ‚ñÖ‚ñÖ‚ñÖ‚ñÑ‚ñÑ‚ñÑ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ
wandb: 
wandb: Run summary:
wandb: val_loss_fit 0.368
wandb: 
wandb: üöÄ View run Run 9 at: https://wandb.ai/lab_frohlich/pierre_mochi__fit_tmodel_3state_doubledeepms/runs/ehzv8h8t
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20230610_015940-ehzv8h8t/logs
wandb: - Waiting for wandb.init()...wandb: \ Waiting for wandb.init()...wandb: wandb version 0.15.4 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.15.3
wandb: Run data is saved locally in /nemo/lab/froehlichf/home/users/demetzp/PierreDeMetz/inst/python/wandb/run-20230610_020229-ecnm16yx
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run Run 10
wandb: ‚≠êÔ∏è View project at https://wandb.ai/lab_frohlich/pierre_mochi__fit_tmodel_3state_doubledeepms
wandb: üöÄ View run at https://wandb.ai/lab_frohlich/pierre_mochi__fit_tmodel_3state_doubledeepms/runs/ecnm16yx
/camp/home/demetzp/.conda/envs/demetzp/lib/python3.8/site-packages/haiku/_src/data_structures.py:143: FutureWarning: jax.tree_flatten is deprecated, and will be removed in a future release. Use jax.tree_util.tree_flatten instead.
  leaves, treedef = jax.tree_flatten(tree)
/camp/home/demetzp/.conda/envs/demetzp/lib/python3.8/site-packages/haiku/_src/data_structures.py:144: FutureWarning: jax.tree_unflatten is deprecated, and will be removed in a future release. Use jax.tree_util.tree_unflatten instead.
  return jax.tree_unflatten(treedef, leaves)
wandb: Waiting for W&B process to finish... (success).
wandb: 
wandb: Run history:
wandb: val_loss_fit ‚ñà‚ñà‚ñá‚ñá‚ñÜ‚ñÜ‚ñÜ‚ñÖ‚ñÖ‚ñÖ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ
wandb: 
wandb: Run summary:
wandb: val_loss_fit 0.365
wandb: 
wandb: üöÄ View run Run 10 at: https://wandb.ai/lab_frohlich/pierre_mochi__fit_tmodel_3state_doubledeepms/runs/ecnm16yx
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20230610_020229-ecnm16yx/logs
wandb: - Waiting for wandb.init()...wandb: \ Waiting for wandb.init()...wandb: wandb version 0.15.4 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.15.3
wandb: Run data is saved locally in /nemo/lab/froehlichf/home/users/demetzp/PierreDeMetz/inst/python/wandb/run-20230610_020519-khs7tysx
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run Run 11
wandb: ‚≠êÔ∏è View project at https://wandb.ai/lab_frohlich/pierre_mochi__fit_tmodel_3state_doubledeepms
wandb: üöÄ View run at https://wandb.ai/lab_frohlich/pierre_mochi__fit_tmodel_3state_doubledeepms/runs/khs7tysx
/camp/home/demetzp/.conda/envs/demetzp/lib/python3.8/site-packages/haiku/_src/data_structures.py:143: FutureWarning: jax.tree_flatten is deprecated, and will be removed in a future release. Use jax.tree_util.tree_flatten instead.
  leaves, treedef = jax.tree_flatten(tree)
/camp/home/demetzp/.conda/envs/demetzp/lib/python3.8/site-packages/haiku/_src/data_structures.py:144: FutureWarning: jax.tree_unflatten is deprecated, and will be removed in a future release. Use jax.tree_util.tree_unflatten instead.
  return jax.tree_unflatten(treedef, leaves)
wandb: Waiting for W&B process to finish... (success).
wandb: 
wandb: Run history:
wandb: val_loss_fit ‚ñà‚ñà‚ñá‚ñá‚ñÜ‚ñÜ‚ñÜ‚ñÖ‚ñÖ‚ñÖ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ
wandb: 
wandb: Run summary:
wandb: val_loss_fit 0.367
wandb: 
wandb: üöÄ View run Run 11 at: https://wandb.ai/lab_frohlich/pierre_mochi__fit_tmodel_3state_doubledeepms/runs/khs7tysx
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20230610_020519-khs7tysx/logs
wandb: - Waiting for wandb.init()...wandb: \ Waiting for wandb.init()...wandb: wandb version 0.15.4 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.15.3
wandb: Run data is saved locally in /nemo/lab/froehlichf/home/users/demetzp/PierreDeMetz/inst/python/wandb/run-20230610_020810-3ogtvnvl
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run Run 12
wandb: ‚≠êÔ∏è View project at https://wandb.ai/lab_frohlich/pierre_mochi__fit_tmodel_3state_doubledeepms
wandb: üöÄ View run at https://wandb.ai/lab_frohlich/pierre_mochi__fit_tmodel_3state_doubledeepms/runs/3ogtvnvl
/camp/home/demetzp/.conda/envs/demetzp/lib/python3.8/site-packages/haiku/_src/data_structures.py:143: FutureWarning: jax.tree_flatten is deprecated, and will be removed in a future release. Use jax.tree_util.tree_flatten instead.
  leaves, treedef = jax.tree_flatten(tree)
/camp/home/demetzp/.conda/envs/demetzp/lib/python3.8/site-packages/haiku/_src/data_structures.py:144: FutureWarning: jax.tree_unflatten is deprecated, and will be removed in a future release. Use jax.tree_util.tree_unflatten instead.
  return jax.tree_unflatten(treedef, leaves)
wandb: Waiting for W&B process to finish... (success).
wandb: 
wandb: Run history:
wandb: val_loss_fit ‚ñà‚ñà‚ñá‚ñÜ‚ñÜ‚ñÖ‚ñÖ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ
wandb: 
wandb: Run summary:
wandb: val_loss_fit 0.361
wandb: 
wandb: üöÄ View run Run 12 at: https://wandb.ai/lab_frohlich/pierre_mochi__fit_tmodel_3state_doubledeepms/runs/3ogtvnvl
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20230610_020810-3ogtvnvl/logs
wandb: - Waiting for wandb.init()...wandb: \ Waiting for wandb.init()...wandb: wandb version 0.15.4 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.15.3
wandb: Run data is saved locally in /nemo/lab/froehlichf/home/users/demetzp/PierreDeMetz/inst/python/wandb/run-20230610_021102-vts8ra66
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run Run 13
wandb: ‚≠êÔ∏è View project at https://wandb.ai/lab_frohlich/pierre_mochi__fit_tmodel_3state_doubledeepms
wandb: üöÄ View run at https://wandb.ai/lab_frohlich/pierre_mochi__fit_tmodel_3state_doubledeepms/runs/vts8ra66
epoch done with 0.410
epoch done with 0.408
epoch done with 0.406
epoch done with 0.403
epoch done with 0.401
epoch done with 0.400
epoch done with 0.398
epoch done with 0.396
epoch done with 0.394
epoch done with 0.392
epoch done with 0.391
epoch done with 0.389
epoch done with 0.388
epoch done with 0.386
epoch done with 0.385
epoch done with 0.384
epoch done with 0.382
epoch done with 0.381
epoch done with 0.380
epoch done with 0.379
epoch done with 0.378
epoch done with 0.377
epoch done with 0.376
epoch done with 0.375
epoch done with 0.374
epoch done with 0.373
epoch done with 0.373
epoch done with 0.372
epoch done with 0.371
epoch done with 0.371
epoch done with 0.370
epoch done with 0.369
epoch done with 0.369
epoch done with 0.368
epoch done with 0.368
Grid search using {'num_samples': 128, 'learning_rate': 0.0001, 'l1_regularization_factor': 0.01, 'l2_regularization_factor': 0.001, 'number_additive_traits': 1, 'model_type': 'two_state_non_equilibrium', 'specs': (False, False)}
epoch done with 1.357
epoch done with 1.317
epoch done with 1.280
epoch done with 1.244
epoch done with 1.209
epoch done with 1.176
epoch done with 1.145
epoch done with 1.115
epoch done with 1.086
epoch done with 1.059
epoch done with 1.032
epoch done with 1.007
epoch done with 0.982
epoch done with 0.958
epoch done with 0.936
epoch done with 0.913
epoch done with 0.892
epoch done with 0.871
epoch done with 0.851
epoch done with 0.833
epoch done with 0.815
epoch done with 0.797
epoch done with 0.780
epoch done with 0.763
epoch done with 0.746
epoch done with 0.730
epoch done with 0.714
epoch done with 0.698
epoch done with 0.682
epoch done with 0.667
epoch done with 0.652
epoch done with 0.638
epoch done with 0.623
epoch done with 0.609
epoch done with 0.596
epoch done with 0.582
epoch done with 0.569
epoch done with 0.556
epoch done with 0.544
epoch done with 0.532
epoch done with 0.521
epoch done with 0.510
epoch done with 0.499
epoch done with 0.490
epoch done with 0.480
epoch done with 0.472
epoch done with 0.464
epoch done with 0.456
epoch done with 0.450
epoch done with 0.443
epoch done with 0.437
epoch done with 0.432
epoch done with 0.426
epoch done with 0.422
epoch done with 0.417
epoch done with 0.413
epoch done with 0.409
epoch done with 0.405
epoch done with 0.401
epoch done with 0.398
epoch done with 0.395
epoch done with 0.392
epoch done with 0.389
epoch done with 0.387
epoch done with 0.384
epoch done with 0.382
epoch done with 0.380
epoch done with 0.377
epoch done with 0.375
epoch done with 0.373
epoch done with 0.371
epoch done with 0.370
epoch done with 0.368
epoch done with 0.366
epoch done with 0.365
Grid search using {'num_samples': 128, 'learning_rate': 0.0001, 'l1_regularization_factor': 0.01, 'l2_regularization_factor': 0.01, 'number_additive_traits': 1, 'model_type': 'two_state_non_equilibrium', 'specs': (False, False)}
epoch done with 0.624
epoch done with 0.615
epoch done with 0.606
epoch done with 0.597
epoch done with 0.588
epoch done with 0.579
epoch done with 0.571
epoch done with 0.563
epoch done with 0.555
epoch done with 0.547
epoch done with 0.540
epoch done with 0.533
epoch done with 0.527
epoch done with 0.520
epoch done with 0.514
epoch done with 0.508
epoch done with 0.502
epoch done with 0.496
epoch done with 0.491
epoch done with 0.485
epoch done with 0.480
epoch done with 0.476
epoch done with 0.471
epoch done with 0.466
epoch done with 0.462
epoch done with 0.457
epoch done with 0.453
epoch done with 0.449
epoch done with 0.445
epoch done with 0.441
epoch done with 0.438
epoch done with 0.434
epoch done with 0.431
epoch done with 0.428
epoch done with 0.425
epoch done with 0.422
epoch done with 0.419
epoch done with 0.416
epoch done with 0.413
epoch done with 0.411
epoch done with 0.409
epoch done with 0.406
epoch done with 0.404
epoch done with 0.402
epoch done with 0.400
epoch done with 0.398
epoch done with 0.396
epoch done with 0.394
epoch done with 0.392
epoch done with 0.391
epoch done with 0.389
epoch done with 0.388
epoch done with 0.386
epoch done with 0.385
epoch done with 0.384
epoch done with 0.382
epoch done with 0.381
epoch done with 0.380
epoch done with 0.379
epoch done with 0.378
epoch done with 0.377
epoch done with 0.376
epoch done with 0.375
epoch done with 0.374
epoch done with 0.373
epoch done with 0.373
epoch done with 0.372
epoch done with 0.371
epoch done with 0.371
epoch done with 0.370
epoch done with 0.369
epoch done with 0.369
epoch done with 0.368
epoch done with 0.368
epoch done with 0.367
Grid search using {'num_samples': 128, 'learning_rate': 0.0001, 'l1_regularization_factor': 0.01, 'l2_regularization_factor': 0.1, 'number_additive_traits': 1, 'model_type': 'two_state_non_equilibrium', 'specs': (False, False)}
epoch done with 1.261
epoch done with 1.215
epoch done with 1.171
epoch done with 1.129
epoch done with 1.089
epoch done with 1.052
epoch done with 1.016
epoch done with 0.981
epoch done with 0.949
epoch done with 0.917
epoch done with 0.888
epoch done with 0.859
epoch done with 0.832
epoch done with 0.807
epoch done with 0.783
epoch done with 0.760
epoch done with 0.738
epoch done with 0.717
epoch done with 0.697
epoch done with 0.677
epoch done with 0.659
epoch done with 0.641
epoch done with 0.624
epoch done with 0.608
epoch done with 0.592
epoch done with 0.578
epoch done with 0.564
epoch done with 0.550
epoch done with 0.538
epoch done with 0.526
epoch done with 0.515
epoch done with 0.505
epoch done with 0.496
epoch done with 0.487
epoch done with 0.479
epoch done with 0.472
epoch done with 0.464
epoch done with 0.458
epoch done with 0.452
epoch done with 0.447
epoch done with 0.441
epoch done with 0.436
epoch done with 0.431
epoch done with 0.427
epoch done with 0.423
epoch done with 0.419
epoch done with 0.415
epoch done with 0.411
epoch done with 0.408
epoch done with 0.405
epoch done with 0.402
epoch done with 0.400
epoch done with 0.397
epoch done with 0.395
epoch done with 0.392
epoch done with 0.390
epoch done with 0.388
epoch done with 0.386
epoch done with 0.384
epoch done with 0.382
epoch done with 0.381
epoch done with 0.379
epoch done with 0.377
epoch done with 0.376
epoch done with 0.374
epoch done with 0.373
epoch done with 0.371
epoch done with 0.370
epoch done with 0.369
epoch done with 0.367
epoch done with 0.366
epoch done with 0.365
epoch done with 0.363
epoch done with 0.362
epoch done with 0.361
Grid search using {'num_samples': 128, 'learning_rate': 0.0001, 'l1_regularization_factor': 0.1, 'l2_regularization_factor': 0.0001, 'number_additive_traits': 1, 'model_type': 'two_state_non_equilibrium', 'specs': (False, False)}
epoch done with 1.176
epoch done with 1.142
epoch done with 1.110
epoch done with 1.080
epoch done with 1.051
epoch done with 1.027
epoch done with 1.003
epoch done with 0.980
epoch done with 0.958
epoch done with 0.936
epoch done with 0.915
epoch done with 0.894
epoch done with 0.874
epoch done with 0.855
epoch done with 0.835
epoch done with 0.817
epoch done with 0.798
epoch done with 0.781
epoch done with 0.763
epoch done with 0.746
epoch done with 0.729
epoch done with 0.713
epoch done with 0.697
epoch done with 0.681
epoch done with 0.666
epoch done with 0.651
epoch done with 0.636
epoch done with 0.622
epoch done with 0.608
epoch done with 0.594
epoch done with 0.580
epoch done with 0.567
epoch done with 0.555
epoch done with 0.543
epoch done with 0.531
epoch done with 0.520
epoch done with 0.509
epoch done with 0.499
epoch done with 0.490
epoch done with 0.481
epoch done with 0.473
epoch done with 0.466
epoch done with 0.459
epoch done with 0.453
epoch done with 0.447
epoch done with 0.441
epoch done with 0.436
epoch done with 0.431
epoch done with 0.426
epoch done with 0.422
epoch done with 0.418
epoch done with 0.415
epoch done with 0.411
epoch done with 0.408
epoch done with 0.404
epoch done with 0.401
epoch done with 0.398
epoch done with 0.396
epoch done with 0.393
epoch done with 0.390
epoch done with 0.388
epoch done with 0.386
epoch done with 0.383
epoch done with 0.381
epoch done with 0.379
epoch done with 0.377
epoch done with 0.376
epoch done with 0.374
epoch done with 0.372
epoch done with 0.371
/camp/home/demetzp/.conda/envs/demetzp/lib/python3.8/site-packages/haiku/_src/data_structures.py:143: FutureWarning: jax.tree_flatten is deprecated, and will be removed in a future release. Use jax.tree_util.tree_flatten instead.
  leaves, treedef = jax.tree_flatten(tree)
/camp/home/demetzp/.conda/envs/demetzp/lib/python3.8/site-packages/haiku/_src/data_structures.py:144: FutureWarning: jax.tree_unflatten is deprecated, and will be removed in a future release. Use jax.tree_util.tree_unflatten instead.
  return jax.tree_unflatten(treedef, leaves)
wandb: Waiting for W&B process to finish... (success).
wandb: 
wandb: Run history:
wandb: val_loss_fit ‚ñà‚ñà‚ñá‚ñá‚ñÜ‚ñÜ‚ñÜ‚ñÖ‚ñÖ‚ñÖ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ
wandb: 
wandb: Run summary:
wandb: val_loss_fit 0.363
wandb: 
wandb: üöÄ View run Run 13 at: https://wandb.ai/lab_frohlich/pierre_mochi__fit_tmodel_3state_doubledeepms/runs/vts8ra66
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20230610_021102-vts8ra66/logs
wandb: - Waiting for wandb.init()...wandb: \ Waiting for wandb.init()...wandb: wandb version 0.15.4 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.15.3
wandb: Run data is saved locally in /nemo/lab/froehlichf/home/users/demetzp/PierreDeMetz/inst/python/wandb/run-20230610_021406-xn5vqcdm
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run Run 14
wandb: ‚≠êÔ∏è View project at https://wandb.ai/lab_frohlich/pierre_mochi__fit_tmodel_3state_doubledeepms
wandb: üöÄ View run at https://wandb.ai/lab_frohlich/pierre_mochi__fit_tmodel_3state_doubledeepms/runs/xn5vqcdm
/camp/home/demetzp/.conda/envs/demetzp/lib/python3.8/site-packages/haiku/_src/data_structures.py:143: FutureWarning: jax.tree_flatten is deprecated, and will be removed in a future release. Use jax.tree_util.tree_flatten instead.
  leaves, treedef = jax.tree_flatten(tree)
/camp/home/demetzp/.conda/envs/demetzp/lib/python3.8/site-packages/haiku/_src/data_structures.py:144: FutureWarning: jax.tree_unflatten is deprecated, and will be removed in a future release. Use jax.tree_util.tree_unflatten instead.
  return jax.tree_unflatten(treedef, leaves)
wandb: Waiting for W&B process to finish... (success).
wandb: 
wandb: Run history:
wandb: val_loss_fit ‚ñà‚ñà‚ñá‚ñÜ‚ñÜ‚ñÜ‚ñÖ‚ñÖ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ
wandb: 
wandb: Run summary:
wandb: val_loss_fit 0.36
wandb: 
wandb: üöÄ View run Run 14 at: https://wandb.ai/lab_frohlich/pierre_mochi__fit_tmodel_3state_doubledeepms/runs/xn5vqcdm
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20230610_021406-xn5vqcdm/logs
wandb: - Waiting for wandb.init()...wandb: \ Waiting for wandb.init()...wandb: wandb version 0.15.4 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.15.3
wandb: Run data is saved locally in /nemo/lab/froehlichf/home/users/demetzp/PierreDeMetz/inst/python/wandb/run-20230610_021702-id34gl7k
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run Run 15
wandb: ‚≠êÔ∏è View project at https://wandb.ai/lab_frohlich/pierre_mochi__fit_tmodel_3state_doubledeepms
wandb: üöÄ View run at https://wandb.ai/lab_frohlich/pierre_mochi__fit_tmodel_3state_doubledeepms/runs/id34gl7k
/camp/home/demetzp/.conda/envs/demetzp/lib/python3.8/site-packages/haiku/_src/data_structures.py:143: FutureWarning: jax.tree_flatten is deprecated, and will be removed in a future release. Use jax.tree_util.tree_flatten instead.
  leaves, treedef = jax.tree_flatten(tree)
/camp/home/demetzp/.conda/envs/demetzp/lib/python3.8/site-packages/haiku/_src/data_structures.py:144: FutureWarning: jax.tree_unflatten is deprecated, and will be removed in a future release. Use jax.tree_util.tree_unflatten instead.
  return jax.tree_unflatten(treedef, leaves)
wandb: Waiting for W&B process to finish... (success).
wandb: - 0.005 MB of 0.005 MB uploaded (0.000 MB deduped)wandb: \ 0.005 MB of 0.005 MB uploaded (0.000 MB deduped)wandb: | 0.005 MB of 0.005 MB uploaded (0.000 MB deduped)wandb: / 0.005 MB of 0.005 MB uploaded (0.000 MB deduped)wandb: 
wandb: Run history:
wandb: val_loss_fit ‚ñà‚ñà‚ñá‚ñá‚ñÜ‚ñÜ‚ñÖ‚ñÖ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ
wandb: 
wandb: Run summary:
wandb: val_loss_fit 0.362
wandb: 
wandb: üöÄ View run Run 15 at: https://wandb.ai/lab_frohlich/pierre_mochi__fit_tmodel_3state_doubledeepms/runs/id34gl7k
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20230610_021702-id34gl7k/logs
wandb: - Waiting for wandb.init()...wandb: \ Waiting for wandb.init()...wandb: wandb version 0.15.4 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.15.3
wandb: Run data is saved locally in /nemo/lab/froehlichf/home/users/demetzp/PierreDeMetz/inst/python/wandb/run-20230610_021956-gz5oxdbd
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run Run 16
wandb: ‚≠êÔ∏è View project at https://wandb.ai/lab_frohlich/pierre_mochi__fit_tmodel_3state_doubledeepms
wandb: üöÄ View run at https://wandb.ai/lab_frohlich/pierre_mochi__fit_tmodel_3state_doubledeepms/runs/gz5oxdbd
/camp/home/demetzp/.conda/envs/demetzp/lib/python3.8/site-packages/haiku/_src/data_structures.py:143: FutureWarning: jax.tree_flatten is deprecated, and will be removed in a future release. Use jax.tree_util.tree_flatten instead.
  leaves, treedef = jax.tree_flatten(tree)
/camp/home/demetzp/.conda/envs/demetzp/lib/python3.8/site-packages/haiku/_src/data_structures.py:144: FutureWarning: jax.tree_unflatten is deprecated, and will be removed in a future release. Use jax.tree_util.tree_unflatten instead.
  return jax.tree_unflatten(treedef, leaves)
wandb: Waiting for W&B process to finish... (success).
wandb: - 0.005 MB of 0.005 MB uploaded (0.000 MB deduped)wandb: \ 0.005 MB of 0.005 MB uploaded (0.000 MB deduped)wandb: | 0.005 MB of 0.005 MB uploaded (0.000 MB deduped)wandb: / 0.005 MB of 0.005 MB uploaded (0.000 MB deduped)wandb: 
wandb: Run history:
wandb: val_loss_fit ‚ñà‚ñà‚ñá‚ñÜ‚ñÜ‚ñÖ‚ñÖ‚ñÖ‚ñÑ‚ñÑ‚ñÑ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ
wandb: 
wandb: Run summary:
wandb: val_loss_fit 0.362
wandb: 
wandb: üöÄ View run Run 16 at: https://wandb.ai/lab_frohlich/pierre_mochi__fit_tmodel_3state_doubledeepms/runs/gz5oxdbd
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20230610_021956-gz5oxdbd/logs
wandb: - Waiting for wandb.init()...wandb: \ Waiting for wandb.init()...wandb: wandb version 0.15.4 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.15.3
wandb: Run data is saved locally in /nemo/lab/froehlichf/home/users/demetzp/PierreDeMetz/inst/python/wandb/run-20230610_022248-y94fmqfu
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run Run 17
wandb: ‚≠êÔ∏è View project at https://wandb.ai/lab_frohlich/pierre_mochi__fit_tmodel_3state_doubledeepms
wandb: üöÄ View run at https://wandb.ai/lab_frohlich/pierre_mochi__fit_tmodel_3state_doubledeepms/runs/y94fmqfu
/camp/home/demetzp/.conda/envs/demetzp/lib/python3.8/site-packages/haiku/_src/data_structures.py:143: FutureWarning: jax.tree_flatten is deprecated, and will be removed in a future release. Use jax.tree_util.tree_flatten instead.
  leaves, treedef = jax.tree_flatten(tree)
/camp/home/demetzp/.conda/envs/demetzp/lib/python3.8/site-packages/haiku/_src/data_structures.py:144: FutureWarning: jax.tree_unflatten is deprecated, and will be removed in a future release. Use jax.tree_util.tree_unflatten instead.
  return jax.tree_unflatten(treedef, leaves)
wandb: Waiting for W&B process to finish... (success).
wandb: 
wandb: Run history:
wandb: val_loss_fit ‚ñà‚ñÜ‚ñÑ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÇ‚ñÇ‚ñÇ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ
wandb: 
wandb: Run summary:
wandb: val_loss_fit 0.131
wandb: 
wandb: üöÄ View run Run 17 at: https://wandb.ai/lab_frohlich/pierre_mochi__fit_tmodel_3state_doubledeepms/runs/y94fmqfu
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20230610_022248-y94fmqfu/logs
wandb: - Waiting for wandb.init()...wandb: \ Waiting for wandb.init()...wandb: wandb version 0.15.4 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.15.3
wandb: Run data is saved locally in /nemo/lab/froehlichf/home/users/demetzp/PierreDeMetz/inst/python/wandb/run-20230610_022541-fmv2ud8f
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run Run 18
wandb: ‚≠êÔ∏è View project at https://wandb.ai/lab_frohlich/pierre_mochi__fit_tmodel_3state_doubledeepms
wandb: üöÄ View run at https://wandb.ai/lab_frohlich/pierre_mochi__fit_tmodel_3state_doubledeepms/runs/fmv2ud8f
epoch done with 0.369
epoch done with 0.368
epoch done with 0.366
epoch done with 0.365
epoch done with 0.363
Grid search using {'num_samples': 128, 'learning_rate': 0.0001, 'l1_regularization_factor': 0.1, 'l2_regularization_factor': 0.001, 'number_additive_traits': 1, 'model_type': 'two_state_non_equilibrium', 'specs': (False, False)}
epoch done with 1.157
epoch done with 1.118
epoch done with 1.081
epoch done with 1.047
epoch done with 1.014
epoch done with 0.983
epoch done with 0.954
epoch done with 0.925
epoch done with 0.898
epoch done with 0.873
epoch done with 0.848
epoch done with 0.824
epoch done with 0.802
epoch done with 0.780
epoch done with 0.759
epoch done with 0.739
epoch done with 0.719
epoch done with 0.700
epoch done with 0.682
epoch done with 0.665
epoch done with 0.648
epoch done with 0.631
epoch done with 0.616
epoch done with 0.600
epoch done with 0.586
epoch done with 0.572
epoch done with 0.558
epoch done with 0.546
epoch done with 0.534
epoch done with 0.522
epoch done with 0.512
epoch done with 0.502
epoch done with 0.493
epoch done with 0.484
epoch done with 0.476
epoch done with 0.469
epoch done with 0.462
epoch done with 0.456
epoch done with 0.450
epoch done with 0.444
epoch done with 0.439
epoch done with 0.434
epoch done with 0.430
epoch done with 0.425
epoch done with 0.421
epoch done with 0.417
epoch done with 0.414
epoch done with 0.410
epoch done with 0.407
epoch done with 0.404
epoch done with 0.401
epoch done with 0.398
epoch done with 0.396
epoch done with 0.393
epoch done with 0.391
epoch done with 0.389
epoch done with 0.387
epoch done with 0.385
epoch done with 0.383
epoch done with 0.381
epoch done with 0.379
epoch done with 0.378
epoch done with 0.376
epoch done with 0.375
epoch done with 0.373
epoch done with 0.372
epoch done with 0.370
epoch done with 0.369
epoch done with 0.368
epoch done with 0.366
epoch done with 0.365
epoch done with 0.364
epoch done with 0.362
epoch done with 0.361
epoch done with 0.360
Grid search using {'num_samples': 128, 'learning_rate': 0.0001, 'l1_regularization_factor': 0.1, 'l2_regularization_factor': 0.01, 'number_additive_traits': 1, 'model_type': 'two_state_non_equilibrium', 'specs': (False, False)}
epoch done with 0.841
epoch done with 0.821
epoch done with 0.802
epoch done with 0.783
epoch done with 0.764
epoch done with 0.746
epoch done with 0.729
epoch done with 0.712
epoch done with 0.695
epoch done with 0.679
epoch done with 0.664
epoch done with 0.648
epoch done with 0.634
epoch done with 0.619
epoch done with 0.606
epoch done with 0.592
epoch done with 0.580
epoch done with 0.567
epoch done with 0.556
epoch done with 0.545
epoch done with 0.534
epoch done with 0.525
epoch done with 0.516
epoch done with 0.508
epoch done with 0.500
epoch done with 0.493
epoch done with 0.485
epoch done with 0.479
epoch done with 0.473
epoch done with 0.467
epoch done with 0.461
epoch done with 0.456
epoch done with 0.451
epoch done with 0.446
epoch done with 0.442
epoch done with 0.438
epoch done with 0.434
epoch done with 0.430
epoch done with 0.426
epoch done with 0.422
epoch done with 0.419
epoch done with 0.416
epoch done with 0.413
epoch done with 0.410
epoch done with 0.408
epoch done with 0.405
epoch done with 0.402
epoch done with 0.400
epoch done with 0.398
epoch done with 0.396
epoch done with 0.393
epoch done with 0.391
epoch done with 0.390
epoch done with 0.388
epoch done with 0.386
epoch done with 0.384
epoch done with 0.383
epoch done with 0.381
epoch done with 0.380
epoch done with 0.378
epoch done with 0.377
epoch done with 0.376
epoch done with 0.375
epoch done with 0.374
epoch done with 0.373
epoch done with 0.372
epoch done with 0.371
epoch done with 0.370
epoch done with 0.369
epoch done with 0.368
epoch done with 0.367
epoch done with 0.366
epoch done with 0.365
epoch done with 0.363
epoch done with 0.362
Grid search using {'num_samples': 128, 'learning_rate': 0.0001, 'l1_regularization_factor': 0.1, 'l2_regularization_factor': 0.1, 'number_additive_traits': 1, 'model_type': 'two_state_non_equilibrium', 'specs': (False, False)}
epoch done with 0.948
epoch done with 0.918
epoch done with 0.890
epoch done with 0.864
epoch done with 0.838
epoch done with 0.814
epoch done with 0.792
epoch done with 0.770
epoch done with 0.749
epoch done with 0.729
epoch done with 0.710
epoch done with 0.691
epoch done with 0.674
epoch done with 0.657
epoch done with 0.641
epoch done with 0.625
epoch done with 0.610
epoch done with 0.596
epoch done with 0.582
epoch done with 0.569
epoch done with 0.557
epoch done with 0.546
epoch done with 0.535
epoch done with 0.524
epoch done with 0.515
epoch done with 0.506
epoch done with 0.498
epoch done with 0.491
epoch done with 0.484
epoch done with 0.477
epoch done with 0.471
epoch done with 0.465
epoch done with 0.459
epoch done with 0.453
epoch done with 0.448
epoch done with 0.444
epoch done with 0.439
epoch done with 0.435
epoch done with 0.430
epoch done with 0.427
epoch done with 0.423
epoch done with 0.419
epoch done with 0.416
epoch done with 0.413
epoch done with 0.410
epoch done with 0.407
epoch done with 0.404
epoch done with 0.402
epoch done with 0.399
epoch done with 0.397
epoch done with 0.395
epoch done with 0.393
epoch done with 0.391
epoch done with 0.389
epoch done with 0.387
epoch done with 0.385
epoch done with 0.384
epoch done with 0.382
epoch done with 0.381
epoch done with 0.380
epoch done with 0.378
epoch done with 0.377
epoch done with 0.376
epoch done with 0.374
epoch done with 0.373
epoch done with 0.372
epoch done with 0.371
epoch done with 0.370
epoch done with 0.369
epoch done with 0.368
epoch done with 0.367
epoch done with 0.366
epoch done with 0.364
epoch done with 0.363
epoch done with 0.362
Grid search using {'num_samples': 128, 'learning_rate': 0.001, 'l1_regularization_factor': 0.0001, 'l2_regularization_factor': 0.0001, 'number_additive_traits': 1, 'model_type': 'two_state_non_equilibrium', 'specs': (False, False)}
epoch done with 0.946
epoch done with 0.757
epoch done with 0.612
epoch done with 0.502
epoch done with 0.439
epoch done with 0.408
epoch done with 0.389
epoch done with 0.376
epoch done with 0.366
epoch done with 0.358
epoch done with 0.343
epoch done with 0.323
epoch done with 0.300
epoch done with 0.274
epoch done with 0.247
epoch done with 0.225
epoch done with 0.206
epoch done with 0.192
epoch done with 0.179
epoch done with 0.169
epoch done with 0.160
epoch done with 0.153
epoch done with 0.147
epoch done with 0.144
epoch done with 0.141
epoch done with 0.140
epoch done with 0.138
epoch done with 0.137
epoch done with 0.136
epoch done with 0.134
epoch done with 0.134
epoch done with 0.133
epoch done with 0.133
epoch done with 0.132
epoch done with 0.132
epoch done with 0.131
epoch done with 0.131
epoch done with 0.131
epoch done with 0.131
epoch done with 0.131
epoch done with 0.131
epoch done with 0.130
epoch done with 0.131
epoch done with 0.130
epoch done with 0.130
epoch done with 0.130
epoch done with 0.131
epoch done with 0.130
epoch done with 0.130
epoch done with 0.130
epoch done with 0.130
epoch done with 0.130
epoch done with 0.130
epoch done with 0.130
epoch done with 0.130
epoch done with 0.130
epoch done with 0.130
epoch done with 0.130
epoch done with 0.130
epoch done with 0.130
epoch done with 0.131
epoch done with 0.130
epoch done with 0.130
epoch done with 0.131
epoch done with 0.131
epoch done with 0.131
epoch done with 0.131
epoch done with 0.131
epoch done with 0.131
epoch done with 0.131
epoch done with 0.131
epoch done with 0.131
epoch done with 0.131
epoch done with 0.131
epoch done with 0.131
Grid search using {'num_samples': 128, 'learning_rate': 0.001, 'l1_regularization_factor': 0.0001, 'l2_regularization_factor': 0.001, 'number_additive_traits': 1, 'model_type': 'two_state_non_equilibrium', 'specs': (False, False)}
epoch done with 0.640
epoch done with 0.523
epoch done with 0.462
epoch done with 0.427
epoch done with 0.405
epoch done with 0.391
epoch done with 0.381
epoch done with 0.375
epoch done with 0.371
epoch done with 0.367
epoch done with 0.354
epoch done with 0.333
epoch done with 0.308
epoch done with 0.279
epoch done with 0.250
/camp/home/demetzp/.conda/envs/demetzp/lib/python3.8/site-packages/haiku/_src/data_structures.py:143: FutureWarning: jax.tree_flatten is deprecated, and will be removed in a future release. Use jax.tree_util.tree_flatten instead.
  leaves, treedef = jax.tree_flatten(tree)
/camp/home/demetzp/.conda/envs/demetzp/lib/python3.8/site-packages/haiku/_src/data_structures.py:144: FutureWarning: jax.tree_unflatten is deprecated, and will be removed in a future release. Use jax.tree_util.tree_unflatten instead.
  return jax.tree_unflatten(treedef, leaves)
wandb: Waiting for W&B process to finish... (success).
wandb: - 0.005 MB of 0.015 MB uploaded (0.000 MB deduped)wandb: \ 0.005 MB of 0.017 MB uploaded (0.000 MB deduped)wandb: | 0.005 MB of 0.017 MB uploaded (0.000 MB deduped)wandb: 
wandb: Run history:
wandb: val_loss_fit ‚ñà‚ñÜ‚ñÖ‚ñÖ‚ñÑ‚ñÑ‚ñÑ‚ñÉ‚ñÇ‚ñÇ‚ñÇ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ
wandb: 
wandb: Run summary:
wandb: val_loss_fit 0.13
wandb: 
wandb: üöÄ View run Run 18 at: https://wandb.ai/lab_frohlich/pierre_mochi__fit_tmodel_3state_doubledeepms/runs/fmv2ud8f
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20230610_022541-fmv2ud8f/logs
wandb: - Waiting for wandb.init()...wandb: \ Waiting for wandb.init()...wandb: wandb version 0.15.4 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.15.3
wandb: Run data is saved locally in /nemo/lab/froehlichf/home/users/demetzp/PierreDeMetz/inst/python/wandb/run-20230610_022834-6o56ynme
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run Run 19
wandb: ‚≠êÔ∏è View project at https://wandb.ai/lab_frohlich/pierre_mochi__fit_tmodel_3state_doubledeepms
wandb: üöÄ View run at https://wandb.ai/lab_frohlich/pierre_mochi__fit_tmodel_3state_doubledeepms/runs/6o56ynme
/camp/home/demetzp/.conda/envs/demetzp/lib/python3.8/site-packages/haiku/_src/data_structures.py:143: FutureWarning: jax.tree_flatten is deprecated, and will be removed in a future release. Use jax.tree_util.tree_flatten instead.
  leaves, treedef = jax.tree_flatten(tree)
/camp/home/demetzp/.conda/envs/demetzp/lib/python3.8/site-packages/haiku/_src/data_structures.py:144: FutureWarning: jax.tree_unflatten is deprecated, and will be removed in a future release. Use jax.tree_util.tree_unflatten instead.
  return jax.tree_unflatten(treedef, leaves)
wandb: Waiting for W&B process to finish... (success).
wandb: - 0.005 MB of 0.005 MB uploaded (0.000 MB deduped)wandb: \ 0.005 MB of 0.017 MB uploaded (0.000 MB deduped)wandb: | 0.011 MB of 0.017 MB uploaded (0.000 MB deduped)wandb: / 0.011 MB of 0.017 MB uploaded (0.000 MB deduped)wandb: 
wandb: Run history:
wandb: val_loss_fit ‚ñà‚ñá‚ñÜ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÉ‚ñÇ‚ñÇ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ
wandb: 
wandb: Run summary:
wandb: val_loss_fit 0.131
wandb: 
wandb: üöÄ View run Run 19 at: https://wandb.ai/lab_frohlich/pierre_mochi__fit_tmodel_3state_doubledeepms/runs/6o56ynme
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20230610_022834-6o56ynme/logs
wandb: - Waiting for wandb.init()...wandb: \ Waiting for wandb.init()...wandb: wandb version 0.15.4 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.15.3
wandb: Run data is saved locally in /nemo/lab/froehlichf/home/users/demetzp/PierreDeMetz/inst/python/wandb/run-20230610_023126-t5sljtjp
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run Run 20
wandb: ‚≠êÔ∏è View project at https://wandb.ai/lab_frohlich/pierre_mochi__fit_tmodel_3state_doubledeepms
wandb: üöÄ View run at https://wandb.ai/lab_frohlich/pierre_mochi__fit_tmodel_3state_doubledeepms/runs/t5sljtjp
srun: Job step aborted: Waiting up to 32 seconds for job step to finish.
slurmstepd: error: *** JOB 52959905 ON ca193 CANCELLED AT 2023-06-10T02:34:07 DUE TO TIME LIMIT ***
slurmstepd: error: *** STEP 52959905.0 ON ca193 CANCELLED AT 2023-06-10T02:34:07 DUE TO TIME LIMIT ***
Will exit after finishing currently running jobs (scheduler).
Will exit after finishing currently running jobs (scheduler).
