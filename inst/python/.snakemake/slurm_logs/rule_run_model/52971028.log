Building DAG of jobs...
Using shell: /usr/bin/bash
Provided cores: 8
Rules claiming more threads will be scaled down.
Provided resources: mem_mb=60000, mem_mib=57221, disk_mb=1000, disk_mib=954
Select jobs to execute...
Building DAG of jobs...
Using shell: /usr/bin/bash
Provided cores: 8
Rules claiming more threads will be scaled down.
Provided resources: mem_mb=60000, mem_mib=57221, disk_mb=1000, disk_mib=954
Select jobs to execute...

[Sat Jun 10 17:49:05 2023]
rule run_model:
    output: Res/PSD95-PDZ3/mochi__fit_tmodel_3state_sparse_dimsum128_subsample100p/tri_state_equilibrium/model_performance_perepoch.pdf, Res/PSD95-PDZ3/mochi__fit_tmodel_3state_sparse_dimsum128_subsample100p/tri_state_equilibrium/predicted_fitness.txt, Res/PSD95-PDZ3/mochi__fit_tmodel_3state_sparse_dimsum128_subsample100p/tri_state_equilibrium/model_weights.txt, Res/PSD95-PDZ3/mochi__fit_tmodel_3state_sparse_dimsum128_subsample100p/tri_state_equilibrium/model_parameters.txt
    jobid: 0
    reason: Missing output files: Res/PSD95-PDZ3/mochi__fit_tmodel_3state_sparse_dimsum128_subsample100p/tri_state_equilibrium/model_performance_perepoch.pdf, Res/PSD95-PDZ3/mochi__fit_tmodel_3state_sparse_dimsum128_subsample100p/tri_state_equilibrium/model_parameters.txt, Res/PSD95-PDZ3/mochi__fit_tmodel_3state_sparse_dimsum128_subsample100p/tri_state_equilibrium/model_weights.txt, Res/PSD95-PDZ3/mochi__fit_tmodel_3state_sparse_dimsum128_subsample100p/tri_state_equilibrium/predicted_fitness.txt
    wildcards: protein=PSD95-PDZ3, dataset=mochi__fit_tmodel_3state_sparse_dimsum128_subsample100p, model_type=tri_state_equilibrium
    resources: mem_mb=60000, mem_mib=57221, disk_mb=1000, disk_mib=954, tmpdir=/tmp/slurm_52971028, slurm_account=u_froehlichf, slurm_partition=cpu

/camp/home/demetzp/.conda/envs/demetzp/lib/python3.8/site-packages/haiku/_src/data_structures.py:37: FutureWarning: jax.tree_structure is deprecated, and will be removed in a future release. Use jax.tree_util.tree_structure instead.
  PyTreeDef = type(jax.tree_structure(None))
No GPU/TPU found, falling back to CPU. (Set TF_CPP_MIN_LOG_LEVEL=0 and rerun for more info.)
wandb: Currently logged in as: demetz-pierre (lab_frohlich). Use `wandb login --relogin` to force relogin
wandb: - Waiting for wandb.init()...wandb: \ Waiting for wandb.init()...wandb: | Waiting for wandb.init()...wandb: wandb version 0.15.4 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.15.3
wandb: Run data is saved locally in /nemo/lab/froehlichf/home/users/demetzp/PierreDeMetz/inst/python/wandb/run-20230610_180128-78p0aeg2
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run Run 1
wandb: ⭐️ View project at https://wandb.ai/lab_frohlich/pierre_mochi__fit_tmodel_3state_doubledeepms
wandb: 🚀 View run at https://wandb.ai/lab_frohlich/pierre_mochi__fit_tmodel_3state_doubledeepms/runs/78p0aeg2
2023-06-10 18:02:48.268456: E external/org_tensorflow/tensorflow/compiler/xla/python/pjit.cc:461] fastpath_data is none
2023-06-10 18:03:09.212934: E external/org_tensorflow/tensorflow/compiler/xla/python/pjit.cc:461] fastpath_data is none
/camp/home/demetzp/.conda/envs/demetzp/lib/python3.8/site-packages/haiku/_src/data_structures.py:143: FutureWarning: jax.tree_flatten is deprecated, and will be removed in a future release. Use jax.tree_util.tree_flatten instead.
  leaves, treedef = jax.tree_flatten(tree)
/camp/home/demetzp/.conda/envs/demetzp/lib/python3.8/site-packages/haiku/_src/data_structures.py:144: FutureWarning: jax.tree_unflatten is deprecated, and will be removed in a future release. Use jax.tree_util.tree_unflatten instead.
  return jax.tree_unflatten(treedef, leaves)
/camp/home/demetzp/.conda/envs/demetzp/lib/python3.8/site-packages/haiku/_src/data_structures.py:143: FutureWarning: jax.tree_flatten is deprecated, and will be removed in a future release. Use jax.tree_util.tree_flatten instead.
  leaves, treedef = jax.tree_flatten(tree)
/camp/home/demetzp/.conda/envs/demetzp/lib/python3.8/site-packages/haiku/_src/data_structures.py:144: FutureWarning: jax.tree_unflatten is deprecated, and will be removed in a future release. Use jax.tree_util.tree_unflatten instead.
  return jax.tree_unflatten(treedef, leaves)
wandb: Waiting for W&B process to finish... (success).
wandb: 
wandb: Run history:
wandb: val_loss_fit ██▇▇▆▆▆▅▅▄▄▄▄▃▃▃▃▃▂▂▂▂▂▂▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁
wandb: 
wandb: Run summary:
wandb: val_loss_fit 0.364
wandb: 
wandb: 🚀 View run Run 1 at: https://wandb.ai/lab_frohlich/pierre_mochi__fit_tmodel_3state_doubledeepms/runs/78p0aeg2
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20230610_180128-78p0aeg2/logs
wandb: - Waiting for wandb.init()...wandb: \ Waiting for wandb.init()...wandb: wandb version 0.15.4 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.15.3
wandb: Run data is saved locally in /nemo/lab/froehlichf/home/users/demetzp/PierreDeMetz/inst/python/wandb/run-20230610_180619-pbpnjdg1
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run Run 2
wandb: ⭐️ View project at https://wandb.ai/lab_frohlich/pierre_mochi__fit_tmodel_3state_doubledeepms
wandb: 🚀 View run at https://wandb.ai/lab_frohlich/pierre_mochi__fit_tmodel_3state_doubledeepms/runs/pbpnjdg1
/camp/home/demetzp/.conda/envs/demetzp/lib/python3.8/site-packages/haiku/_src/data_structures.py:143: FutureWarning: jax.tree_flatten is deprecated, and will be removed in a future release. Use jax.tree_util.tree_flatten instead.
  leaves, treedef = jax.tree_flatten(tree)
/camp/home/demetzp/.conda/envs/demetzp/lib/python3.8/site-packages/haiku/_src/data_structures.py:144: FutureWarning: jax.tree_unflatten is deprecated, and will be removed in a future release. Use jax.tree_util.tree_unflatten instead.
  return jax.tree_unflatten(treedef, leaves)
wandb: Waiting for W&B process to finish... (success).
wandb: - 0.005 MB of 0.005 MB uploaded (0.000 MB deduped)wandb: \ 0.005 MB of 0.005 MB uploaded (0.000 MB deduped)wandb: | 0.005 MB of 0.005 MB uploaded (0.000 MB deduped)wandb: / 0.005 MB of 0.005 MB uploaded (0.000 MB deduped)wandb: - 0.005 MB of 0.005 MB uploaded (0.000 MB deduped)wandb: \ 0.005 MB of 0.005 MB uploaded (0.000 MB deduped)wandb: | 0.005 MB of 0.005 MB uploaded (0.000 MB deduped)wandb: / 0.005 MB of 0.005 MB uploaded (0.000 MB deduped)wandb: - 0.005 MB of 0.005 MB uploaded (0.000 MB deduped)wandb: 
wandb: Run history:
wandb: val_loss_fit ██▇▇▆▆▅▅▅▄▄▄▄▃▃▃▃▃▂▂▂▂▂▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb: 
wandb: Run summary:
wandb: val_loss_fit 0.363
wandb: 
wandb: 🚀 View run Run 2 at: https://wandb.ai/lab_frohlich/pierre_mochi__fit_tmodel_3state_doubledeepms/runs/pbpnjdg1
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20230610_180619-pbpnjdg1/logs
wandb: - Waiting for wandb.init()...wandb: \ Waiting for wandb.init()...wandb: | Waiting for wandb.init()...wandb: wandb version 0.15.4 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.15.3
wandb: Run data is saved locally in /nemo/lab/froehlichf/home/users/demetzp/PierreDeMetz/inst/python/wandb/run-20230610_181005-uil5nr9z
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run Run 3
wandb: ⭐️ View project at https://wandb.ai/lab_frohlich/pierre_mochi__fit_tmodel_3state_doubledeepms
wandb: 🚀 View run at https://wandb.ai/lab_frohlich/pierre_mochi__fit_tmodel_3state_doubledeepms/runs/uil5nr9z
/camp/home/demetzp/.conda/envs/demetzp/lib/python3.8/site-packages/haiku/_src/data_structures.py:143: FutureWarning: jax.tree_flatten is deprecated, and will be removed in a future release. Use jax.tree_util.tree_flatten instead.
  leaves, treedef = jax.tree_flatten(tree)
/camp/home/demetzp/.conda/envs/demetzp/lib/python3.8/site-packages/haiku/_src/data_structures.py:144: FutureWarning: jax.tree_unflatten is deprecated, and will be removed in a future release. Use jax.tree_util.tree_unflatten instead.
  return jax.tree_unflatten(treedef, leaves)
wandb: Waiting for W&B process to finish... (success).
wandb: - 0.005 MB of 0.005 MB uploaded (0.000 MB deduped)wandb: \ 0.005 MB of 0.005 MB uploaded (0.000 MB deduped)wandb: | 0.005 MB of 0.005 MB uploaded (0.000 MB deduped)wandb: / 0.005 MB of 0.005 MB uploaded (0.000 MB deduped)wandb: - 0.005 MB of 0.005 MB uploaded (0.000 MB deduped)wandb: \ 0.005 MB of 0.005 MB uploaded (0.000 MB deduped)wandb: | 0.005 MB of 0.005 MB uploaded (0.000 MB deduped)wandb: 
wandb: Run history:
wandb: val_loss_fit ██▇▇▆▆▆▅▅▄▄▄▄▃▃▃▃▃▂▂▂▂▂▂▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁
wandb: 
wandb: Run summary:
wandb: val_loss_fit 0.364
wandb: 
wandb: 🚀 View run Run 3 at: https://wandb.ai/lab_frohlich/pierre_mochi__fit_tmodel_3state_doubledeepms/runs/uil5nr9z
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20230610_181005-uil5nr9z/logs
wandb: - Waiting for wandb.init()...wandb: \ Waiting for wandb.init()...wandb: wandb version 0.15.4 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.15.3
wandb: Run data is saved locally in /nemo/lab/froehlichf/home/users/demetzp/PierreDeMetz/inst/python/wandb/run-20230610_181356-6whaxol9
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run Run 4
wandb: ⭐️ View project at https://wandb.ai/lab_frohlich/pierre_mochi__fit_tmodel_3state_doubledeepms
wandb: 🚀 View run at https://wandb.ai/lab_frohlich/pierre_mochi__fit_tmodel_3state_doubledeepms/runs/6whaxol9
/camp/home/demetzp/.conda/envs/demetzp/lib/python3.8/site-packages/haiku/_src/data_structures.py:143: FutureWarning: jax.tree_flatten is deprecated, and will be removed in a future release. Use jax.tree_util.tree_flatten instead.
  leaves, treedef = jax.tree_flatten(tree)
/camp/home/demetzp/.conda/envs/demetzp/lib/python3.8/site-packages/haiku/_src/data_structures.py:144: FutureWarning: jax.tree_unflatten is deprecated, and will be removed in a future release. Use jax.tree_util.tree_unflatten instead.
  return jax.tree_unflatten(treedef, leaves)
wandb: Waiting for W&B process to finish... (success).
wandb: 
wandb: Run history:
wandb: val_loss_fit ██▇▇▆▆▆▅▅▄▄▄▄▃▃▃▃▃▂▂▂▂▂▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb: 
wandb: Run summary:
wandb: val_loss_fit 0.362
wandb: 
wandb: 🚀 View run Run 4 at: https://wandb.ai/lab_frohlich/pierre_mochi__fit_tmodel_3state_doubledeepms/runs/6whaxol9
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20230610_181356-6whaxol9/logs
wandb: - Waiting for wandb.init()...wandb: \ Waiting for wandb.init()...wandb: wandb version 0.15.4 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.15.3
wandb: Run data is saved locally in /nemo/lab/froehlichf/home/users/demetzp/PierreDeMetz/inst/python/wandb/run-20230610_181741-7rfk0w4a
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run Run 5
wandb: ⭐️ View project at https://wandb.ai/lab_frohlich/pierre_mochi__fit_tmodel_3state_doubledeepms
wandb: 🚀 View run at https://wandb.ai/lab_frohlich/pierre_mochi__fit_tmodel_3state_doubledeepms/runs/7rfk0w4a
Warning: Output model directory already exists.
Warning: Output plot directory already exists.
Warning: Output weights directory already exists.
Warning: Output boostrap directory already exists.
Grid search using {'num_samples': 128, 'learning_rate': 0.0001, 'l1_regularization_factor': 0.0001, 'l2_regularization_factor': 0.0001, 'number_additive_traits': 1, 'model_type': 'tri_state_equilibrium', 'specs': (False, False)}
epoch done with 0.802
epoch done with 0.787
epoch done with 0.772
epoch done with 0.758
epoch done with 0.743
epoch done with 0.729
epoch done with 0.715
epoch done with 0.701
epoch done with 0.688
epoch done with 0.674
epoch done with 0.661
epoch done with 0.649
epoch done with 0.636
epoch done with 0.624
epoch done with 0.612
epoch done with 0.600
epoch done with 0.589
epoch done with 0.578
epoch done with 0.567
epoch done with 0.557
epoch done with 0.547
epoch done with 0.537
epoch done with 0.528
epoch done with 0.520
epoch done with 0.512
epoch done with 0.505
epoch done with 0.497
epoch done with 0.490
epoch done with 0.484
epoch done with 0.478
epoch done with 0.472
epoch done with 0.466
epoch done with 0.461
epoch done with 0.456
epoch done with 0.451
epoch done with 0.446
epoch done with 0.442
epoch done with 0.438
epoch done with 0.434
epoch done with 0.430
epoch done with 0.426
epoch done with 0.423
epoch done with 0.420
epoch done with 0.417
epoch done with 0.414
epoch done with 0.411
epoch done with 0.408
epoch done with 0.405
epoch done with 0.403
epoch done with 0.400
epoch done with 0.398
epoch done with 0.396
epoch done with 0.393
epoch done with 0.391
epoch done with 0.389
epoch done with 0.387
epoch done with 0.385
epoch done with 0.384
epoch done with 0.382
epoch done with 0.380
epoch done with 0.379
epoch done with 0.377
epoch done with 0.376
epoch done with 0.375
epoch done with 0.373
epoch done with 0.372
epoch done with 0.371
epoch done with 0.370
epoch done with 0.369
epoch done with 0.368
epoch done with 0.367
epoch done with 0.366
epoch done with 0.366
epoch done with 0.365
epoch done with 0.364
Grid search using {'num_samples': 128, 'learning_rate': 0.0001, 'l1_regularization_factor': 0.0001, 'l2_regularization_factor': 0.001, 'number_additive_traits': 1, 'model_type': 'tri_state_equilibrium', 'specs': (False, False)}
epoch done with 0.815
epoch done with 0.798
epoch done with 0.782
epoch done with 0.766
epoch done with 0.751
epoch done with 0.735
epoch done with 0.720
epoch done with 0.706
epoch done with 0.691
epoch done with 0.677
epoch done with 0.664
epoch done with 0.650
epoch done with 0.637
epoch done with 0.625
epoch done with 0.612
epoch done with 0.600
epoch done with 0.589
epoch done with 0.578
epoch done with 0.567
epoch done with 0.556
epoch done with 0.546
epoch done with 0.537
epoch done with 0.528
epoch done with 0.519
epoch done with 0.511
epoch done with 0.504
epoch done with 0.497
epoch done with 0.490
epoch done with 0.483
epoch done with 0.477
epoch done with 0.471
epoch done with 0.466
epoch done with 0.460
epoch done with 0.455
epoch done with 0.450
epoch done with 0.445
epoch done with 0.441
epoch done with 0.437
epoch done with 0.433
epoch done with 0.428
epoch done with 0.425
epoch done with 0.421
epoch done with 0.418
epoch done with 0.415
epoch done with 0.412
epoch done with 0.409
epoch done with 0.406
epoch done with 0.403
epoch done with 0.401
epoch done with 0.398
epoch done with 0.396
epoch done with 0.394
epoch done with 0.392
epoch done with 0.390
epoch done with 0.388
epoch done with 0.386
epoch done with 0.385
epoch done with 0.383
epoch done with 0.382
epoch done with 0.380
epoch done with 0.379
epoch done with 0.377
epoch done with 0.376
epoch done with 0.375
epoch done with 0.374
epoch done with 0.372
epoch done with 0.371
epoch done with 0.370
epoch done with 0.369
epoch done with 0.368
epoch done with 0.367
epoch done with 0.366
epoch done with 0.365
epoch done with 0.364
epoch done with 0.363
Grid search using {'num_samples': 128, 'learning_rate': 0.0001, 'l1_regularization_factor': 0.0001, 'l2_regularization_factor': 0.01, 'number_additive_traits': 1, 'model_type': 'tri_state_equilibrium', 'specs': (False, False)}
epoch done with 0.798
epoch done with 0.783
epoch done with 0.769
epoch done with 0.754
epoch done with 0.740
epoch done with 0.726
epoch done with 0.712
epoch done with 0.698
epoch done with 0.684
epoch done with 0.671
epoch done with 0.658
epoch done with 0.645
epoch done with 0.633
epoch done with 0.621
epoch done with 0.609
epoch done with 0.597
epoch done with 0.586
epoch done with 0.575
epoch done with 0.565
epoch done with 0.555
epoch done with 0.545
epoch done with 0.536
epoch done with 0.527
epoch done with 0.518
epoch done with 0.511
epoch done with 0.503
epoch done with 0.496
epoch done with 0.489
epoch done with 0.483
epoch done with 0.477
epoch done with 0.471
epoch done with 0.466
epoch done with 0.460
epoch done with 0.455
epoch done with 0.451
epoch done with 0.446
epoch done with 0.441
epoch done with 0.437
epoch done with 0.433
epoch done with 0.430
epoch done with 0.426
epoch done with 0.423
epoch done with 0.419
epoch done with 0.416
epoch done with 0.413
epoch done with 0.410
epoch done with 0.408
epoch done with 0.405
epoch done with 0.402
epoch done with 0.400
epoch done with 0.398
epoch done with 0.395
epoch done with 0.393
epoch done with 0.391
epoch done with 0.389
epoch done with 0.387
epoch done with 0.385
epoch done with 0.384
epoch done with 0.382
epoch done with 0.380
epoch done with 0.379
epoch done with 0.377
epoch done with 0.376
epoch done with 0.375
epoch done with 0.373
epoch done with 0.372
epoch done with 0.371
epoch done with 0.370
epoch done with 0.369
epoch done with 0.368
epoch done with 0.367
epoch done with 0.366
epoch done with 0.366
epoch done with 0.365
epoch done with 0.364
Grid search using {'num_samples': 128, 'learning_rate': 0.0001, 'l1_regularization_factor': 0.0001, 'l2_regularization_factor': 0.1, 'number_additive_traits': 1, 'model_type': 'tri_state_equilibrium', 'specs': (False, False)}
epoch done with 0.865
epoch done with 0.847
epoch done with 0.830
epoch done with 0.813
epoch done with 0.796
epoch done with 0.780
epoch done with 0.764
epoch done with 0.748
epoch done with 0.732
epoch done with 0.717
epoch done with 0.702
epoch done with 0.688
epoch done with 0.673
epoch done with 0.660
epoch done with 0.646
epoch done with 0.633
epoch done with 0.620
epoch done with 0.607
epoch done with 0.595
epoch done with 0.583
epoch done with 0.572
epoch done with 0.561
epoch done with 0.550
epoch done with 0.540
epoch done with 0.530
epoch done with 0.521
epoch done with 0.512
epoch done with 0.505
epoch done with 0.497
epoch done with 0.490
epoch done with 0.483
epoch done with 0.477
epoch done with 0.471
epoch done with 0.465
epoch done with 0.459
epoch done with 0.454
epoch done with 0.449
epoch done with 0.444
epoch done with 0.439
epoch done with 0.435
epoch done with 0.431
epoch done with 0.426
epoch done with 0.423
epoch done with 0.419
epoch done with 0.416
epoch done with 0.412
epoch done with 0.409
epoch done with 0.406
epoch done with 0.403
epoch done with 0.401
epoch done with 0.398
epoch done with 0.396
epoch done with 0.394
epoch done with 0.392
epoch done with 0.390
epoch done with 0.388
epoch done with 0.386
epoch done with 0.384
epoch done with 0.383
epoch done with 0.381
epoch done with 0.379
epoch done with 0.378
epoch done with 0.376
epoch done with 0.375
epoch done with 0.374
epoch done with 0.372
epoch done with 0.371
epoch done with 0.370
epoch done with 0.369
epoch done with 0.368
epoch done with 0.367
epoch done with 0.365
epoch done with 0.364
epoch done with 0.363
epoch done with 0.362
Grid search using {'num_samples': 128, 'learning_rate': 0.0001, 'l1_regularization_factor': 0.001, 'l2_regularization_factor': 0.0001, 'number_additive_traits': 1, 'model_type': 'tri_state_equilibrium', 'specs': (False, False)}
epoch done with 0.954
epoch done with 0.938
epoch done with 0.921
epoch done with 0.905
epoch done with 0.888
epoch done with 0.872
epoch done with 0.856
epoch done with 0.841
epoch done with 0.825
epoch done with 0.810
epoch done with 0.795
/camp/home/demetzp/.conda/envs/demetzp/lib/python3.8/site-packages/haiku/_src/data_structures.py:143: FutureWarning: jax.tree_flatten is deprecated, and will be removed in a future release. Use jax.tree_util.tree_flatten instead.
  leaves, treedef = jax.tree_flatten(tree)
/camp/home/demetzp/.conda/envs/demetzp/lib/python3.8/site-packages/haiku/_src/data_structures.py:144: FutureWarning: jax.tree_unflatten is deprecated, and will be removed in a future release. Use jax.tree_util.tree_unflatten instead.
  return jax.tree_unflatten(treedef, leaves)
wandb: Waiting for W&B process to finish... (success).
wandb: 
wandb: Run history:
wandb: val_loss_fit ██▇▇▇▆▆▆▅▅▅▄▄▄▄▃▃▃▃▂▂▂▂▂▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁
wandb: 
wandb: Run summary:
wandb: val_loss_fit 0.365
wandb: 
wandb: 🚀 View run Run 5 at: https://wandb.ai/lab_frohlich/pierre_mochi__fit_tmodel_3state_doubledeepms/runs/7rfk0w4a
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20230610_181741-7rfk0w4a/logs
wandb: - Waiting for wandb.init()...wandb: \ Waiting for wandb.init()...wandb: wandb version 0.15.4 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.15.3
wandb: Run data is saved locally in /nemo/lab/froehlichf/home/users/demetzp/PierreDeMetz/inst/python/wandb/run-20230610_182123-0e0pskql
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run Run 6
wandb: ⭐️ View project at https://wandb.ai/lab_frohlich/pierre_mochi__fit_tmodel_3state_doubledeepms
wandb: 🚀 View run at https://wandb.ai/lab_frohlich/pierre_mochi__fit_tmodel_3state_doubledeepms/runs/0e0pskql
/camp/home/demetzp/.conda/envs/demetzp/lib/python3.8/site-packages/haiku/_src/data_structures.py:143: FutureWarning: jax.tree_flatten is deprecated, and will be removed in a future release. Use jax.tree_util.tree_flatten instead.
  leaves, treedef = jax.tree_flatten(tree)
/camp/home/demetzp/.conda/envs/demetzp/lib/python3.8/site-packages/haiku/_src/data_structures.py:144: FutureWarning: jax.tree_unflatten is deprecated, and will be removed in a future release. Use jax.tree_util.tree_unflatten instead.
  return jax.tree_unflatten(treedef, leaves)
wandb: Waiting for W&B process to finish... (success).
wandb: - 0.005 MB of 0.005 MB uploaded (0.000 MB deduped)wandb: \ 0.005 MB of 0.005 MB uploaded (0.000 MB deduped)wandb: | 0.005 MB of 0.005 MB uploaded (0.000 MB deduped)wandb: / 0.005 MB of 0.005 MB uploaded (0.000 MB deduped)wandb: - 0.005 MB of 0.005 MB uploaded (0.000 MB deduped)wandb: \ 0.005 MB of 0.005 MB uploaded (0.000 MB deduped)wandb: | 0.005 MB of 0.005 MB uploaded (0.000 MB deduped)wandb: / 0.005 MB of 0.005 MB uploaded (0.000 MB deduped)wandb: 
wandb: Run history:
wandb: val_loss_fit ██▇▇▆▆▆▅▅▄▄▄▄▃▃▃▃▃▂▂▂▂▂▂▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁
wandb: 
wandb: Run summary:
wandb: val_loss_fit 0.364
wandb: 
wandb: 🚀 View run Run 6 at: https://wandb.ai/lab_frohlich/pierre_mochi__fit_tmodel_3state_doubledeepms/runs/0e0pskql
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20230610_182123-0e0pskql/logs
wandb: wandb version 0.15.4 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.15.3
wandb: Run data is saved locally in /nemo/lab/froehlichf/home/users/demetzp/PierreDeMetz/inst/python/wandb/run-20230610_182513-e2wklflk
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run Run 7
wandb: ⭐️ View project at https://wandb.ai/lab_frohlich/pierre_mochi__fit_tmodel_3state_doubledeepms
wandb: 🚀 View run at https://wandb.ai/lab_frohlich/pierre_mochi__fit_tmodel_3state_doubledeepms/runs/e2wklflk
/camp/home/demetzp/.conda/envs/demetzp/lib/python3.8/site-packages/haiku/_src/data_structures.py:143: FutureWarning: jax.tree_flatten is deprecated, and will be removed in a future release. Use jax.tree_util.tree_flatten instead.
  leaves, treedef = jax.tree_flatten(tree)
/camp/home/demetzp/.conda/envs/demetzp/lib/python3.8/site-packages/haiku/_src/data_structures.py:144: FutureWarning: jax.tree_unflatten is deprecated, and will be removed in a future release. Use jax.tree_util.tree_unflatten instead.
  return jax.tree_unflatten(treedef, leaves)
wandb: Waiting for W&B process to finish... (success).
wandb: - 0.005 MB of 0.005 MB uploaded (0.000 MB deduped)wandb: \ 0.005 MB of 0.005 MB uploaded (0.000 MB deduped)wandb: | 0.005 MB of 0.005 MB uploaded (0.000 MB deduped)wandb: / 0.005 MB of 0.005 MB uploaded (0.000 MB deduped)wandb: - 0.005 MB of 0.005 MB uploaded (0.000 MB deduped)wandb: \ 0.005 MB of 0.005 MB uploaded (0.000 MB deduped)wandb: | 0.005 MB of 0.005 MB uploaded (0.000 MB deduped)wandb: / 0.005 MB of 0.005 MB uploaded (0.000 MB deduped)wandb: - 0.005 MB of 0.005 MB uploaded (0.000 MB deduped)wandb: 
wandb: Run history:
wandb: val_loss_fit ██▇▇▆▆▆▅▅▄▄▄▄▃▃▃▃▃▂▂▂▂▂▂▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁
wandb: 
wandb: Run summary:
wandb: val_loss_fit 0.364
wandb: 
wandb: 🚀 View run Run 7 at: https://wandb.ai/lab_frohlich/pierre_mochi__fit_tmodel_3state_doubledeepms/runs/e2wklflk
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20230610_182513-e2wklflk/logs
wandb: - Waiting for wandb.init()...wandb: \ Waiting for wandb.init()...wandb: wandb version 0.15.4 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.15.3
wandb: Run data is saved locally in /nemo/lab/froehlichf/home/users/demetzp/PierreDeMetz/inst/python/wandb/run-20230610_182900-15tlnx0f
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run Run 8
wandb: ⭐️ View project at https://wandb.ai/lab_frohlich/pierre_mochi__fit_tmodel_3state_doubledeepms
wandb: 🚀 View run at https://wandb.ai/lab_frohlich/pierre_mochi__fit_tmodel_3state_doubledeepms/runs/15tlnx0f
/camp/home/demetzp/.conda/envs/demetzp/lib/python3.8/site-packages/haiku/_src/data_structures.py:143: FutureWarning: jax.tree_flatten is deprecated, and will be removed in a future release. Use jax.tree_util.tree_flatten instead.
  leaves, treedef = jax.tree_flatten(tree)
/camp/home/demetzp/.conda/envs/demetzp/lib/python3.8/site-packages/haiku/_src/data_structures.py:144: FutureWarning: jax.tree_unflatten is deprecated, and will be removed in a future release. Use jax.tree_util.tree_unflatten instead.
  return jax.tree_unflatten(treedef, leaves)
wandb: Waiting for W&B process to finish... (success).
wandb: - 0.005 MB of 0.005 MB uploaded (0.000 MB deduped)wandb: \ 0.005 MB of 0.005 MB uploaded (0.000 MB deduped)wandb: | 0.005 MB of 0.005 MB uploaded (0.000 MB deduped)wandb: / 0.005 MB of 0.005 MB uploaded (0.000 MB deduped)wandb: - 0.005 MB of 0.005 MB uploaded (0.000 MB deduped)wandb: \ 0.005 MB of 0.005 MB uploaded (0.000 MB deduped)wandb: | 0.005 MB of 0.005 MB uploaded (0.000 MB deduped)wandb: / 0.005 MB of 0.005 MB uploaded (0.000 MB deduped)wandb: - 0.005 MB of 0.005 MB uploaded (0.000 MB deduped)wandb: 
wandb: Run history:
wandb: val_loss_fit ██▇▇▆▆▆▅▅▅▄▄▄▄▃▃▃▃▃▂▂▂▂▂▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁
wandb: 
wandb: Run summary:
wandb: val_loss_fit 0.367
wandb: 
wandb: 🚀 View run Run 8 at: https://wandb.ai/lab_frohlich/pierre_mochi__fit_tmodel_3state_doubledeepms/runs/15tlnx0f
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20230610_182900-15tlnx0f/logs
wandb: - Waiting for wandb.init()...wandb: \ Waiting for wandb.init()...wandb: | Waiting for wandb.init()...wandb: / Waiting for wandb.init()...wandb: wandb version 0.15.4 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.15.3
wandb: Run data is saved locally in /nemo/lab/froehlichf/home/users/demetzp/PierreDeMetz/inst/python/wandb/run-20230610_183245-ptodvn5l
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run Run 9
wandb: ⭐️ View project at https://wandb.ai/lab_frohlich/pierre_mochi__fit_tmodel_3state_doubledeepms
wandb: 🚀 View run at https://wandb.ai/lab_frohlich/pierre_mochi__fit_tmodel_3state_doubledeepms/runs/ptodvn5l
epoch done with 0.781
epoch done with 0.766
epoch done with 0.752
epoch done with 0.738
epoch done with 0.724
epoch done with 0.711
epoch done with 0.697
epoch done with 0.684
epoch done with 0.671
epoch done with 0.658
epoch done with 0.645
epoch done with 0.633
epoch done with 0.621
epoch done with 0.609
epoch done with 0.597
epoch done with 0.585
epoch done with 0.574
epoch done with 0.563
epoch done with 0.552
epoch done with 0.542
epoch done with 0.532
epoch done with 0.522
epoch done with 0.513
epoch done with 0.504
epoch done with 0.495
epoch done with 0.488
epoch done with 0.480
epoch done with 0.473
epoch done with 0.467
epoch done with 0.460
epoch done with 0.455
epoch done with 0.450
epoch done with 0.445
epoch done with 0.440
epoch done with 0.436
epoch done with 0.431
epoch done with 0.427
epoch done with 0.423
epoch done with 0.419
epoch done with 0.416
epoch done with 0.413
epoch done with 0.409
epoch done with 0.406
epoch done with 0.403
epoch done with 0.401
epoch done with 0.398
epoch done with 0.395
epoch done with 0.393
epoch done with 0.390
epoch done with 0.388
epoch done with 0.386
epoch done with 0.384
epoch done with 0.382
epoch done with 0.380
epoch done with 0.378
epoch done with 0.376
epoch done with 0.375
epoch done with 0.373
epoch done with 0.372
epoch done with 0.370
epoch done with 0.369
epoch done with 0.367
epoch done with 0.366
epoch done with 0.365
Grid search using {'num_samples': 128, 'learning_rate': 0.0001, 'l1_regularization_factor': 0.001, 'l2_regularization_factor': 0.001, 'number_additive_traits': 1, 'model_type': 'tri_state_equilibrium', 'specs': (False, False)}
epoch done with 0.799
epoch done with 0.784
epoch done with 0.769
epoch done with 0.754
epoch done with 0.740
epoch done with 0.726
epoch done with 0.712
epoch done with 0.698
epoch done with 0.685
epoch done with 0.671
epoch done with 0.658
epoch done with 0.646
epoch done with 0.633
epoch done with 0.621
epoch done with 0.609
epoch done with 0.598
epoch done with 0.586
epoch done with 0.576
epoch done with 0.565
epoch done with 0.555
epoch done with 0.545
epoch done with 0.536
epoch done with 0.527
epoch done with 0.519
epoch done with 0.511
epoch done with 0.504
epoch done with 0.497
epoch done with 0.490
epoch done with 0.483
epoch done with 0.477
epoch done with 0.471
epoch done with 0.466
epoch done with 0.461
epoch done with 0.456
epoch done with 0.451
epoch done with 0.446
epoch done with 0.442
epoch done with 0.438
epoch done with 0.434
epoch done with 0.430
epoch done with 0.426
epoch done with 0.423
epoch done with 0.420
epoch done with 0.417
epoch done with 0.414
epoch done with 0.411
epoch done with 0.408
epoch done with 0.405
epoch done with 0.403
epoch done with 0.400
epoch done with 0.398
epoch done with 0.396
epoch done with 0.394
epoch done with 0.392
epoch done with 0.389
epoch done with 0.388
epoch done with 0.386
epoch done with 0.384
epoch done with 0.382
epoch done with 0.380
epoch done with 0.379
epoch done with 0.377
epoch done with 0.376
epoch done with 0.375
epoch done with 0.374
epoch done with 0.373
epoch done with 0.371
epoch done with 0.370
epoch done with 0.369
epoch done with 0.368
epoch done with 0.368
epoch done with 0.367
epoch done with 0.366
epoch done with 0.365
epoch done with 0.364
Grid search using {'num_samples': 128, 'learning_rate': 0.0001, 'l1_regularization_factor': 0.001, 'l2_regularization_factor': 0.01, 'number_additive_traits': 1, 'model_type': 'tri_state_equilibrium', 'specs': (False, False)}
epoch done with 0.813
epoch done with 0.798
epoch done with 0.783
epoch done with 0.769
epoch done with 0.754
epoch done with 0.740
epoch done with 0.726
epoch done with 0.712
epoch done with 0.698
epoch done with 0.685
epoch done with 0.671
epoch done with 0.658
epoch done with 0.646
epoch done with 0.633
epoch done with 0.621
epoch done with 0.609
epoch done with 0.598
epoch done with 0.586
epoch done with 0.575
epoch done with 0.565
epoch done with 0.555
epoch done with 0.545
epoch done with 0.535
epoch done with 0.526
epoch done with 0.518
epoch done with 0.510
epoch done with 0.503
epoch done with 0.495
epoch done with 0.489
epoch done with 0.482
epoch done with 0.476
epoch done with 0.470
epoch done with 0.465
epoch done with 0.459
epoch done with 0.454
epoch done with 0.449
epoch done with 0.445
epoch done with 0.440
epoch done with 0.436
epoch done with 0.432
epoch done with 0.429
epoch done with 0.425
epoch done with 0.422
epoch done with 0.419
epoch done with 0.415
epoch done with 0.412
epoch done with 0.410
epoch done with 0.407
epoch done with 0.404
epoch done with 0.402
epoch done with 0.399
epoch done with 0.397
epoch done with 0.395
epoch done with 0.392
epoch done with 0.390
epoch done with 0.388
epoch done with 0.386
epoch done with 0.384
epoch done with 0.383
epoch done with 0.381
epoch done with 0.379
epoch done with 0.378
epoch done with 0.376
epoch done with 0.375
epoch done with 0.374
epoch done with 0.372
epoch done with 0.371
epoch done with 0.370
epoch done with 0.369
epoch done with 0.368
epoch done with 0.367
epoch done with 0.366
epoch done with 0.365
epoch done with 0.364
epoch done with 0.364
Grid search using {'num_samples': 128, 'learning_rate': 0.0001, 'l1_regularization_factor': 0.001, 'l2_regularization_factor': 0.1, 'number_additive_traits': 1, 'model_type': 'tri_state_equilibrium', 'specs': (False, False)}
epoch done with 0.624
epoch done with 0.615
epoch done with 0.606
epoch done with 0.597
epoch done with 0.588
epoch done with 0.579
epoch done with 0.571
epoch done with 0.563
epoch done with 0.555
epoch done with 0.547
epoch done with 0.540
epoch done with 0.533
epoch done with 0.527
epoch done with 0.520
epoch done with 0.514
epoch done with 0.508
epoch done with 0.502
epoch done with 0.496
epoch done with 0.491
epoch done with 0.486
epoch done with 0.480
epoch done with 0.476
epoch done with 0.471
epoch done with 0.466
epoch done with 0.462
epoch done with 0.457
epoch done with 0.453
epoch done with 0.449
epoch done with 0.445
epoch done with 0.442
epoch done with 0.438
epoch done with 0.435
epoch done with 0.431
epoch done with 0.428
epoch done with 0.425
epoch done with 0.422
epoch done with 0.419
epoch done with 0.416
epoch done with 0.414
epoch done with 0.411
epoch done with 0.409
epoch done with 0.406
epoch done with 0.404
epoch done with 0.402
epoch done with 0.400
epoch done with 0.398
epoch done with 0.396
epoch done with 0.394
epoch done with 0.392
epoch done with 0.391
epoch done with 0.389
epoch done with 0.388
epoch done with 0.386
epoch done with 0.385
epoch done with 0.384
epoch done with 0.382
epoch done with 0.381
epoch done with 0.380
epoch done with 0.379
epoch done with 0.378
epoch done with 0.377
epoch done with 0.376
epoch done with 0.375
epoch done with 0.374
epoch done with 0.373
epoch done with 0.373
epoch done with 0.372
epoch done with 0.371
epoch done with 0.371
epoch done with 0.370
epoch done with 0.369
epoch done with 0.369
epoch done with 0.368
epoch done with 0.368
epoch done with 0.367
Grid search using {'num_samples': 128, 'learning_rate': 0.0001, 'l1_regularization_factor': 0.01, 'l2_regularization_factor': 0.0001, 'number_additive_traits': 1, 'model_type': 'tri_state_equilibrium', 'specs': (False, False)}
epoch done with 0.684
epoch done with 0.670
epoch done with 0.657
epoch done with 0.645
epoch done with 0.632
epoch done with 0.620
epoch done with 0.609
epoch done with 0.597
epoch done with 0.587
epoch done with 0.577
epoch done with 0.567
epoch done with 0.558
epoch done with 0.549
epoch done with 0.541
epoch done with 0.533
epoch done with 0.525
epoch done with 0.517
epoch done with 0.510
epoch done with 0.504
epoch done with 0.497
epoch done with 0.491
epoch done with 0.485
epoch done with 0.479
epoch done with 0.474
epoch done with 0.469
epoch done with 0.464
epoch done with 0.459
epoch done with 0.454
epoch done with 0.450
epoch done with 0.446
epoch done with 0.442
epoch done with 0.438
epoch done with 0.434
epoch done with 0.430
epoch done with 0.427
epoch done with 0.424
epoch done with 0.421
epoch done with 0.418
epoch done with 0.415
epoch done with 0.412
epoch done with 0.410
epoch done with 0.408
/camp/home/demetzp/.conda/envs/demetzp/lib/python3.8/site-packages/haiku/_src/data_structures.py:143: FutureWarning: jax.tree_flatten is deprecated, and will be removed in a future release. Use jax.tree_util.tree_flatten instead.
  leaves, treedef = jax.tree_flatten(tree)
/camp/home/demetzp/.conda/envs/demetzp/lib/python3.8/site-packages/haiku/_src/data_structures.py:144: FutureWarning: jax.tree_unflatten is deprecated, and will be removed in a future release. Use jax.tree_util.tree_unflatten instead.
  return jax.tree_unflatten(treedef, leaves)
wandb: Waiting for W&B process to finish... (success).
wandb: - 0.005 MB of 0.005 MB uploaded (0.000 MB deduped)wandb: \ 0.005 MB of 0.005 MB uploaded (0.000 MB deduped)wandb: | 0.005 MB of 0.005 MB uploaded (0.000 MB deduped)wandb: / 0.005 MB of 0.005 MB uploaded (0.000 MB deduped)wandb: - 0.005 MB of 0.005 MB uploaded (0.000 MB deduped)wandb: \ 0.005 MB of 0.005 MB uploaded (0.000 MB deduped)wandb: | 0.005 MB of 0.005 MB uploaded (0.000 MB deduped)wandb: / 0.005 MB of 0.005 MB uploaded (0.000 MB deduped)wandb: - 0.005 MB of 0.005 MB uploaded (0.000 MB deduped)wandb: 
wandb: Run history:
wandb: val_loss_fit ██▇▇▆▆▅▅▄▄▄▄▃▃▃▃▃▂▂▂▂▂▂▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb: 
wandb: Run summary:
wandb: val_loss_fit 0.368
wandb: 
wandb: 🚀 View run Run 9 at: https://wandb.ai/lab_frohlich/pierre_mochi__fit_tmodel_3state_doubledeepms/runs/ptodvn5l
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20230610_183245-ptodvn5l/logs
wandb: - Waiting for wandb.init()...wandb: \ Waiting for wandb.init()...wandb: | Waiting for wandb.init()...wandb: wandb version 0.15.4 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.15.3
wandb: Run data is saved locally in /nemo/lab/froehlichf/home/users/demetzp/PierreDeMetz/inst/python/wandb/run-20230610_183634-2wb3oxqu
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run Run 10
wandb: ⭐️ View project at https://wandb.ai/lab_frohlich/pierre_mochi__fit_tmodel_3state_doubledeepms
wandb: 🚀 View run at https://wandb.ai/lab_frohlich/pierre_mochi__fit_tmodel_3state_doubledeepms/runs/2wb3oxqu
/camp/home/demetzp/.conda/envs/demetzp/lib/python3.8/site-packages/haiku/_src/data_structures.py:143: FutureWarning: jax.tree_flatten is deprecated, and will be removed in a future release. Use jax.tree_util.tree_flatten instead.
  leaves, treedef = jax.tree_flatten(tree)
/camp/home/demetzp/.conda/envs/demetzp/lib/python3.8/site-packages/haiku/_src/data_structures.py:144: FutureWarning: jax.tree_unflatten is deprecated, and will be removed in a future release. Use jax.tree_util.tree_unflatten instead.
  return jax.tree_unflatten(treedef, leaves)
wandb: Waiting for W&B process to finish... (success).
wandb: - 0.005 MB of 0.005 MB uploaded (0.000 MB deduped)wandb: \ 0.005 MB of 0.005 MB uploaded (0.000 MB deduped)wandb: | 0.005 MB of 0.005 MB uploaded (0.000 MB deduped)wandb: / 0.005 MB of 0.005 MB uploaded (0.000 MB deduped)wandb: - 0.005 MB of 0.005 MB uploaded (0.000 MB deduped)wandb: \ 0.005 MB of 0.005 MB uploaded (0.000 MB deduped)wandb: | 0.005 MB of 0.005 MB uploaded (0.000 MB deduped)wandb: 
wandb: Run history:
wandb: val_loss_fit ██▇▇▇▆▆▅▅▅▅▄▄▄▄▃▃▃▃▂▂▂▂▂▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁
wandb: 
wandb: Run summary:
wandb: val_loss_fit 0.364
wandb: 
wandb: 🚀 View run Run 10 at: https://wandb.ai/lab_frohlich/pierre_mochi__fit_tmodel_3state_doubledeepms/runs/2wb3oxqu
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20230610_183634-2wb3oxqu/logs
wandb: - Waiting for wandb.init()...wandb: \ Waiting for wandb.init()...wandb: | Waiting for wandb.init()...wandb: wandb version 0.15.4 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.15.3
wandb: Run data is saved locally in /nemo/lab/froehlichf/home/users/demetzp/PierreDeMetz/inst/python/wandb/run-20230610_184016-79inko3e
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run Run 11
wandb: ⭐️ View project at https://wandb.ai/lab_frohlich/pierre_mochi__fit_tmodel_3state_doubledeepms
wandb: 🚀 View run at https://wandb.ai/lab_frohlich/pierre_mochi__fit_tmodel_3state_doubledeepms/runs/79inko3e
/camp/home/demetzp/.conda/envs/demetzp/lib/python3.8/site-packages/haiku/_src/data_structures.py:143: FutureWarning: jax.tree_flatten is deprecated, and will be removed in a future release. Use jax.tree_util.tree_flatten instead.
  leaves, treedef = jax.tree_flatten(tree)
/camp/home/demetzp/.conda/envs/demetzp/lib/python3.8/site-packages/haiku/_src/data_structures.py:144: FutureWarning: jax.tree_unflatten is deprecated, and will be removed in a future release. Use jax.tree_util.tree_unflatten instead.
  return jax.tree_unflatten(treedef, leaves)
wandb: Waiting for W&B process to finish... (success).
wandb: - 0.005 MB of 0.005 MB uploaded (0.000 MB deduped)wandb: \ 0.005 MB of 0.005 MB uploaded (0.000 MB deduped)wandb: | 0.005 MB of 0.005 MB uploaded (0.000 MB deduped)wandb: / 0.005 MB of 0.005 MB uploaded (0.000 MB deduped)wandb: - 0.005 MB of 0.005 MB uploaded (0.000 MB deduped)wandb: \ 0.005 MB of 0.005 MB uploaded (0.000 MB deduped)wandb: | 0.005 MB of 0.005 MB uploaded (0.000 MB deduped)wandb: / 0.005 MB of 0.005 MB uploaded (0.000 MB deduped)wandb: 
wandb: Run history:
wandb: val_loss_fit ██▇▇▆▆▆▅▅▅▄▄▄▄▃▃▃▃▃▂▂▂▂▂▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁
wandb: 
wandb: Run summary:
wandb: val_loss_fit 0.367
wandb: 
wandb: 🚀 View run Run 11 at: https://wandb.ai/lab_frohlich/pierre_mochi__fit_tmodel_3state_doubledeepms/runs/79inko3e
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20230610_184016-79inko3e/logs
wandb: wandb version 0.15.4 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.15.3
wandb: Run data is saved locally in /nemo/lab/froehlichf/home/users/demetzp/PierreDeMetz/inst/python/wandb/run-20230610_184402-1ru8ly2x
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run Run 12
wandb: ⭐️ View project at https://wandb.ai/lab_frohlich/pierre_mochi__fit_tmodel_3state_doubledeepms
wandb: 🚀 View run at https://wandb.ai/lab_frohlich/pierre_mochi__fit_tmodel_3state_doubledeepms/runs/1ru8ly2x
srun: Job step aborted: Waiting up to 32 seconds for job step to finish.
slurmstepd: error: *** JOB 52971028 ON ca140 CANCELLED AT 2023-06-10T18:45:01 DUE TO TIME LIMIT ***
slurmstepd: error: *** STEP 52971028.0 ON ca140 CANCELLED AT 2023-06-10T18:45:01 DUE TO TIME LIMIT ***
Will exit after finishing currently running jobs (scheduler).
Will exit after finishing currently running jobs (scheduler).
